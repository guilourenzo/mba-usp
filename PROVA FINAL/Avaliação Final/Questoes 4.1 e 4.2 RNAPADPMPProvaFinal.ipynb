{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNAPADPMPProvaFinal.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flekT6GFDN6m"
      },
      "source": [
        "#PREENCHA SEU NOME COMPLETO AQUI: \n",
        "\n",
        "### <span style=\"color:blue\">MBA em Ciência de Dados</span>\n",
        "### <span style=\"color:blue\">Redes Neurais e Arquiteturas Profundas</span>\n",
        "### <span style=\"color:blue\">Análise de Dados com Base em Processamento Massivo em Paralelo</span>\n",
        "\n",
        "### <span style=\"color:blue\">Prova Final</span>\n",
        "\n",
        "**Material Produzido por:**<br>\n",
        ">**Profa. Dra. Cristina Dutra de Aguiar Ciferri**<br>\n",
        ">**Prof. Dr. Moacir A. Ponti**<br> \n",
        "\n",
        "**CEMEAI - ICMC/USP São Carlos**\n",
        "\n",
        "\n",
        "*A prova final contém 1 questão, dividida em 4 itens. Por favor, procurem por Questão para encontrar a especificação da questão e por Item para encontrar a especificação do item. Também é possível localizar a questão e os itens utilizando o menu de navegação. O notebook contém a constelação de fatos da BI Solutions que deve ser utilizada para responder à questão e também todas as `bibliotecas`, `bases de dados`, `inicializações`, `instalações`, `importações`, `geração de dataFrames`, `geração de visões temporárias` e `conversão dos tipos de dados` necessárias para a realização da questão.*\n",
        "\n",
        "\n",
        "**INSTRUÇÕES**:<br>\n",
        "1) Você deve exportar esse notebook com sua solução para as questões da prova em formato .py e fazer upload no Moodle. Atenção: você não deve fazer upload de um arquivo notebook (.ipynb), mas sim um arquivo texto .py contendo os códigos python que utilizou para resolver as questões. O arquivo .py pode ser gerado através da opção:<br>\n",
        "File --> Download as --> Python (.py)\n",
        "disponível no Jupyter Notebook.\n",
        "\n",
        "ou\n",
        "File --> Download .py\n",
        "no Google Colab\n",
        "\n",
        "Caso não esteja utilizando o Jupyter, copie e cole seu código em um arquivo ASCII (Texto) salvando com a extensão .py\n",
        "\n",
        "2) Você deve salvar esse notebook com sua solução para as questões da prova em formato .pdf e fazer upload no Moodle\n",
        "\n",
        "3) Os arquivos devem ser nomeados com seu nome e sobrenome, sem espaços. Exemplo: moacirponti.py e moacirponti.pdf\n",
        "\n",
        "4) É OBRIGATÓRIO conter no cabeçalho (início) do arquivo um comentário / texto com o seu nome completo\n",
        "\n",
        "\n",
        "**Desejamos uma boa prova!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3o3dN_WLQcyD"
      },
      "source": [
        "#1 Constelação de Fatos da BI Solutions\n",
        "\n",
        "A aplicação de *data warehousing* da BI Solutions utiliza como base uma contelação de fatos, conforme descrita a seguir.\n",
        "\n",
        "**Tabelas de dimensão**\n",
        "\n",
        "- data (dataPK, dataCompleta, dataDia, dataMes, dataBimestre, dataTrimestre, dataSemestre, dataAno)\n",
        "- funcionario (funcPK, funcMatricula, funcNome, funcSexo, funcDataNascimento, funcDiaNascimento, funcMesNascimento, funcAnoNascimento, funcCidade, funcEstadoNome, funcEstadoSigla, funcRegiaoNome, funcRegiaoSigla, funcPaisNome, funcPaisSigla)\n",
        "- equipe (equipePK, equipeNome, filialNome, filialCidade, filialEstadoNome, filialEstadoSigla, filialRegiaoNome, filialRegiaoSigla, filialPaisNome, filialPaisSigla)\n",
        "- cargo (cargoPK, cargoNome, cargoRegimeTrabalho, cargoEscolaridadeMinima, cargoNivel)\n",
        "- cliente (clientePK, clienteNomeFantasia, clienteSetor, clienteCidade, clienteEstadoNome, clienteEstadoSigla, clienteRegiaoNome, clienteRegiaoSigla, clientePaisNome, clientePaisSigla)\n",
        "\n",
        "**Tabelas de fatos**\n",
        "- pagamento (dataPK, funcPK, equipePK, cargoPK, salario, quantidadeLancamentos)\n",
        "- negociacao (dataPK, equipePK, clientePK, receita, quantidadeNegociacoes)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGeh8KdXwVCQ"
      },
      "source": [
        "#2 Configurações \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWX2HVtNSw8w"
      },
      "source": [
        "## 2.1 Obtenção dos Dados da BI Solutions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3e0Eao1K0EYG"
      },
      "source": [
        "#instalando o módulo wget\n",
        "%%capture\n",
        "!pip install -q wget\n",
        "!mkdir data\n",
        "\n",
        "#baixando os dados das tabelas de dimensão e das tabelas de fatos\n",
        "import wget\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/cdaciferri/DataMartBISolutions/main/data.csv\"\n",
        "wget.download(url, \"data/data.csv\")\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/cdaciferri/DataMartBISolutions/main/funcionario.csv\"\n",
        "wget.download(url, \"data/funcionario.csv\")\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/cdaciferri/DataMartBISolutions/main/equipe.csv\"\n",
        "wget.download(url, \"data/equipe.csv\")\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/cdaciferri/DataMartBISolutions/main/cargo.csv\"\n",
        "wget.download(url, \"data/cargo.csv\")\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/cdaciferri/DataMartBISolutions/main/cliente.csv\"\n",
        "wget.download(url, \"data/cliente.csv\")\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/cdaciferri/DataMartBISolutions/main/pagamento.csv\"\n",
        "wget.download(url, \"data/pagamento.csv\")\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/cdaciferri/DataMartBISolutions/main/negociacao.csv\"\n",
        "wget.download(url, \"data/negociacao.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sO16-7-jOioq"
      },
      "source": [
        "## 2.2 Instalações e Inicializações"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFfZ3QoxuV6q"
      },
      "source": [
        "#instalando Java Runtime Environment (JRE) versão 8\n",
        "%%capture\n",
        "!apt-get remove openjdk*\n",
        "!apt-get update --fix-missing\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpSR-ffXuZS3"
      },
      "source": [
        "#baixando Apache Spark versão 3.0.0\n",
        "%%capture\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop2.7.tgz\n",
        "!tar xf spark-3.0.0-bin-hadoop2.7.tgz && rm spark-3.0.0-bin-hadoop2.7.tgz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XC4whsGcuiEF"
      },
      "source": [
        "import os\n",
        "#configurando a variável de ambiente JAVA_HOME\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "#configurando a variável de ambiente SPARK_HOME\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-bin-hadoop2.7\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMmfchBEuky0"
      },
      "source": [
        "%%capture\n",
        "#instalando o pacote findspark\n",
        "!pip install -q findspark==1.4.2\n",
        "#instalando o pacote pyspark\n",
        "!pip install -q pyspark==3.0.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_8fiSPVwqAO"
      },
      "source": [
        "## 2.3 Bibliotecas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXls3bfoglKW"
      },
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"pyspark-notebook\").master(\"local[*]\").getOrCreate()\n",
        "\n",
        "from pyspark.sql.types import IntegerType\n",
        "from pyspark.sql.types import FloatType\n",
        "from pyspark.sql.functions import round, desc\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from numpy.random import seed\n",
        "from tensorflow.random import set_seed\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qL9SiR_pQE2"
      },
      "source": [
        "## 2.4 Geração dos DataFrames em Spark da BI Solutions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "id": "FNR-3dV6oYk4"
      },
      "source": [
        "#criando os DataFrames em Spark \n",
        "cargo = spark.read.csv(path=\"data/cargo.csv\", header=True, sep=\",\")\n",
        "cliente = spark.read.csv(path=\"data/cliente.csv\", header=True, sep=\",\")\n",
        "data = spark.read.csv(path=\"data/data.csv\", header=True, sep=\",\")\n",
        "equipe = spark.read.csv(path=\"data/equipe.csv\", header=True, sep=\",\")\n",
        "funcionario = spark.read.csv(path=\"data/funcionario.csv\", header=True, sep=\",\")\n",
        "negociacao = spark.read.csv(path=\"data/negociacao.csv\", header=True, sep=\",\")\n",
        "pagamento = spark.read.csv(path=\"data/pagamento.csv\", header=True, sep=\",\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmCV6Mur__z6"
      },
      "source": [
        "#convertendo os dados necessários para o tipo de dado inteiro\n",
        "colunas_cargo = [\"cargoPK\"]\n",
        "colunas_cliente = [\"clientePK\"]\n",
        "colunas_data = [\"dataPk\", \"dataDia\", \"dataMes\", \"dataBimestre\", \"dataTrimestre\", \"dataSemestre\", \"dataAno\"]\n",
        "colunas_equipe = [\"equipePK\"]\n",
        "colunas_funcionario = [\"funcPK\", \"funcDiaNascimento\", \"funcMesNascimento\", \"funcAnoNascimento\"]\n",
        "colunas_negociacao = [\"equipePK\", \"clientePK\", \"dataPK\", \"quantidadeNegociacoes\"]\n",
        "colunas_pagamento = [\"funcPK\", \"equipePK\", \"dataPK\", \"cargoPK\", \"quantidadeLancamentos\"]\n",
        "\n",
        "for coluna in colunas_cargo:\n",
        "  cargo = cargo.withColumn(coluna, cargo[coluna].cast(IntegerType()))\n",
        "\n",
        "for coluna in colunas_cliente:\n",
        "  cliente = cliente.withColumn(coluna, cliente[coluna].cast(IntegerType()))\n",
        "\n",
        "for coluna in colunas_data:\n",
        "  data = data.withColumn(coluna, data[coluna].cast(IntegerType()))\n",
        "\n",
        "for coluna in colunas_equipe:\n",
        "  equipe = equipe.withColumn(coluna, equipe[coluna].cast(IntegerType()))\n",
        "\n",
        "for coluna in colunas_funcionario:\n",
        "  funcionario = funcionario.withColumn(coluna, funcionario[coluna].cast(IntegerType()))\n",
        "\n",
        "for coluna in colunas_negociacao:\n",
        "  negociacao = negociacao.withColumn(coluna, negociacao[coluna].cast(IntegerType()))\n",
        "\n",
        "for coluna in colunas_pagamento:\n",
        "  pagamento = pagamento.withColumn(coluna, pagamento[coluna].cast(IntegerType()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBcQ7Ep7AWqN"
      },
      "source": [
        "#convertendo os dados necessários para o tipo de dado float\n",
        "colunas_negociacao = [\"receita\"]\n",
        "colunas_pagamento = [\"salario\"]\n",
        "\n",
        "for coluna in colunas_negociacao:\n",
        "  negociacao = negociacao.withColumn(coluna, negociacao[coluna].cast(FloatType()))\n",
        "\n",
        "for coluna in colunas_pagamento:\n",
        "  pagamento = pagamento.withColumn(coluna, pagamento[coluna].cast(FloatType()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJsqRI3TwsjS"
      },
      "source": [
        "#criando as visões temporárias \n",
        "cargo.createOrReplaceTempView(\"cargo\")\n",
        "cliente.createOrReplaceTempView(\"cliente\")\n",
        "data.createOrReplaceTempView(\"data\")\n",
        "equipe.createOrReplaceTempView(\"equipe\")\n",
        "funcionario.createOrReplaceTempView(\"funcionario\")\n",
        "negociacao.createOrReplaceTempView(\"negociacao\")\n",
        "pagamento.createOrReplaceTempView(\"pagamento\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEiHuA0Ly0o1"
      },
      "source": [
        "# 3 Questão"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVxVjOMEzXIW"
      },
      "source": [
        "A empresa BI Solutions resolveu realizar uma sindicância nos salários dos funcionários do ano de 2017. O objetivo da sindicância é identificar funcionários cujo padrão de recebimento de salário em 2017 esteja fora do padrão dos salários pagos em 2016, considerando que os dados de 2016 já foram auditados e estão adequados.\n",
        "\n",
        "Para tanto, a empresa resolveu utilizar uma rede neural para identificar *anomalias* e, a partir dessas *anomalias*, investigar os funcionários."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIUhajAB0Rwk"
      },
      "source": [
        "**IMPORTANTE**: Leia a questão com muita atenção, desde que vários passos da questão já se encontram implementados. Os locais nos quais os comandos da questão devem ser especificados são identificados em comentários. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFJlq2rfCZ3L"
      },
      "source": [
        "### 3.1 Dados para o Modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fz1CGW8Xy8a-"
      },
      "source": [
        "#### 3.1.1 Consulta dataAno 2016\n",
        "\n",
        "*Esta parte da questão já está resolvida. Realize a leitura do enunciado e da resposta.*\n",
        "\n",
        "Considere a consulta a seguir, a qual retorna os valores dos atributos `funcPK`, `equipePK`, `dataPK`, `cargoPK` e `salario` da relação `pagamento`, considerando os pagamentos realizados na `dataAno` de `2016`. \n",
        "**A resposta desta consulta deve ser usada para treinar o modelo posteriormente. Note que essa resposta é armazenada no dataFrame `respostaPandas16`, que é um dataFrame em Pandas.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJyc3oF2TBXs"
      },
      "source": [
        "#especificando a consulta que retorna os valores dos atributos\n",
        "#funcPK, equipePK, dataPK, cargoPK e salario\n",
        "#para pagamentos realizados na dataAno de 2016\n",
        "query2016 = query = \"\"\"\n",
        "SELECT funcPK, equipePK, pagamento.dataPK, cargoPK, salario\n",
        "FROM pagamento, data\n",
        "WHERE pagamento.dataPK = data.dataPK\n",
        "      AND dataAno = 2016\n",
        "ORDER BY funcPK, equipePK, pagamento.dataPK, cargoPK, salario       \n",
        "\"\"\"\n",
        "\n",
        "#transformando o resultado em um dataFrame em Pandas\n",
        "respostaPandas16 = spark.sql(query2016).toPandas()\n",
        "\n",
        "#exibindo algumas linhas do dataFrame respostaPandas16\n",
        "respostaPandas16"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urLtbiouODuI"
      },
      "source": [
        "#### 3.1.2 Consulta dataAno 2017\n",
        "\n",
        "*Esta parte da questão já está resolvida. Realize a leitura do enunciado e da resposta.*\n",
        "\n",
        "Considere a consulta a seguir, a qual retorna os valores dos atributos `funcPK`, `equipePK`, `dataPK`, `cargoPK` e `salario` da relação `pagamento`, considerando os pagamentos realizados na `dataAno` de `2017`. \n",
        "**A resposta desta consulta deve ser verificada posteriormente utilizando a rede neural já treinada (mais detalhes abaixo). Note que essa resposta é armazenada no dataFrame `respostaPandas17`, que é um dataFrame em Pandas.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOOMFF8i3L0M"
      },
      "source": [
        "#especificando a consulta que retorna os valores dos atributos\n",
        "#funcPK, equipePK, dataPK, cargoPK e salario\n",
        "#para pagamentos realizados na dataAno de 2017\n",
        "query2017 = query = \"\"\"\n",
        "SELECT funcPK, equipePK, pagamento.dataPK, cargoPK, salario\n",
        "FROM pagamento, data\n",
        "WHERE pagamento.dataPK = data.dataPK\n",
        "      AND dataAno = 2017\n",
        "ORDER BY funcPK desc, equipePK, pagamento.dataPK, cargoPK, salario       \n",
        "\"\"\"\n",
        "\n",
        "#transformando o resultado em um dataFrame em Pandas\n",
        "respostaPandas17 = spark.sql(query2017).toPandas()\n",
        "\n",
        "#exibindo algumas linhas do dataFrame respostaPandas17\n",
        "respostaPandas17"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SRZ-KczCnVI"
      },
      "source": [
        "### 3.2 Especificação da Rede Neural"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zalv2ejOCqUv"
      },
      "source": [
        "#### 3.2.1 Conversão dos dataFrames"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mYT9-kiC0wI"
      },
      "source": [
        "*Esta parte da questão já está resolvida. Realize a leitura do enunciado e da resposta.*\n",
        "\n",
        "Execute os comandos a seguir, os quais convertem os `dataFrames` `respostaPandas16` e `respostaPandas17` para `numpy array`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2p-U8Ho4DF5u"
      },
      "source": [
        "#Convertendo os dataFrames para numpy array\n",
        "x_train = respostaPandas16.to_numpy(copy=False)\n",
        "x_test = respostaPandas17.to_numpy(copy=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OgmsrtTDYFE"
      },
      "source": [
        "#### 3.2.2 Inicialização das Sementes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wf7Qfr47Def2"
      },
      "source": [
        "*Esta parte da questão já está resolvida. Realize a leitura do enunciado e da resposta.*\n",
        "\n",
        "Execute os comandos a seguir, os quais inicializam as sementes a serem utilizadas. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQYCSI-PDtZy"
      },
      "source": [
        "#inicializando as sementes\n",
        "seed(1)\n",
        "set_seed(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABcqQsfqD0-y"
      },
      "source": [
        "#### 3.2.3 Normalização\n",
        "\n",
        "*Esta parte da questão já está resolvida. Realize a leitura do enunciado e da resposta.*\n",
        "\n",
        "Execute os comandos a seguir, nos quais atributos de entrada são normalizados por `min-max` para o intervalo `[0-1]`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RkAaxfsEPG0"
      },
      "source": [
        "#Normalizando min-max para 0-1 (usando max e min do treinamento)\n",
        "max = x_train.max(axis=0)\n",
        "min = x_train.min(axis=0)\n",
        "x_train = (x_train-min)/(max-min)\n",
        "print(x_train.shape)\n",
        "\n",
        "x_test = (x_test-min)/(max-min)\n",
        "print(x_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_y1ykVvVEh1p"
      },
      "source": [
        "#### 3.2.4 Item 1 (RESOLVER)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xVYuxXeEtny"
      },
      "source": [
        "Projete um **autoencoder** com a seguinte arquitetura, sendo todas as camadas com função de ativação `relu`:\n",
        "  * Camada de entrada com 5 valores\n",
        "  * Camada densa com 4 neurônios\n",
        "  * Camada densa com 3 neurônios\n",
        "  * Dropout com taxa= 1/3.0\n",
        "  * Camada densa com 4 neurônios\n",
        "  * Camada de saída\n",
        "\n",
        "O objetivo desse autoencoder consiste em aprender uma representação para os dados do ano de 2016."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjuuVhxyFVpJ"
      },
      "source": [
        "#Escreva aqui a sua resposta para o Item 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HMBVI9fFtn-"
      },
      "source": [
        "#### 3.2.5 Item 2 (RESOLVER)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qplRm6eFzN_"
      },
      "source": [
        "**Compile** o modelo e **treine** por 250 épocas com batch_size 16, utilizando Adam com taxa de aprendizado inicial 0.002 e decaimento exponencial de -0.005 conforme funcão pré-definida abaixo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePkk1BRYmfu2"
      },
      "source": [
        "# Função que define decaimento para a taxa de aprendizado \n",
        "def scheduler(epoch, lr):\n",
        "    return lr * tf.math.exp(-0.005)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fjv2qnV7F8PY"
      },
      "source": [
        "#Escreva aqui a sua resposta para o Item 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIFxq_LqGjUc"
      },
      "source": [
        "#### 3.2.6 Item 3 (RESOLVER)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTCIp2MEGvCc"
      },
      "source": [
        "Utilizando o conjunto de teste (`x_test`) como entrada para o modelo treinado, obtenha as saídas da predição usando `modelo.predict(x_test)`.  A seguir, compute a soma do erro quadrático, i.e. $||x - \\hat{x}||^2$, sendo $x$ os atributos de entrada e $\\hat{x}$ os valores de saída da rede neural, ou seja, o erro de reconstrução de cada exemplo de teste (original) com relação ao predito pelo modelo (exemplos resultados da saída da predição).\n",
        "\n",
        "Armazene o erro de cada elemento organizados, em ordem, num array `error`.\n",
        "\n",
        "O objetivo é verificar se alguma tupla do ano de 2017 possui alto erro de reconstrução, o que indicaria uma possível anomalia a ser investigada com relação aos atributos (cargo, equipe, salário, etc.)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFVXhXNlG8Rw"
      },
      "source": [
        "#Escreva aqui a sua resposta para o Item 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlfKGAUYHlTg"
      },
      "source": [
        "#### 3.2.7 Obtendo o Valor de funcPK Relativo ao Maior Erro\n",
        "\n",
        "*Esta parte da questão já está resolvida. Realize a leitura do enunciado e da resposta.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rq_bYBq1IX24"
      },
      "source": [
        "Obtenha o valor do atributo `funcPK` não normalizado, referente ao `dataFrame` original, relativo à instância que possui o maior erro no teste. **O valor de `funcPK` deve ser usado para a especificação da consulta OLAP definida na seção 6.2.8.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbHnUY-b-vJw"
      },
      "source": [
        "# Obtendo o valor de funcPK desejado \n",
        "\n",
        "k_maiores = 1 # obtem os k valores com maior erro\n",
        "for i in np.argpartition(error,-k_maiores)[-k_maiores:]:\n",
        "    print(respostaPandas17.iloc[i,0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PQ8uSjYTlj-"
      },
      "source": [
        "## 3.3 Investigação do Funcionário"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CoWcv4sqTyV2"
      },
      "source": [
        "Uma vez identificado o código do funcionário, observou-se que ele recebeu salário muito superior quando comparado com os salários dos demais funcionários da empresa. Esse funcionário deve ser investigado considerando, portanto, seu salário. Essa investigação deve ser feita por meio da especificação de consultas OLAP. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTiNBAtwWvc4"
      },
      "source": [
        "Observações importantes:\n",
        "- A **consulta OLAP** pode ser especificada:\n",
        "   - Usando a **linguagem SQL** (conceitos apresentados na Aula 07)  \n",
        "   OU (escolha somente uma forma)\n",
        "   - Usando os métodos de **pyspark.sql** (conceitos apresentados na Aula 08). \n",
        "- Na listagem das respostas:\n",
        "   - As **colunas** solicitadas devem ser exibidas exatamente na mesma ordem que a definida.\n",
        "   - As **linhas** retornadas como respostas devem ser exibidas exatamente na mesma ordem que a definida. \n",
        "   - Os **nomes das colunas** renomeadas devem seguir estritamente os nomes definidos.\n",
        "- Quando a consulta OLAP for especificada usando a **linguagem SQL**, use:\n",
        "  - O comando `spark.sql(consultaSQL).show(20,truncate=False)` para exibir o resultado da consulta. Esse comando deve ser o último comando a ser especificado.\n",
        "  - A função `ROUND(funçãoDeAgregação,2)` para arredondar o dado até duas casas decimais.\n",
        "- Quando a consulta OLAP for especificada usando os demais **métodos de pyspark.sql**, use:\n",
        "  - O comando `nomeDoDataFrame.show(20,truncate=False)` para exibir o resultado da consulta. Esse comando deve ser o último comando a ser especificado.\n",
        "  - O método `round(funçãoDeAgregação,2)` para arredondar o dado até duas casas decimais. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWS1kgB3I5fj"
      },
      "source": [
        "#### 3.3.1 Item 4 (RESOLVER)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nih4rqBOdxpc"
      },
      "source": [
        "**Faça uma consulta que tem como objetivo investigar os meses e os valores dos salários recebidos no ano de 2017 pelo funcionário que está sendo inspecionado.** Ou seja, liste para cada mês do ano de 2017, o nome do funcionário, o mês e o salário recebido pelo funcionário cujo valor de `funcPK` foi identificado logo após o Item 3. Devem ser exibidas as colunas na ordem e com os nomes especificados a seguir: \"NOMEFUNCIONARIO\", \"MES\" e \"SALARIO\". Arredonde os salários para até duas casas decimais. Ordene as linhas exibidas primeiro por salário em ordem **descendente** e depois por mês em ordem **ascendente**. Liste as primeiras 20 linhas da resposta, sem truncamento das *strings*. **O maior valor de salário pode ser encontrado na primeira linha exibida para inspeção.** \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6JCZfVqfDOA"
      },
      "source": [
        "#Escreva aqui a sua resposta para o Item 4"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}