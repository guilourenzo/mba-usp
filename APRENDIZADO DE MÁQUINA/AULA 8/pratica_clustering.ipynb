{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plano\n",
    "\n",
    "Nessa prática abordaremos basicamente dois algoritmos para clustering particional, que se baseiam em diferentes estratégias:\n",
    "\n",
    "- Protótipo\n",
    "    1. k-Means\n",
    "    2. k-Medians\n",
    "    3. Partitioning Around Medoids (PAM) ou k-Medoids\n",
    "- Densidade\n",
    "    3. DBSCAN\n",
    "    \n",
    "## Lembrete:\n",
    "\n",
    "Apesar de alguns algoritmos de agrupamento oferecerem uma função `predict`, estamos falando de um paradigma descritivo de aprendizado, como apresentado em aula. É importante sempre nos lembrarmos das diferenças desse tipo de atividade em relação à tarefas de predição (aprendizado supervisionado).\n",
    " \n",
    "\n",
    "# 1. Clustering Particional: protótipos\n",
    "\n",
    "## 1.1. [k-Means](https://scikit-learn.org/stable/modules/clustering.html#k-means)\n",
    "\n",
    "O k-Means busca separar os dados em grupos de igual variância. Para tal, o k-Means minimiza um critério conhecido como *Inertia* (inércia) ou *Within-cluster sum-of-squares*. É o equivalente a miminizar a distância euclidiana ao quadrado de cada ponto para o seu centróide ou, ainda, a variância intra-cluster ([leitura interessante](https://stats.stackexchange.com/questions/158210/k-means-why-minimizing-wcss-is-maximizing-distance-between-clusters)). A inércia mede quão coerentes os clusters são internamente. No entanto, temos alguns problemas com essa abordagem:\n",
    "\n",
    "- A inércia assume que os clusters são convexos e isotrópicos (seus raios são iguais), o que nem sempre é verdade.\n",
    "- A inércia não é uma métrica com *range* bem definido: apenas sabemos que quanto menor, melhor e, que zero é o valor mínimo possível. Em espaços com muitas dimensões, o uso da distância euclidiana pode nos levar a problemas devido à um [caso específico](https://stats.stackexchange.com/questions/99171/why-is-euclidean-distance-not-a-good-metric-in-high-dimensions) da [tão dita maldição da dimensionalidade](https://builtin.com/data-science/curse-dimensionality). Nesse caso, vale a pena utilizar um algoritmo para redução de dimensões, como a Análise de Componentes Principais (PCA), antes do k-Means (de bônus, reduzimos o tempo de computação).\n",
    "\n",
    "\n",
    "Começarei definindo uma implementação simples desse algoritmo e depois avaliaremos o passo-a-passo de sua implementação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeans:\n",
    "    def __init__(self, k, etol=1e-3, max_iter=100, random_state=42):\n",
    "        self.k = k\n",
    "        self.etol = etol\n",
    "        self.max_iter = max_iter\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        # Define semente de geração aleatória\n",
    "        np.random.seed(self.random_state)\n",
    "        # Variáveis internas\n",
    "        self.cluster_centers_ = None\n",
    "        self.n_iter_ = 0  # Número de iterações\n",
    "        \n",
    "    \n",
    "    def _clustering_criterion(self, X, cluster_center):\n",
    "        # Within cluster sum-of-squares\n",
    "        return np.sum((X - cluster_center) ** 2, axis=1)\n",
    "    \n",
    "    \n",
    "    def _pick_centers(self, X, verbose=False):\n",
    "        # Vou selecionar aleatoriamente k linhas dos meus dados para serem\n",
    "        # os centros iniciais\n",
    "        sel_rows = np.random.choice(X.shape[0], size=self.k, replace=False)\n",
    "        \n",
    "        if verbose:\n",
    "            print('\\nLinhas selecionadas como centros dos clusters:', sel_rows)\n",
    "        \n",
    "        self.cluster_centers_ = np.zeros((self.k, X.shape[1]))\n",
    "        \n",
    "        # Note que o \"label\" que cada centro recebe depende na seleção inicial\n",
    "        # (não existe uma noção de ordem aqui)\n",
    "        for cluster_id, row in enumerate(sel_rows):\n",
    "            self.cluster_centers_[cluster_id] = X[row]\n",
    "        \n",
    "        if verbose:\n",
    "            print('Centros selecionados:')\n",
    "            for center_id, center in enumerate(self.cluster_centers_):\n",
    "                print(f'{center_id}: {center}')\n",
    "    \n",
    "    def _update_centers(self, X, verbose=False):\n",
    "        scores = self.predict(X)\n",
    "        \n",
    "        new_centers = np.zeros_like(self.cluster_centers_)\n",
    "        \n",
    "        for center_id in range(self.k):\n",
    "            new_centers[center_id] = np.mean(X[scores == center_id], axis=0)\n",
    "        \n",
    "        if verbose:\n",
    "            print('\\nMudança dos centros:')\n",
    "            for center_id in range(self.k):\n",
    "                print(f'{center_id}: {self.cluster_centers_[center_id]} -> {new_centers[center_id]}')\n",
    "                \n",
    "        return new_centers\n",
    "    \n",
    "    def _stop_criteria_convergence(self, new_centers, verbose=False):\n",
    "        diff = np.sum((self.cluster_centers_ - new_centers) ** 2)\n",
    "        check = diff < self.etol\n",
    "        \n",
    "        if verbose:\n",
    "            print(f'Variação dos centros: {diff}')\n",
    "            if check:\n",
    "                print(f'\\nVariação dos centros é menor que \"etol\": {diff}. Parando.')\n",
    "        return check\n",
    "    \n",
    "    def _stop_criteria_max_iter(self, verbose=False):\n",
    "        check = self.n_iter_ >= self.max_iter\n",
    "        \n",
    "        if verbose:\n",
    "            print(f'Iteração {self.n_iter_}')\n",
    "            if check:\n",
    "                print(f'\\nNúmero máximo de iterações atingido: {self.n_iter_}. Parando.')\n",
    "        return check\n",
    "    \n",
    "    def fit(self, X, verbose=False, cycle_callback=None):\n",
    "        # Inicializa os centros\n",
    "        self._pick_centers(X, verbose=verbose)\n",
    "        \n",
    "        p_out = None\n",
    "        \n",
    "        while True:  # Loop infinito. Os critérios de parada definirão o fim dos ciclos\n",
    "            new_centers = self._update_centers(X, verbose=verbose)\n",
    "            \n",
    "            # Pequeno acochambramento\n",
    "            if cycle_callback is not None:\n",
    "                p_out = cycle_callback(X, self.predict(X), new_centers, fig=p_out)\n",
    "            \n",
    "            if self._stop_criteria_convergence(new_centers, verbose=verbose):\n",
    "                break\n",
    "            \n",
    "            self.cluster_centers_ = new_centers\n",
    "            \n",
    "            # Pequeno acochambramento\n",
    "            if cycle_callback is not None:\n",
    "                p_out = cycle_callback(X, self.predict(X), self.cluster_centers_, fig=p_out)\n",
    "            \n",
    "            # Atualiza o número de iterações\n",
    "            self.n_iter_ += 1\n",
    "        \n",
    "            if self._stop_criteria_max_iter(verbose=verbose):\n",
    "                break\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X, verbose=False):\n",
    "        # Matriz com n_instances x n_clusters\n",
    "        errors = np.zeros((X.shape[0], self.k))\n",
    "        \n",
    "        for center_id, center in enumerate(self.cluster_centers_):\n",
    "            errors[:, center_id] = self._clustering_criterion(X, center)\n",
    "        \n",
    "        if verbose:\n",
    "            print(errors)\n",
    "            \n",
    "        return np.argmin(errors, axis=1)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Agora vamos aos poucos entender o papel de cada parte\n",
    "\n",
    "### 1.1.1. A iniciar pela seleção dos centros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Vou gerar uma matriz de numeros aleatórios\n",
    "np.random.seed(7)\n",
    "X_toy = np.random.uniform(size=(20, 2))\n",
    "\n",
    "print('Dados:')\n",
    "print(X_toy)\n",
    "\n",
    "kmeans = KMeans(k=3)\n",
    "kmeans._pick_centers(X_toy, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Como sempre, vou definir uma função simples de plot para não ficar repetindo código sem necessidade**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cluster(X, labels=None, centers=None, fig=None):\n",
    "    # Figura começando do zero\n",
    "    if fig is None:\n",
    "        dims = (3, 3)  # Sem legenda\n",
    "        if labels is not None:\n",
    "            if centers is None:\n",
    "                dims = (4, 3)  # Sem centros\n",
    "            else:\n",
    "                dims = (5, 3)  # Com centros\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=dims)\n",
    "    else:  # Reciclando figura anterior\n",
    "        ax = fig.axes[0]\n",
    "    \n",
    "    if labels is None:\n",
    "        ax.scatter(X[:, 0], X[:, 1])\n",
    "    else:\n",
    "        for center in np.unique(labels):\n",
    "            imask = labels == center\n",
    "            ax.scatter(X[imask, 0], X[imask, 1], label=center)\n",
    "\n",
    "        if centers is not None:\n",
    "            ax.scatter(centers[:, 0], centers[:, 1], marker='+', c='black',\n",
    "                       label='center', s=200)\n",
    "        ax.legend(loc='upper left', bbox_to_anchor=(1.05, 1))\n",
    "    plt.tight_layout()\n",
    "    plt.close()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cluster(X_toy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2. Função predict\n",
    "\n",
    "Notem que a as linhas escolhidas como centros iniciais tem inércia zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scores = kmeans.predict(X_toy, verbose=True)\n",
    "print('\\nScores:', scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sem visualizar os centroides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_cluster(X_toy, scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com os centroides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cluster(X_toy, scores, kmeans.cluster_centers_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3. Por fim, vamos testar a função que atualiza os centroides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_centroids = kmeans._update_centers(X_toy, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cluster(X_toy, scores, new_centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Vou forçar a troca dos centroides\n",
    "kmeans.cluster_centers_ = new_centroids\n",
    "scores = kmeans.predict(X_toy)\n",
    "print('\\nScores:', scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_cluster(X_toy, scores, kmeans.cluster_centers_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.4. Vamos avaliar nossos critérios de parada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.n_iter_ += 1\n",
    "# Iterações (iteramos apenas uma vez)\n",
    "kmeans._stop_criteria_max_iter(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let's try again: mais uma iteração"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "new_centroids = kmeans._update_centers(X_toy, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.n_iter_ += 1\n",
    "# Iterações (iteramos duas vezes)\n",
    "kmeans._stop_criteria_max_iter(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convergência\n",
    "kmeans._stop_criteria_convergence(new_centroids, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vou atualizar mais uma vez os centroides de forma manual\n",
    "kmeans.cluster_centers_ = new_centroids\n",
    "scores = kmeans.predict(X_toy)\n",
    "print('\\nScores:', scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_cluster(X_toy, scores, kmeans.cluster_centers_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E assim por diante...\n",
    "\n",
    "\n",
    "##### E se mudarmos a inicialização?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(k=3, random_state=0)\n",
    "kmeans._pick_centers(X_toy, verbose=True)\n",
    "scores = kmeans.predict(X_toy)\n",
    "print('\\nScores:', scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_cluster(X_toy, scores, kmeans.cluster_centers_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_centroids = kmeans._update_centers(X_toy, verbose=True)\n",
    "kmeans.cluster_centers_ = new_centroids\n",
    "scores = kmeans.predict(X_toy)\n",
    "print('\\nScores:', scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_cluster(X_toy, scores, kmeans.cluster_centers_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A inicialização faz toda a diferença. A literatura nos traz formas mais \"espertas\" de se inicializar os clusters. Por exemplo, esse [artigo](http://ilpubs.stanford.edu:8090/778/1/2006-13.pdf) é uma boa referência no assunto e sua proposta é implementada no sklearn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.5. Estamos com o queijo, a faca, e a goiabada na mão para entender o nosso método `fit`.\n",
    "\n",
    "Agora é só usar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(k=3)\n",
    "kmeans.fit(X_toy, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.6. Tem algo misterioso faltando nesse código\n",
    "\n",
    "E aquele parâmetro `cycle_callback`?\n",
    "\n",
    "Eu deixei esse parâmetro sendo chamado em dois momentos:\n",
    "\n",
    "- Quando novos centróides são definidos\n",
    "- Quando eles são atualizados\n",
    "\n",
    "`cycle_callback` é chamado como uma função... porque eu passarei uma função como parãmetro.\n",
    "\n",
    "Ideia: vamos visualizar o que está acontecendo na nossa implementação, passo-a-passo. Para tal, vou aproveitar aquela função de plot que eu havia definido anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_callback_factory(delay=0.2):\n",
    "    # Recicla plots para simular animações\n",
    "    def plot_at_each_cycle(X, scores, centroids, fig=None, delay=delay):\n",
    "        if fig is not None:\n",
    "            fig.axes[0].clear()\n",
    "        # Chama a nossa função de plot\n",
    "        fig = plot_cluster(X, scores, centroids, fig)\n",
    "        # Exibe os resultados\n",
    "        fig.show()\n",
    "        fig.canvas.draw()\n",
    "        time.sleep(delay)\n",
    "        \n",
    "        return fig\n",
    "    \n",
    "    return plot_at_each_cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Habilitarei plots interativos e desabilitarei a captura interativa dos plots\n",
    "%matplotlib notebook\n",
    "plt.ioff()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots com delay de meio segundo entre atualizações\n",
    "callback = plot_callback_factory(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(k=3)\n",
    "kmeans.fit(X_toy, cycle_callback=callback)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voltaremos a esse tipo de plot novamente :D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. (variante) k-Medians\n",
    "\n",
    "Enquanto o k-Means utiliza a média dos pontos como protótipo dos clusters. Outra alternativa é utilizar a mediana como o \"centro\" de cada cluster. Isso tem o efeito de minimizar a norma Manhattan (1-norm), ao invés do quadrado da norma Euclidiana (2-norm), como no k-Means. Vamos aproveitar a nossa implementação do k-Means e mudar apenas o necessário:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMedians(KMeans):\n",
    "    def _clustering_criterion(self, X, center):\n",
    "        # Taxicab norm or Manhattan norm (1-norm)\n",
    "        return np.sum(np.abs(X - center), axis=1)\n",
    "    \n",
    "    def _update_centers(self, X, verbose=False):\n",
    "        scores = self.predict(X)\n",
    "        \n",
    "        new_centers = np.zeros_like(self.cluster_centers_)\n",
    "        \n",
    "        for center_id in range(self.k):\n",
    "            # Aqui está a mudança!\n",
    "            new_centers[center_id] = np.median(X[scores == center_id], axis=0)\n",
    "        \n",
    "        if verbose:\n",
    "            print('\\nMudança dos centros:')\n",
    "            for center_id in range(self.k):\n",
    "                print(f'{center_id}: {self.cluster_centers_[center_id]} -> {new_center[center_id]}')\n",
    "                \n",
    "        return new_centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "plt.ioff()\n",
    "\n",
    "kmedians = KMedians(k=3, random_state=8)\n",
    "kmedians.fit(X_toy, cycle_callback=callback)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 (variante) Partitioning Around Medoids (PAM) ou k-Medoids\n",
    "\n",
    "O algoritmo PAM ou k-Medoids sempre utiliza pontos do próprio conjunto de dados como protótipo. Ele parte de um conjunto de *medoids* iniciais escolhidos aleatoriamente e funciona fazendo sucessivas substituições dos protótipos atuais por outros pontos que não são atualmente *medoids*, enquanto busca minimizar um função de distância. Aqui falamos diretamente em distância e, de fato, podemos usar qualquer métrica de distância com esse algorimo. Ao minimizar a distância total dos pontos para os medoids intuitivamente estamos escolhendo os \"pontos mais centrais\" nos clusters com os protótipos.\n",
    "\n",
    "Um problema dessa abordagem de troca de *medoids* é o seu custo computacional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMedoids(KMeans):\n",
    "    def __init__(self, k, etol=1e-3, max_iter=100, random_state=42, p=1.5):\n",
    "        super().__init__(k=k, etol=etol, max_iter=max_iter, random_state=random_state)\n",
    "        self.p = p\n",
    "    \n",
    "    def _clustering_criterion(self, X, center):\n",
    "        # Estou utilizando a distância Minkowski\n",
    "        return np.sum((np.abs(X - center) ** self.p), axis=1) ** (1 / self.p)\n",
    "    \n",
    "    def _update_centers(self, X, verbose=False):\n",
    "        best_cost = self._cost(X, self.cluster_centers_)\n",
    "        \n",
    "        # Inicio com os centros atuais\n",
    "        new_centers = self.cluster_centers_.copy()\n",
    "        \n",
    "        # Variavel auxiliar\n",
    "        aux_centers = self.cluster_centers_.copy()\n",
    "        \n",
    "        for center_id in range(self.k):\n",
    "            # Aqui está a mudança!\n",
    "            for x in X:\n",
    "                # Se x já é medoid, pule\n",
    "                if np.all(np.isclose(aux_centers - x, 0.)):\n",
    "                    continue\n",
    "                aux_centers[center_id] = x\n",
    "            \n",
    "                new_cost = self._cost(X, aux_centers)\n",
    "\n",
    "                if new_cost < best_cost:\n",
    "                    best_cost = new_cost\n",
    "                    new_centers[center_id] = x \n",
    "                else:\n",
    "                    aux_centers[center_id] = new_centers[center_id]\n",
    "        \n",
    "        if verbose:\n",
    "            print('\\nMudança dos centros:')\n",
    "            for center_id in range(self.k):\n",
    "                print(f'{center_id}: {self.cluster_centers_[center_id]} -> {new_center[center_id]}')\n",
    "                \n",
    "        return new_centers\n",
    "    \n",
    "    def _cost(self, X, centers):\n",
    "        # Matriz com n_instances x n_clusters\n",
    "        errors = np.zeros((X.shape[0], self.k))\n",
    "        \n",
    "        for center_id, center in enumerate(centers):\n",
    "            errors[:, center_id] = self._clustering_criterion(X, center)\n",
    "        \n",
    "        selected = np.argmin(errors, axis=1)\n",
    "        \n",
    "        return sum(sum(errors[selected == cluster_id, cluster_id]) for cluster_id in range(self.k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "plt.ioff()\n",
    "\n",
    "kmedoids = KMedoids(k=3, random_state=8)\n",
    "kmedoids.fit(X_toy, cycle_callback=callback)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Explorando casos mais interessantes\n",
    "\n",
    "Vou gerar alguns exemplos extras para observarmos como os algoritmos de agrupamento se comportam.\n",
    "\n",
    "### 1.3.1 Blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "# Estamos usando y apenas para pTaxicab norm or Manhattan normTaxicab norm or Manhattan normropósitos educativos. Nosso problema é não supervisionado\n",
    "X_blob1, y_blob1 = datasets.make_blobs(n_samples=100, centers=3, n_features=2, random_state=0, cluster_std=1)\n",
    "\n",
    "plot_cluster(X_blob1, y_blob1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### k-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "plt.ioff()\n",
    "\n",
    "# Note que \"sabemos o número de clusters\"\n",
    "kmeans_blob1 = KMeans(k=3, random_state=8)\n",
    "kmeans_blob1.fit(X_blob1, cycle_callback=callback)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### k-Medians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "plt.ioff()\n",
    "\n",
    "# Note que \"sabemos o número de clusters\"\n",
    "kmedians_blob1 = KMedians(k=3, random_state=8)\n",
    "kmedians_blob1.fit(X_blob1, cycle_callback=callback)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### k-Medoids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "plt.ioff()\n",
    "\n",
    "# Note que \"sabemos o número de clusters\"\n",
    "kmedoids_blob1 = KMedoids(k=3, random_state=3)\n",
    "kmedoids_blob1.fit(X_blob1, cycle_callback=callback)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Vamos complicar um pouco as coisas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_blob2, y_blob2 = datasets.make_blobs(n_samples=100, centers=4, n_features=2, random_state=0, cluster_std=0.5)\n",
    "\n",
    "plot_cluster(X_blob2, y_blob2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "plt.ioff()\n",
    "\n",
    "# Estou passando o número \"errado\" de clusters\n",
    "kmeans_blob2 = KMeans(k=3, random_state=8)\n",
    "kmeans_blob2.fit(X_blob2, cycle_callback=callback)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O k-Means encontrará os três clusters, como pedimos. Ou quatro..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "plt.ioff()\n",
    "\n",
    "kmeans_blob2 = KMeans(k=4, random_state=8)\n",
    "kmeans_blob2.fit(X_blob2, cycle_callback=callback)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ou dez!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "plt.ioff()\n",
    "\n",
    "# Estou passando o número \"errado\" de clusters\n",
    "kmeans_blob2 = KMeans(k=10, random_state=8)\n",
    "kmeans_blob2.fit(X_blob2, cycle_callback=callback)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2 Selecionando o número de clusters ([referência](https://scikit-learn.org/stable/modules/clustering.html#clustering-performance-evaluation))\n",
    "\n",
    "Buscaremos avaliar as partições que encontramos utilizando métricas de avaliação.\n",
    "\n",
    "#### 1.3.2.1 Caso não-supervisionado: Elbow method\n",
    "\n",
    "Foi o método apresentado em aula e é provavelmente uma das heurísticas mais conhecidas para a escolha do número de clusters no k-Means. Calculamos a soma das diferenças quadráticas de cada ponto para o centróide (*within cluster sum of squares* ou *inertia*) ao qual pertence e fazemos um plot variando o valor de `k`. Nesse gráfico procuramos pelo \"ponto de cotovelo\". No `sklearn` ela pode ser acessada utilizando `kmeans.inertia_`, onde `kmeans` é um modelo já treinado.\n",
    "\n",
    "A equação da Inertia (inércia) ou é dada por:\n",
    "\n",
    "$\\text{Inertia} = \\sum_{c \\in C} \\sum_{x \\in c} (x - \\overline{c})^2$, onde $C$ é o conjunto de todos os clusters e $\\overline{c}$ é o centróide do cluster $c$.\n",
    "\n",
    "Nós a implementaremos aqui utilizando métodos do nosso modelo kmeans treinado. Se estiver utilizando o k-Means do `sklearn` substitua essa função pela propriedade `inertia_` do `KMeans`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_inertia(X, kmeans):\n",
    "    scores = kmeans.predict(X)\n",
    "    \n",
    "    sum_inertia = 0.\n",
    "    for cluster_id in np.unique(scores):\n",
    "        imask = scores == cluster_id\n",
    "        sum_inertia += np.sum(\n",
    "            kmeans._clustering_criterion(X[imask], kmeans.cluster_centers_[cluster_id]))\n",
    "    \n",
    "    return sum_inertia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primeiro dataset\n",
    "inertia_scores = []\n",
    "for k in range(2, 10):\n",
    "    kmeans = KMeans(k=k, random_state=8)\n",
    "    kmeans.fit(X_blob1)\n",
    "    inertia_scores.append(sum_inertia(X_blob1, kmeans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 3))\n",
    "ax.plot(range(2, 10), inertia_scores)\n",
    "ax.set_xlabel('k')\n",
    "ax.set_ylabel('Inertia')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O \"cotovelo\" está em `k=3`, como esperado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segundo dataset\n",
    "inertia_scores = []\n",
    "for k in range(2, 10):\n",
    "    kmeans = KMeans(k=k, random_state=8)\n",
    "    kmeans.fit(X_blob2)\n",
    "    inertia_scores.append(sum_inertia(X_blob2, kmeans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 3))\n",
    "ax.plot(range(2, 10), inertia_scores)\n",
    "ax.set_xlabel('k')\n",
    "ax.set_ylabel('Inertia')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui, a mudança mais brusca (cotovelo) ocorre em `k=4`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.2.2 Caso não supervisionado: silhueta (silhouette)\n",
    "\n",
    "Uma forma simples de estimarmos o número de clusters é avaliarmos valores crescentes de `k` plotarmos os valores de silhueta obtidos. A nossa medida aqui é a silhueta que varia entre $[-1, 1]$, sendo $1$ o melhor valor possível. Essa métrica avalia a densidade das partições encontradas.\n",
    "\n",
    "- **Vantagens silhueta**\n",
    "   - Métrica em intervalo bem definido: de -1 (ruim), passando por 0 (clusters com *overlap*), até 1 (bom)\n",
    "   - Os valores são altos quando os clusters são densos e bem separados, o que está intimamente ligado à noção usual de agrupamento.\n",
    "- **Desvantagem**\n",
    "    - O k-Means cria estruturas convexas, de fato, estruturas similares à hiper-esferas (ou uma gaussiana multivariada). Essa métrica favorece esse tipo de estrutura. Clusters encontrados por outras estratégias de particionamento, como o agrupamento por densidade, tendem a obter menores valores de silhueta (e nem por isso são piores).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Primeiro dataset\n",
    "silh_scores = []\n",
    "for k in range(2, 10):\n",
    "    kmeans = KMeans(k=k, random_state=8)\n",
    "    kmeans.fit(X_blob1)\n",
    "    silh_scores.append(silhouette_score(X_blob1, kmeans.predict(X_blob1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 3))\n",
    "ax.plot(range(2, 10), silh_scores)\n",
    "ax.set_xlabel('k')\n",
    "ax.set_ylabel('Silhouette')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segundo dataset\n",
    "silh_scores = []\n",
    "for k in range(2, 10):\n",
    "    kmeans = KMeans(k=k, random_state=8)\n",
    "    kmeans.fit(X_blob2)\n",
    "    silh_scores.append(silhouette_score(X_blob2, kmeans.predict(X_blob2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 3))\n",
    "ax.plot(range(2, 10), silh_scores)\n",
    "ax.set_xlabel('k')\n",
    "ax.set_ylabel('Silhouette')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Escolhemos o ponto que maximiza a silhueta média (`k=3` e `k=4` para o primeiro e segundo casos, respectivamente).\n",
    "\n",
    "Devemos lembrar que a silhueta é definida para cada ponto. Até agora consideramos apenas valores médios.\n",
    "\n",
    "##### Abordagem mais geral\n",
    "\n",
    "Utilizarei uma outra abordagem, sugerida pelo `sklearn`, baseada em [silhouette analysis](https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html). Esta abordagem considera tanto a silhueta média, como a silhueta de cada ponto.\n",
    "\n",
    "Definirei uma função de plot extra:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "\n",
    "def plot_silhouettes(X, k, random_state=42):\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    ax.set_xlim([-1, 1])\n",
    "\n",
    "    # Use esta linha para a implementação didática\n",
    "    clusterer = KMeans(k=k, random_state=random_state)\n",
    "    # Use essa linha se estiver usando a versão do sklearn\n",
    "    # clusterer = KMeans(n_clusters=k, random_state=random_state)\n",
    "\n",
    "    clusterer.fit(X)\n",
    "    cluster_labels = clusterer.predict(X)\n",
    "\n",
    "    # Silhueta para cada ponto\n",
    "    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
    "    \n",
    "    # Silhueta média\n",
    "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "    \n",
    "    y_lower = 10  # Padding\n",
    "    for i in range(k):\n",
    "        # Seleciona as silhuetas de cada cluster e as ordena\n",
    "        cluster_silh = sample_silhouette_values[cluster_labels == i]\n",
    "        cluster_silh.sort()\n",
    "\n",
    "        size_cluster = len(cluster_silh)\n",
    "        y_upper = y_lower + size_cluster\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / k)\n",
    "        ax.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                         0, cluster_silh, facecolor=color,\n",
    "                         edgecolor=color, alpha=0.7)\n",
    "\n",
    "        # Nome dos clusters\n",
    "        ax.text(-0.05, y_lower + 0.5 * size_cluster, str(i))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax.set_title(f'k={k}')\n",
    "    ax.set_xlabel('Silhueta')\n",
    "    ax.set_ylabel('Cluster')\n",
    "\n",
    "    # Linha vertical indicando a silhueta média\n",
    "    ax.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "    \n",
    "    ax.set_yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**O que devemos observar nesse plot:**\n",
    "\n",
    "1. O eixo x apresenta os valores de silhueta\n",
    "2. Os clusters são apresentados de forma separada\n",
    "3. Para cada cluster, as instâncias que a eles pertecem estão ordenadas pelos seus valores individuais de silhueta\n",
    "4. A silhueta média é indicada pela linha vertical (serrilhada) vermelha\n",
    "\n",
    "**Como escolher `k` a partir desse plot?**\n",
    "1. Queremos que a largura das barras (variação no eixo `y`) sejam similares.\n",
    "2. Queremos que os comprimentos das barras estejam acima ou próximas do valor de silhueta médio (nunca abaixo).\n",
    "3. Queremos um valor de `k` que não acarrete flutuações nos comprimentos (todas as barras com comprimentos similares).\n",
    "4. Não queremos silhuetas negativas!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_silhouettes(X_blob2, 2, random_state=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_silhouettes(X_blob2, 3, random_state=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_silhouettes(X_blob2, 4, random_state=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_silhouettes(X_blob2, 5, random_state=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O nosso vencedor foi `k=4`, como esperado.\n",
    "\n",
    "**Atenção:** notem como eu usei uma random seed diferente. Em nossa implementação, utilizamos apenas uma inicialização aleatória dos centroides. Como eu já mencionei, existem formas mais \"espertas\" de se fazer isso. Tais estratégias fazem toda a diferença! Além disso, o `sklearn` prevê também utilizar várias inicializações e escolher qual delas for a melhor (isso é um hiperparâmetro).\n",
    "\n",
    "**\"Tarefa\":** quem tiver curiosidade, experimente mudar os `random_state` nos gráficos anteriores e nos próximos para observar o impacto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.2.3 Caso supervisionado: Adjusted Rand Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "# Primeiro dataset\n",
    "rand_scores = []\n",
    "for k in range(2, 10):\n",
    "    kmeans = KMeans(k=k, random_state=8)\n",
    "    kmeans.fit(X_blob1)\n",
    "    rand_scores.append(adjusted_rand_score(y_blob1, kmeans.predict(X_blob1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 3))\n",
    "ax.plot(range(2, 10), rand_scores)\n",
    "ax.set_xlabel('k')\n",
    "ax.set_ylabel('Adjusted Rand Index')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segundo dataset\n",
    "rand_scores = []\n",
    "for k in range(2, 10):\n",
    "    kmeans = KMeans(k=k, random_state=8)\n",
    "    kmeans.fit(X_blob2)\n",
    "    rand_scores.append(adjusted_rand_score(y_blob2, kmeans.predict(X_blob2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 3))\n",
    "ax.plot(range(2, 10), rand_scores)\n",
    "ax.set_xlabel('k')\n",
    "ax.set_ylabel('Adjusted Rand Index')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vantagens do _Adjusted Rand Index_ (ARI)**:\n",
    "- Predição aleatória origina um valor de ARI próximo de zero\n",
    "- Ranges bem definidos: $[-1, 1]$\n",
    "    - $-1$ é ruim\n",
    "    - $0$ é a predição aleatória\n",
    "    - $1$ é o melhor valor possível\n",
    "- Ignora permutações\n",
    "- É simétrica `ARI(A, B) == ARI(B, A)`\n",
    "- Não assume nada sobre a estrutura dos clusters\n",
    "\n",
    "**Desvantagem:**\n",
    "- Supervisionada, o que não é realístico na maioria dos casos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.3. Nem tudo são flores: dataset Circles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_circ1, y_circ1 = datasets.make_circles(n_samples=500, noise=0.05, random_state=8, factor=0.1)\n",
    "\n",
    "plot_cluster(X_circ1, y_circ1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dois clusters! Sopinha de algodão, certo? Vamos ver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "plt.ioff()\n",
    "\n",
    "kmeans_circ1 = KMeans(k=2, random_state=8)\n",
    "kmeans_circ1.fit(X_circ1, cycle_callback=callback)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "plt.ioff()\n",
    "\n",
    "kmeans_circ1 = KMeans(k=2, random_state=1)\n",
    "kmeans_circ1.fit(X_circ1, cycle_callback=callback)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "plt.ioff()\n",
    "\n",
    "kmeans_circ1 = KMeans(k=2, random_state=3)\n",
    "kmeans_circ1.fit(X_circ1, cycle_callback=callback)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E agora? Uma saída poderia ser aumentar o número de clusters e contar com um especialista humano para nos dizer que múltiplos grupos representam um mesmo conceito:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "plt.ioff()\n",
    "\n",
    "kmeans_circ1 = KMeans(k=5, random_state=8)\n",
    "kmeans_circ1.fit(X_circ1, cycle_callback=callback)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesse caso, e para essa seed aleatória (e apenas uma inicialização) temos que os clusters $k \\in \\{0, 2, 3, 4\\}$ representam o mesmo \"conceito\", i.e., o círculo externo. Nós sabemos isso porque nossos dados estão em duas dimensões e foram gerados artificialmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_silhouettes(X_circ1, 2, random_state=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_silhouettes(X_circ1, 5, random_state=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_silhouettes(X_circ1, 10, random_state=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "plt.ioff()\n",
    "\n",
    "kmeans_circ1 = KMeans(k=10, random_state=8)\n",
    "kmeans_circ1.fit(X_circ1, cycle_callback=callback)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Percebemos que apesar de `k=5` responder bem ao nosso problema, como observado empiricamente, a silhueta está privilegiando estruturas (hiper-esféricas) circulares e densas, como previamente discutido. Por essa razão $k=10$ poderia ser uma escolha boa segundo os critérios previamente discutidos.\n",
    "\n",
    "O problema vai mais longe... E se a estrutura fosse mais dúbia?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_circ2, y_circ2 = datasets.make_circles(n_samples=500, noise=0.05, random_state=8, factor=0.7)\n",
    "\n",
    "plot_cluster(X_circ2, y_circ2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "plt.ioff()\n",
    "\n",
    "# k-Means\n",
    "kmeans_circ2 = KMeans(k=5, random_state=8)\n",
    "kmeans_circ2.fit(X_circ2, cycle_callback=callback)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "plt.ioff()\n",
    "\n",
    "# k-Medians\n",
    "kmedians_circ2 = KMedians(k=5, random_state=8)\n",
    "kmedians_circ2.fit(X_circ2, cycle_callback=callback)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É, agora complicou o meio de campo. :P\n",
    "\n",
    "E esse é apenas um dos exemplos. Podemos manter o padrão da primeira variante desse dataset com a adição de ruído para perceber um outro problema nesse tipo de abordagem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_circ3, y_circ3 = datasets.make_circles(n_samples=500, noise=0.15, random_state=8, factor=0.3)\n",
    "\n",
    "# Vou até acelerar os plots:\n",
    "callback = plot_callback_factory(0.1)\n",
    "\n",
    "plot_cluster(X_circ3, y_circ3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "plt.ioff()\n",
    "\n",
    "# k-Means\n",
    "kmeans_circ3 = KMeans(k=2, random_state=8)\n",
    "kmeans_circ3.fit(X_circ3, cycle_callback=callback)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "plt.ioff()\n",
    "\n",
    "# k-Means\n",
    "kmeans_circ3 = KMeans(k=3, random_state=8)\n",
    "kmeans_circ3.fit(X_circ3, cycle_callback=callback)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "plt.ioff()\n",
    "\n",
    "# k-Means\n",
    "kmeans_circ3 = KMeans(k=5, random_state=8)\n",
    "kmeans_circ3.fit(X_circ3, cycle_callback=callback)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "plt.ioff()\n",
    "\n",
    "# k-Means\n",
    "kmeans_circ3 = KMeans(k=10, random_state=8)\n",
    "kmeans_circ3.fit(X_circ3, cycle_callback=callback)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.4 Moons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_moons1, y_moons1 = datasets.make_moons(random_state=8, noise=0.05)\n",
    "\n",
    "plot_cluster(X_moons1, y_moons1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "plt.ioff()\n",
    "\n",
    "# k-Means\n",
    "kmeans_moons1 = KMeans(k=2, random_state=8)\n",
    "kmeans_moons1.fit(X_moons1, cycle_callback=callback)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "plt.ioff()\n",
    "\n",
    "# k-Means\n",
    "kmeans_moons1 = KMeans(k=5, random_state=8)\n",
    "kmeans_moons1.fit(X_moons1, cycle_callback=callback)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_silhouettes(X_moons1, 2, random_state=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_moons1 = KMeans(k=2, random_state=8)\n",
    "kmeans_moons1.fit(X_moons1)\n",
    "adjusted_rand_score(y_moons1, kmeans_moons1.predict(X_moons1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_silhouettes(X_moons1, 3, random_state=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_moons1 = KMeans(k=3, random_state=8)\n",
    "kmeans_moons1.fit(X_moons1)\n",
    "adjusted_rand_score(y_moons1, kmeans_moons1.predict(X_moons1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_silhouettes(X_moons1, 5, random_state=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_moons1 = KMeans(k=5, random_state=8)\n",
    "kmeans_moons1.fit(X_moons1)\n",
    "adjusted_rand_score(y_moons1, kmeans_moons1.predict(X_moons1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Houston we have a problem!**\n",
    "\n",
    "O víes de aprendizado dos algorítmos baseados em protótipos não é adequado para esses problemas cujos clusters não são convexos e nem hiper-esféricos. De fato, o k-Means pode ser visto como um caso especial de *Gaussian Mixture models*. Se abstrairmos um pouco, o que estamos fazendo é posicionar funções gaussianas multivariadas nos dados. A média dessas gaussianas é justamente os centróides. Cool, huh?\n",
    "\n",
    "Avaliaremos um outro tipo de agrupamento particional para tentar resolver esse problema específico.\n",
    "\n",
    "\n",
    "**Mas calma!** Não é o fim da linha para o nosso amigo k-Means. Deixo aqui esse [post](https://pafnuty.wordpress.com/2013/08/14/non-convex-sets-with-k-means-and-hierarchical-clustering/) que achei muito interessante. O autor demonstra como podemos combinar agrupamento hierárquico (a ser ainda discutido) com o k-Means para lidar com problemas não-convexos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Clustering particional: densidade\n",
    "\n",
    "*Density-based spatial clustering of applications with noise* [(DBSCAN)](https://scikit-learn.org/stable/modules/clustering.html#dbscan) é um algoritmo para agrupamento de dados que funciona de forma diferente das abordagens que vimos até agora. O DBSCAN não assume nada sobre as estruturas dos clusters nos dados. De fato, para o DBSCAN, clusters são regiões de alta densidade separadas por regiões de baixa densidade, não importando a sua forma. Além disso, prevê a existência de pontos que não pertencem a nenhum dos clusters, em outras palavras, *outliers*.\n",
    "\n",
    "O DBSCAN parte dos seguintes pressupostos principais:\n",
    "\n",
    "- *Core points*: são pontos em regiões de alta densidade;\n",
    "- *Border points*: pontos \"acessíveis\" através de um *core* point, mas que não são *core points*.\n",
    "- *Noise/Outlier*: pontos que não são acessíveis por um ponto *core*.\n",
    "\n",
    "O DBSCAN possui dois hiper-parâmetros:\n",
    "\n",
    "- `eps`: define um raio de vizinhança, em outras palavras, um ponto $p$ está conectado a outro ponto $q$ se $d(p, q) \\le \\epsilon$, onde $d$ é uma função de distância (aqui não assumimos nada sobre a métrica de distância utilizada).\n",
    "- `min_samples`: define quantos pontos devem estar na `eps`-vizinhança de um ponto, para que tal ponto seja considerado *core*.\n",
    "\n",
    "No entanto, o DBSCAN tem problemas com datasets que possuem muitas variações em densidade e são esparsos. Se os clusters possuem diferentes densidades não será possível se encontrar uma boa combinação de `eps` e `min_samples`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KDTree\n",
    "\n",
    "class DBSCAN:\n",
    "    def __init__(self, eps=0.5, min_samples=4, p=2):\n",
    "        self.eps = eps\n",
    "        self.min_samples = min_samples\n",
    "        self.p = p  # Distância Minkowski\n",
    "    \n",
    "    def _minkowski_distance(self, x1, x2):\n",
    "        return np.sum(np.abs(x1 - x2) ** self.p, axis=1) ** (1 / self.p)\n",
    "    \n",
    "    def fit(self, X, cycle_callback=None):\n",
    "        # Plot inicial vazio\n",
    "        p_out = None\n",
    "        \n",
    "        # Indices na base dados\n",
    "        indices = np.array(range(X.shape[0]))\n",
    "\n",
    "        # O label de todos os pontos é indefinido\n",
    "        self.labels_ = np.full(X.shape[0], -999)\n",
    "        \n",
    "        # Faz primeiro plot\n",
    "        if cycle_callback is not None:\n",
    "            p_out = cycle_callback(X, self.labels_, fig=p_out)\n",
    "        \n",
    "        # Core samples\n",
    "        self.core_sample_indices_ = set()\n",
    "        \n",
    "        # Estrutura para buscar vizinhos mais próximos\n",
    "        kdtree = KDTree(X, p=self.p)\n",
    "\n",
    "        c = 0  # Id do cluster\n",
    "        for i in range(X.shape[0]):\n",
    "            # Já é core point, ponto de borda ou outlier\n",
    "            if self.labels_[i] != -999:\n",
    "                continue\n",
    "            \n",
    "            # Todos os vizinhos de X_i, com ele incluso no raio eps\n",
    "            neighbors = kdtree.query_radius(X[i].reshape(1, -1), r=self.eps)[0]\n",
    "            \n",
    "            if len(neighbors) < self.min_samples: # Não é denso o suficiente\n",
    "                self.labels_[i] = -1  # Noise ou outlier\n",
    "                \n",
    "                # Atualiza plot\n",
    "                if cycle_callback is not None:\n",
    "                    p_out = cycle_callback(X, self.labels_, fig=p_out)\n",
    "                continue\n",
    "            \n",
    "            self.core_sample_indices_.add(i)\n",
    "            self.labels_[i] = c\n",
    "            \n",
    "            # Atualiza plot\n",
    "            if cycle_callback is not None:\n",
    "                p_out = cycle_callback(X, self.labels_, fig=p_out)\n",
    "            \n",
    "            # Remove X_i de sua própria vizinhança e define conjunto para expansão\n",
    "            seed_set = neighbors[neighbors != i].tolist()\n",
    "            \n",
    "            # Agora expandiremos a vizinhança do nosso ponto p\n",
    "            for j in seed_set:\n",
    "                if self.labels_[j] == -1:  # É ruido (até agora)\n",
    "                    self.labels_[j] = c  # Muda ponto previamente considerado ruído\n",
    "                    # Atualiza plot\n",
    "                    if cycle_callback is not None:\n",
    "                        p_out = cycle_callback(X, self.labels_, fig=p_out)\n",
    "                \n",
    "                if self.labels_[j] != -999:\n",
    "                    continue  # Ponto já foi processado\n",
    "                \n",
    "                self.labels_[j] = c\n",
    "\n",
    "                # Atualiza plot\n",
    "                if cycle_callback is not None:\n",
    "                    p_out = cycle_callback(X, self.labels_, fig=p_out)\n",
    "                \n",
    "                # Vamos procurar mais pontos que podem ser acessados por esse elemento\n",
    "                # conectado ao nosso core point\n",
    "                neighbors = kdtree.query_radius(X[j].reshape(1, -1), r=self.eps)[0]\n",
    "                \n",
    "                if len(neighbors) >= self.min_samples:  # Também é um core point\n",
    "                    seed_set.extend(neighbors.tolist())  # Adiciona pontos para o seed set\n",
    "                    self.core_sample_indices_.add(j)\n",
    "            \n",
    "            c += 1 # Vamos para o próximo cluster!\n",
    "        \n",
    "        # Converte para array do numpy (só para ficar parecido com o sklearn)\n",
    "        self.core_sample_indices_ = np.array(list(self.core_sample_indices_))\n",
    "        \n",
    "        # Vou salvar os core points (só para ficar igual ao sklearn também)\n",
    "        self.components_ = np.zeros((len(self.core_sample_indices_), X.shape[1]))\n",
    "        for i, core_p in enumerate(self.core_sample_indices_):\n",
    "            self.components_[i] = X[core_p]\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def fit_predict(self, X):\n",
    "        self.fit(X)\n",
    "        \n",
    "        return self.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Criarei um função diferente para plot, visto as diferenças do algoritmo de clustering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dbscan_plot_callback_factory(delay=0.2):\n",
    "    # Recicla plots para simular animações\n",
    "    def plot_at_each_cycle(X, labels, fig=None, delay=delay):\n",
    "        if fig is not None:\n",
    "            fig.axes[0].clear()\n",
    "            ax = fig.axes[0]\n",
    "        else:\n",
    "            fig, ax = plt.subplots(figsize=(5, 3))\n",
    "        \n",
    "        label_transformer = lambda x: (f'Cluster {x}' if x >= 0 else 'Noise') \\\n",
    "            if x >= -1 else 'Indefinido'\n",
    "        marker_transformer = lambda x: ('o' if x >= 0 else 'x') if x >= -1 else '.'\n",
    "        \n",
    "        # Transforma cluster label em valor que pode mapeado (estou supondo no max 15 clusters)\n",
    "        c_normalizer = matplotlib.colors.Normalize(vmin=0., vmax=15)\n",
    "        cmap = matplotlib.cm.get_cmap('hsv')\n",
    "\n",
    "        color_transformer = lambda x: ((.5, .5, .5, 1.) if x == -999 else (0., 0., 0., 1.)) \\\n",
    "            if x < 0 else cmap(c_normalizer(x))\n",
    "\n",
    "        for label in np.unique(labels):\n",
    "            imask = labels == label\n",
    "            colors = [color_transformer(label) for _ in range(np.sum(imask))]\n",
    "            ax.scatter(X[imask, 0], X[imask, 1], label=label_transformer(label),\n",
    "                       marker=marker_transformer(label), c=colors)\n",
    "\n",
    "        ax.legend(loc='upper left', bbox_to_anchor=(1.05, 1))\n",
    "        plt.tight_layout()\n",
    "        plt.close()\n",
    "        # Exibe os resultados\n",
    "        fig.show()\n",
    "        fig.canvas.draw()\n",
    "        time.sleep(delay)\n",
    "        \n",
    "        return fig\n",
    "    \n",
    "    return plot_at_each_cycle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Função callback para plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan_callback = dbscan_plot_callback_factory(0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DBSCAN com valores default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "plt.ioff()\n",
    "\n",
    "dbscan = DBSCAN()\n",
    "dbscan.fit(X_toy, cycle_callback=dbscan_callback)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Não deu muito certo. Precisamos ter em mente características dos nossos dados (notem que os ranges dessa base são entre 0 e 1). Vamos tentar outros valores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "plt.ioff()\n",
    "\n",
    "dbscan = DBSCAN(eps=0.2)\n",
    "dbscan.fit(X_toy, cycle_callback=dbscan_callback)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Que tal esses?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "plt.ioff()\n",
    "\n",
    "dbscan = DBSCAN(eps=0.25)\n",
    "dbscan.fit(X_toy, cycle_callback=dbscan_callback)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dataset blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan_callback = dbscan_plot_callback_factory(0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "plt.ioff()\n",
    "\n",
    "dbscan = DBSCAN()\n",
    "dbscan.fit(X_blob1, cycle_callback=dbscan_callback)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "plt.ioff()\n",
    "\n",
    "dbscan = DBSCAN(min_samples=3)\n",
    "dbscan.fit(X_blob1, cycle_callback=dbscan_callback)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos tentar a outra variação que criamos para esse dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "plt.ioff()\n",
    "\n",
    "dbscan = DBSCAN()\n",
    "dbscan.fit(X_blob2, cycle_callback=dbscan_callback)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Está melhor.\n",
    "\n",
    "Vamos supor que nesse dataset nós esperaramos encontrar quatro grupos (sabemos as características do problema) e estamos percebendo muitos ruídos. Podemos tentar aumentar o valor de `eps`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "plt.ioff()\n",
    "\n",
    "dbscan = DBSCAN(eps=0.7)\n",
    "dbscan.fit(X_blob2, cycle_callback=dbscan_callback)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " A escolha de hiperparâmetros parece uma tarefa um pouco mais complicada nesse caso (e ainda temos o auxilio das visualizações). Escolher o `k` do k-Means foi muito mais fácil nesses casos!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Usando o DBSCAN em geometrias não-convexas\n",
    "\n",
    "Como será que o DBSCAN se sai nos casos onde o k-Means teve suas dificuldades?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "plt.ioff()\n",
    "\n",
    "dbscan = DBSCAN()\n",
    "dbscan.fit(X_circ1, cycle_callback=dbscan_callback)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "plt.ioff()\n",
    "\n",
    "dbscan = DBSCAN(eps=0.15, p=1, min_samples=6)\n",
    "dbscan.fit(X_circ2, cycle_callback=dbscan_callback)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(variei muito os hiperparâmetros até chegar nisso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "plt.ioff()\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "dbscan = DBSCAN()\n",
    "dbscan.fit(StandardScaler().fit_transform(X_moons1), cycle_callback=dbscan_callback)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No fim das contas, tudo é uma questão de boas escolhas :D\n",
    "\n",
    "\n",
    "## 2.2 Como escolher os valores de `eps` e `min_samples`\n",
    "\n",
    "Essa não é uma resposta trivial. Recentemente, os autores do algoritmo publicaram um [artigo](https://www.ccs.neu.edu/home/vip/teach/DMcourse/2_cluster_EM_mixt/notes_slides/revisitofrevisitDBSCAN.pdf) revisitando vários pontos do DBSCAN e discutem algumas possíveis formas de se escolher os seus parâmetros.\n",
    "\n",
    "- `min_samples`: uma *\"thumb rule\"* para selecionar esse hiper-parâmetro é `2 * n_features`. Notem que o valor padrão é `4` e estamos utilizando exemplos bi-dimensionais. No entanto, para datasets muito grandes, com muitas features, ou com muito ruído, é uma boa ideia mudar os valores desse parâmetro.\n",
    "- `eps`: esse hiper-parâmetro é mais complicado de ser ajustado e também depende da função de distância utilizada. Idealmente ele deve ser o menor possível e deveria ser ajustado com base em um especialista do domínio. Por exemplo, se estamos agrupando dados de GPS, um especialista poderia nos dizer que a distância de 1km deveria ligar dois pontos. Nesse caso, pode ser válido ajustar também o valore de `min_samples`. Os autores do DBSCAN aconselham deixar um dos hiper-parâmetros livres. Existem heurísticas parecidas com a do \"cotovelo\", mas não são tão triviais como no caso do k-Means.\n",
    "\n",
    "Em geral, ao explorarmos valores de hiper-parâmetros:\n",
    "- Não queremos que muitos pontos sejam marcados como ruído:\n",
    "    - Os autores do DBSCAN afirmam que um valor adequado de instâncias identificadas como ruído está entre $1\\%$ e $30\\%$\n",
    "- Se o maior componente (cluster) tem muitos dados (entre $20\\%$ até $50\\%$), isso pode ser um indício de que o valor de `eps` escolhido está muito alto. Nesse caso, os autores apontam dois caminhos:\n",
    "\n",
    "    - Diminuir o valor de `eps`\n",
    "    - Utilizar uma abordagem modificada do DBSCAN que utilizam hierarquias: OPTICS e HDBSCAN. Ambos esses algoritmos estão disponíveis no `sklearn`.\n",
    "\n",
    "De fato, a [seção de clustering](https://scikit-learn.org/stable/modules/clustering.html) do sklearn apresenta vários algoritmos de agrupamento que estão disponíveis, assim como métricas de avaliação, dicas e comparações entre os clusterizadores. Existe, inclusive, uma [tabela comparativa](https://scikit-learn.org/stable/modules/clustering.html#overview-of-clustering-methods) dos algorítmos disponíveis apontando casos de uso. Recomendo também a seção 4 do artigo que mencionei anteriormente, dos próprios autores do DBSCAN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Relembrando as diferenças\n",
    "\n",
    "- Protótipos vs Densidade\n",
    "    - O k-Means e similares utilizam uma noção de um centro, ou ponto representativo do grupo\n",
    "    - O DBSCAN não possui tais noções. Os pontos *core* não definem um protótipo para o grupo: um mesmo cluster pode possuir vários pontos *core*.\n",
    "\n",
    "- Número de clusters\n",
    "    - O número é pré-definido no k-Means\n",
    "        - Porém existem várias estratégias para se encontrar valores adequados de k\n",
    "        - Os grupos encontrados são hiper-esféricos\n",
    "    - O número é encontrado automaticamente no DBSCAN\n",
    "        - No entanto, encontrar valores para o seus hiper-parâmetros não é trivial\n",
    "        - Nada é assumido acerca da forma dos grupos\n",
    "\n",
    "- Ruídos e outliers\n",
    "    - O k-Means é sensível a ruídos e outliers (esses tipo de dados podem deslocar os centros)\n",
    "    - O DBSCAN prevê a existência de ruídos nos dados (o algorítmo deve estar bem ajustado para que funcione bem)\n",
    "\n",
    "- Inicialização\n",
    "    - O resultado do k-Means depende de sua inicialização\n",
    "    - O DBSCAN  é determinístico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Para uso em emergências:\n",
    "\n",
    "Executa as porções essenciais do código caso algum problema ocorra. Evita que seja necessário rodar todas as animações sequencialmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "Jupyter.notebook.execute_cells([0, 3, 4, 5, 7, 37, 40, 44, 47, 50, 58, 65, 73, 78, 86,\n",
    "                                92, 105, 109, 115, 126, 128, 138])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
