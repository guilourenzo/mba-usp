{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K8CDQUj8yqpq"
   },
   "source": [
    "## MBA em Ciência de Dados\n",
    "# Redes Neurais e Arquiteturas Profundas\n",
    "\n",
    "### <span style=\"color:darkred\">Módulo III - Arquiteturas de CNNS e treinamento de redes profundas</span>\n",
    "\n",
    "\n",
    "### <span style=\"color:darkred\">Exercícios com soluções</span>\n",
    "\n",
    "Moacir Antonelli Ponti\n",
    "\n",
    "CeMEAI - ICMC/USP São Carlos\n",
    "\n",
    "---\n",
    "\n",
    "#### <span style=\"color:red\">Recomenda-se fortemente que os exercícios sejam feitos sem consultar as respostas antecipadamente.</span>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xMJ4IFd7yqpt"
   },
   "source": [
    " ### Exercício 1)\n",
    "\n",
    "Considere 4 funções de custo distintas: 1. entropia cruzada binária, 2. perda quadrática, vistas em aula, e mais duas adicionais:\n",
    "\n",
    "3. Perda 0-1\n",
    "\n",
    "$$\\frac{1}{N} \\sum_{i=1}^N \n",
    "\\left\\{\n",
    "\t\\begin{array}{ll}\n",
    "\t\t0  & \\mbox{if } y_i = \\hat{y}_i \\\\\n",
    "\t\t1 & \\mbox{if } y_i \\neq \\hat{y}_i \n",
    "\t\\end{array}\n",
    "\\right.$$\n",
    "\n",
    "4. Perda SVM/Hinge\n",
    " \n",
    "$$\\frac{1}{N} \\sum_{i=1}^N \\max(0, 1- y^{h}_i\\cdot f(x_i)),$$\n",
    "essa função considera que as classes são -1 e 1, sendo $f(x_i)=\\hat{y}_i^{h}$ um valor de saída considerando valores negativos (os quais gerarão classificação para a classe -1) e positivos (classificação para a classe 1). Portanto será preciso adaptar as classes do problema e a  saída $\\hat{y}^{h}$ para esse cenário da seguinte forma:\n",
    "* $y^{h} \\in \\{-1,1\\}$, e\n",
    "* $\\hat{y}^{h} = 2\\cdot(\\hat{y}-0.5)$,\n",
    "sendo $\\hat{y}$ a probabilidade de uma instância pertencer à classe positiva (1).\n",
    "\n",
    "Considere o exemplo dado em aula, com os pontos unidimensionais conforme o código abaixo.\n",
    "\n",
    "A seguir, treine um classificador de Regressão Logística com solver `lbfgs` e compute as quatro perdas nesse conjunto de dados após o treinamento. Note que as perdas 1,2 e 4 são calculadas com base nas probabilidades, enquanto que 3 é calculada com base na classificação.\n",
    "\n",
    "Imprima as perdas por instância para inspeção e logo após a perda média no conjunto de treinamento. Qual a ordem de magnitude das perdas, da menor para a maior?\n",
    "\n",
    "(a) Hinge, Quadrática, Entropia Cruzada, 0-1<br>\n",
    "(b) Quadrática, Entropia Cruzada, Hinge e 0-1<br>\n",
    "(c) 0-1, Quadrática, Entropia Cruzada, Hinge<br>\n",
    "<font color='red'>(d) Quadrática, 0-1, Entropia Cruzada, Hinge</font><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 284
    },
    "id": "6uamxcISyqpv",
    "outputId": "0bcb9cab-624c-41d0-fe3f-a908efb7c1a4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f4c366cb1f0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.array([-1.8,-1.5,-0.8,-0.4,-0.2, 0.0, 0.1, 0.5, 1.0, 1.3])\n",
    "y = np.array([ 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0,  1.0, 1.0, 1.0])\n",
    "yh = np.array([ -1.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, 1.0, 1.0])\n",
    "\n",
    "plt.scatter(x,np.zeros(10), c=y,cmap=plt.cm.coolwarm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 94
    },
    "id": "PWg5eHSXyqp7",
    "outputId": "192b7a47-1c1c-463c-d97d-0e399247201d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y1    = [0. 0. 0. 1. 0. 1. 0. 1. 1. 1.]\n",
      "y_hat = [0.133 0.178 0.325 0.433 0.49  0.547 0.576 0.682 0.792 0.843]\n",
      "y_hat_hi= [-0.735 -0.645 -0.349 -0.134 -0.02   0.095  0.151  0.365  0.585  0.687]\n",
      "1-y*y_hat= [0.265 0.355 0.651 1.134 0.98  0.905 1.151 0.635 0.415 0.313]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# treinando o modelo\n",
    "logr1 = LogisticRegression(solver='lbfgs')\n",
    "logr1.fit(x.reshape(-1, 1), y)\n",
    "\n",
    "# pegando as probabilidades de saída\n",
    "y_hat = logr1.predict_proba(x.reshape(-1, 1))[:, 1].ravel()\n",
    "print('y1    = {}'.format(np.round(y , 3)))\n",
    "print('y_hat = {}'.format(np.round(y_hat, 3)))\n",
    "\n",
    "# classificando para calcular a perda 0-1\n",
    "y_clas = y_hat.copy()\n",
    "y_clas[y_clas>=0.5] = 1\n",
    "y_clas[y_clas<0.5] = 0\n",
    "\n",
    "# calculando a saída hinge, entre -1 e 1\n",
    "y_hat_hi = (y_hat-0.5)*2\n",
    "print('y_hat_hi= {}'.format(np.round(y_hat_hi, 3)))\n",
    "\n",
    "# perda quadrática\n",
    "loss_qu = np.power(y-y_hat,2)\n",
    "# perda de entropia cruzada\n",
    "loss_ec = -(y*np.log(y_hat+.00001) + (1-y)*np.log(1-y_hat +.00001))\n",
    "# perda zero-um\n",
    "loss_01 = (y!=y_clas)*1\n",
    "\n",
    "# perda hinge\n",
    "hi_mult = 1-(yh*y_hat_hi)\n",
    "loss_hi = [max(0,mi) for mi in hi_mult]\n",
    "print('1-y*y_hat= {}'.format(np.round(hi_mult, 3)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 209
    },
    "id": "k2jBVOEzyqqD",
    "outputId": "d6aaba3c-4699-4e98-a56f-3a382eae01d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perdas calculadas por instância:\n",
      "[0.018 0.032 0.106 0.321 0.24  0.205 0.331 0.101 0.043 0.025]\n",
      "[0.142 0.195 0.394 0.837 0.674 0.603 0.857 0.382 0.233 0.17 ]\n",
      "[0 0 0 1 0 0 1 0 0 0]\n",
      "[0.265 0.355 0.651 1.134 0.98  0.905 1.151 0.635 0.415 0.313]\n",
      "\n",
      "Perda quadrática = 0.1421\n",
      "Entropia cruzada = 0.4487\n",
      "Perda 0-1        = 0.2000\n",
      "Perda hinge/svm  = 0.6805\n"
     ]
    }
   ],
   "source": [
    "print(\"Perdas calculadas por instância:\")\n",
    "print(np.round(loss_qu,3))\n",
    "print(np.round(loss_ec,3))\n",
    "print(np.round(loss_01,3))\n",
    "print(np.round(loss_hi,3))\n",
    "print()\n",
    "\n",
    "print(\"Perda quadrática = %.4f\" % (np.mean(loss_qu)))\n",
    "print(\"Entropia cruzada = %.4f\" % (np.mean(loss_ec)))\n",
    "print(\"Perda 0-1        = %.4f\" % (np.mean(loss_01)))\n",
    "print(\"Perda hinge/svm  = %.4f\" % (np.mean(loss_hi)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "038CuS5syqqL"
   },
   "source": [
    "---\n",
    "\n",
    "### Exercício 2)\n",
    "\n",
    "Considere as funções de custo vistas em aula e estudadas no exercício anterior: Perda Quadrática (MSE), Erro Absoluto (MAE), Perda 0-1, Perda Hinge/SVM, Entropia Cruzada. Como escolher uma função para realizar o treinamento de uma rede neural?\n",
    "\n",
    "(a) Na dúvida escolher sempre a entropia cruzada, pois é a mais popular e considerada um padrão na literatura da área de redes neurais<br>\n",
    "(b) Desde que a função permita medir o erro do modelo atual, permite por consequência também medir o custo de escolher os parâmetros atuais, então qualquer função pode ser utilizada sem restrições<br>\n",
    "<font color='red'>(c) Considerar o problema em questão: classificação binária, multiclasse, regressão, etc e entender a magnitude dos valores das funções com base no problema e sua capacidade de guiar o modelo no processo de convergência<br></font>\n",
    "(d) Em geral, a entropia cruzada deve ser utilizada para problemas de classificação, e a perda quadrática para problemas de regressão, não sendo necessário investigar outras funções de custo pois são mais relevantes outros parâmetros como a taxa de aprendizado e o tamanho do batch<br>\n",
    "\n",
    "**Justificativa:** Apesar de mais popular, a entropia cruzada pode não funcionar bem em todos os cenários. Por outro lado, fazer uma busca exaustiva é impraticável. Assim, é preciso tomar uma decisão \"educada\" com base no problema em questão, os valores de saída, e selecionar um subconjunto de funções candidatas a serem investigadas para resolver o problema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nJQ0-S3myqqL"
   },
   "source": [
    "---\n",
    "### Exercício 3)\n",
    "\n",
    "Considerando as funções de perda: entropia cruzada categórica e perda quadrática, qual é o valor das perdas para um exemplo arbitrário no momento da inicialização aleatória de um modelo numa tarefa de classificação de 5 classes?\n",
    "\n",
    " (a) <font color='red'>Entropia Cruzada = 1.6; Quadrática = 0.8</font><br>\n",
    " (b) Entropia Cruzada = 2.3; Quadrática = 0.8<br>\n",
    " (c) Entropia Cruzada = 1.6; Quadrática = 0.16<br>\n",
    " (d) Entropia Cruzada = 0.32; Quadrática = 0.8<br>\n",
    " \n",
    " **Justificativa**: veja código abaixo. Na inicialização aleatória, teríamos um classificador gerando um vetor de probabilidade com a distribuição aproximadamente uniforme, ou seja, todos os valores 0.2=1/5. Computando a entropia cruzada categórica, temos apenas o -log do valor predito para a classe verdadeira, enquanto que na quadrática, a soma dos erros cometidos ao longo do vetor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "id": "JmVBm5CByqqM",
    "outputId": "5d912e29-5f38-4038-f886-8b7b1ceee008"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6093879136840585\n",
      "0.8000000000000002\n"
     ]
    }
   ],
   "source": [
    "y = np.array([.0, .0, .0, .0, 1.0])\n",
    "yh = np.array([0.2, 0.2, 0.2, 0.2, 0.2])\n",
    "\n",
    "loss_ec = -np.sum((y*np.log(yh+.00001)))\n",
    "\n",
    "loss_qu = np.sum(np.power(y-yh,2))\n",
    "print(loss_ec)\n",
    "print(loss_qu)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6exl-MrVyqqT"
   },
   "source": [
    "---\n",
    "### Exercício 4)\n",
    "\n",
    "Sobre os métodos de otimização, o que podemos dizer quando comparamos SGD e Adam?\n",
    "\n",
    " <font color='red'>(a) Ambos realizam atualização iterativa dos parâmetros usando o gradiente, mas o Adam incorpora mecanismos baseados em gradientes anteriores, e o segundo momento do gradiente como ponderação da taxa de aprendizado</font><br>\n",
    " (a) O SGD é equivalente ao Adam quando aplicado Momentum no algoritmo SGD base<br>\n",
    " (c) Ambos realizam atualização iterativa dos parâmetros usando o gradiente, mas apenas SGD permite decaimento da taxa de aprendizado <br>\n",
    " (d) O Adam é um algoritmo de otimização que obtém sempre melhores resultados do que o SGD e suas variações<br>\n",
    " \n",
    "  \n",
    " **Justificativa**: o SGD utiliza apenas o gradiente, enquanto o Adam computa primeiro momento do gradiente corrigida, e o segundo momento como forma de ponderar a magnitude do passo. As outras alternativas são inválidas porque: Adam utiliza uma estratégia similar, mas não igual ao momentum, e também possui um tipo de taxa de aprendizado adaptativa; Adam também permite decaimento da taxa de aprendizado; finalmente, não é possível dizer que um algoritmo de otimização é melhor sempre. Ainda que tenha mecanismos mais sofisticados, notar por exemplo que muitos modelos do estado da arte são treinados com SGD + Momentum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yfnm0YgLyqqU"
   },
   "source": [
    "---\n",
    "\n",
    "### Exercício 5)\n",
    "\n",
    "Dentre as alternativas, escolha a prática válida mais relevante ao projetar o treinamento de redes profundas\n",
    "\n",
    "(a) Inicializar todos os pesos com valores aleatórios e utilizar o maior número de instâncias possíveis no treinamento, garantindo que os hiperparâmetros com valor padrão obterão bons resultados<br>\n",
    "(b) Utilizar sempre a função de custo entropia cruzada, para a qual é recomendado o uso do otimizador Adam e taxa de aprendizado com decaimento. Definir a melhor taxa de decaimento de forma a minimizar a diferença entre o custo de treinamento e validação<br>\n",
    "<font color='red'>(c) Utilizar conjunto pequeno de instâncias para busca grosseira de hiperparâmetros como: otimizador, taxa de aprendizado, momentum e tamanho de batch, e depois refinar a busca num conjunto maior com base em métricas obtidas nos conjuntos de validação e treinamento<br></font>\n",
    "(d) Rezar para Yan LeCun, Yoshua Bengio, Geoffrey Hinton e Kunihiko Fukushima.\n",
    "\n",
    "\n",
    " **Justificativa**: nem sempre os valores padrão serão bons hiperparâmetros. Ainda que algumas escolhas sejam populares (como uso de Adam e Entropia Cruzada), o melhor é sempre realizar uma busca, ainda que grosseira com poucos dados, por parâmetros que se ajustem à arquitetura projetada. Se você acredita, rezar pode até te acalmar, mas não vai ajudar no treinamento da rede. Felizmente os 4 estão vivos, então tentar contatá-los no Twitter pode ser uma opção ;)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J6VfUMk8yqqW"
   },
   "source": [
    "---\n",
    "\n",
    "### Exercício 6)\n",
    "\n",
    "Qual a principal diferença das arquiteturas VGGNet, Inception e Residual Network com relação à suas camadas convolucionais?\n",
    "\n",
    "(a) A VGGNet possui camadas convolucionais com filtros de mesmo tamanho $3\\times3$, enquanto as outras arquiteturas, Inception e ResNet aplicam filtros $5\\times5$ ou com concatenação de mapas de ativação ao longo da rede<br>\n",
    "(b) A rede Inception permite treinamento com maior número de camadas quanto comparada à VGGNet, que por sua vez permite treinamento com maior número de camadas quanto comparada à ResNet <br>\n",
    "(c) A VGGNet possui camadas convolucionais sequenciais, eventualmente seguidas de MaxPooling, enquanto a ResNet computa mapas de ativação de com diferentes filtros, concatenando-os, e a Inception possui um módulo do tipo banco de filtros, que permite saltar para camadas futuras, facilitando o treinamento com mais camadas<br>\n",
    "<font color='red'>(d) A VGGNet possui camadas convolucionais sequenciais, enquanto Inception possui camadas convolucionais paralelas, e ResNet tem mapas de ativação que desviam da lógica sequencial e pulam camadas<br></font>\n",
    "\n",
    " **Justificativa**: Sua principal diferença é o fluxo durante a rede, sendo a VGG sequencial e as outras duas cujas ativações dão saltos (ResNet) ou possuem paralelismo (Inception). Alternativa (a) está errada pois ResNet não aplica filtros de tamanho maior do que 5x5, nem realiza concatenação de mapas de ativação (mas sim a soma); (b) é inválida pois a ResNet permite treinar com mais camadas do que a VGG; (c) está errada pois Inception não possui saltos nas camadas, nem ResNet possui concatenação de mapas. 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C1sh5GgYyqqY"
   },
   "source": [
    "---\n",
    "\n",
    "### Exercício 7)\n",
    "\n",
    "Utilizando a biblioteca Keras, investige o impacto do uso de parâmetros padrão de learning rate na base de dados Boston Housing, com relação ao uso de decaimento de learning rate, a partir de um valor estabelecido.\n",
    "\n",
    "Carregue a base de dados e normalize os atributos com z-score. Crie uma rede com camadas densas: 16, 8 e 1 (de saída), todas com ativação `relu`.\n",
    "\n",
    "Treine por 50 épocas 2 redes neurais com a função de custo `mse`, medindo também a `mae`\n",
    "\n",
    "A. Com otimizador Adam e todos os outros parâmetros no valor padrão\n",
    "B. Com otimizador Adam, iniciando com learning rate 0.02 e decaimento exponencial de 0.05 a partir da época 10\n",
    "\n",
    "Antes de projetar, compilar e treinar cada rede, defina as sementes do numpy para 1 e do tensorflow para 2.\n",
    "\n",
    "Use os dados de teste como \"validação\" durante o treinamento.\n",
    "\n",
    "Considerando os valores de erro (MSE e MAE) na última época de ambos A e B, e considerando ainda generalização como a divergência entre os erros de treinamento e validação, podemos dizer que:\n",
    "\n",
    "(a) B obteve menores valores de erro (MSE e MAE) do que A, mas em termos de generalização (entre treinamento e validação) ambos tiveram comportamento similar, com pequena vantagem para B<br>\n",
    "<font color='red'>(b) B obteve menores valores de erro (MSE e MAE) do que A, porém A obteve melhor generalização dos erros (entre treinamento e validação).<br></font>\n",
    "(c) B obteve valores de MSE significativamente menores do que A, mas A generalizou melhor com relação ao MAE, indicando que A pdoeria ser treinado por mais épocas.<br>\n",
    "(d) B obteve MSE de validação próximo a 7, A obteve 21 na mesma métrica, indicando que A não convergiu e apenas B generalizou<br>\n",
    "\n",
    "**Justificativa**: Notar no código abaixo que A obteve no treinamento mse: 21 e mae: 3, e na validacao mse: 28 e mae: 4; enquanto isso B obteve no treinamento mse: 7, mae: 2, e na validacao mse: 23, mae:3, assim B foi melhor em geral, mas em termos de generalização A apresentou valores mais próximos quando comparamos treinamento e validação.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "K5Owfr6GyqqY"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.datasets import boston_housing\n",
    "from numpy.random import seed\n",
    "from tensorflow.random import set_seed\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = boston_housing.load_data()\n",
    "\n",
    "mean = x_train.mean(axis=0)\n",
    "x_train -= mean\n",
    "std = x_train.std(axis=0)\n",
    "x_train /= std\n",
    "\n",
    "x_test -= mean\n",
    "x_test /= std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "id": "rU1SMr5Nyqqe",
    "outputId": "89530a6f-57c9-4c44-afe2-2484246bc49a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_41 (Dense)             (None, 16)                224       \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 369\n",
      "Trainable params: 369\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "seed(1)\n",
    "set_seed(2)\n",
    "\n",
    "model1 = keras.Sequential()\n",
    "model1.add(keras.layers.Dense(16, activation=\"relu\", input_shape=(x_train.shape[1],)))\n",
    "model1.add(keras.layers.Dense(8, activation=\"relu\"))\n",
    "model1.add(keras.layers.Dense(1, activation=\"relu\"))\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "k-6WqQIyyqqk"
   },
   "outputs": [],
   "source": [
    "epochs=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "3H42tssayqqp",
    "outputId": "cde8dca1-4ffa-43ad-a69b-7e93b563a602"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 585.1754 - mae: 22.3727 - val_loss: 613.9129 - val_mae: 23.0327\n",
      "Epoch 2/50\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 583.2806 - mae: 22.3219 - val_loss: 611.5378 - val_mae: 22.9651\n",
      "Epoch 3/50\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 580.0660 - mae: 22.2220 - val_loss: 607.6867 - val_mae: 22.8512\n",
      "Epoch 4/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 575.4921 - mae: 22.0783 - val_loss: 602.3406 - val_mae: 22.6769\n",
      "Epoch 5/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 569.1945 - mae: 21.8791 - val_loss: 595.4497 - val_mae: 22.4521\n",
      "Epoch 6/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 560.9045 - mae: 21.6187 - val_loss: 586.7144 - val_mae: 22.2004\n",
      "Epoch 7/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 550.8667 - mae: 21.3266 - val_loss: 576.0062 - val_mae: 21.8998\n",
      "Epoch 8/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 539.3816 - mae: 20.9878 - val_loss: 564.3648 - val_mae: 21.5709\n",
      "Epoch 9/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 527.2777 - mae: 20.6267 - val_loss: 552.4490 - val_mae: 21.2194\n",
      "Epoch 10/50\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 514.6431 - mae: 20.2411 - val_loss: 539.1353 - val_mae: 20.8213\n",
      "Epoch 11/50\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 500.5473 - mae: 19.8228 - val_loss: 524.1532 - val_mae: 20.4014\n",
      "Epoch 12/50\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 484.5043 - mae: 19.3594 - val_loss: 506.8472 - val_mae: 19.9354\n",
      "Epoch 13/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 466.3911 - mae: 18.8561 - val_loss: 486.8180 - val_mae: 19.4548\n",
      "Epoch 14/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 445.1684 - mae: 18.2758 - val_loss: 464.1213 - val_mae: 18.9416\n",
      "Epoch 15/50\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 421.7577 - mae: 17.6592 - val_loss: 438.3264 - val_mae: 18.3936\n",
      "Epoch 16/50\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 396.1416 - mae: 16.9767 - val_loss: 409.8697 - val_mae: 17.7608\n",
      "Epoch 17/50\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 368.3080 - mae: 16.2548 - val_loss: 379.9591 - val_mae: 17.0779\n",
      "Epoch 18/50\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 338.5601 - mae: 15.4908 - val_loss: 349.1888 - val_mae: 16.3332\n",
      "Epoch 19/50\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 308.6522 - mae: 14.7313 - val_loss: 316.1881 - val_mae: 15.5137\n",
      "Epoch 20/50\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 276.8495 - mae: 13.9183 - val_loss: 283.0538 - val_mae: 14.6546\n",
      "Epoch 21/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 245.3207 - mae: 13.0655 - val_loss: 249.0259 - val_mae: 13.6994\n",
      "Epoch 22/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 213.2803 - mae: 12.1205 - val_loss: 215.9491 - val_mae: 12.6860\n",
      "Epoch 23/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 182.3002 - mae: 11.1449 - val_loss: 185.0045 - val_mae: 11.6367\n",
      "Epoch 24/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 154.2476 - mae: 10.1345 - val_loss: 155.4173 - val_mae: 10.6212\n",
      "Epoch 25/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 127.4696 - mae: 9.1332 - val_loss: 129.5807 - val_mae: 9.6613\n",
      "Epoch 26/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 105.9056 - mae: 8.1996 - val_loss: 107.1700 - val_mae: 8.7351\n",
      "Epoch 27/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 86.8243 - mae: 7.3650 - val_loss: 90.3774 - val_mae: 7.9372\n",
      "Epoch 28/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 72.9218 - mae: 6.6659 - val_loss: 76.8644 - val_mae: 7.2161\n",
      "Epoch 29/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 61.6419 - mae: 6.0603 - val_loss: 67.2146 - val_mae: 6.6441\n",
      "Epoch 30/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 53.8190 - mae: 5.5992 - val_loss: 60.0445 - val_mae: 6.1753\n",
      "Epoch 31/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 48.0302 - mae: 5.2448 - val_loss: 54.5258 - val_mae: 5.7935\n",
      "Epoch 32/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 43.3715 - mae: 4.9288 - val_loss: 50.5551 - val_mae: 5.5295\n",
      "Epoch 33/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 39.9749 - mae: 4.6981 - val_loss: 47.2896 - val_mae: 5.3326\n",
      "Epoch 34/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 37.2680 - mae: 4.5086 - val_loss: 44.5110 - val_mae: 5.1706\n",
      "Epoch 35/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 34.9643 - mae: 4.3443 - val_loss: 42.2012 - val_mae: 5.0323\n",
      "Epoch 36/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 32.9622 - mae: 4.2131 - val_loss: 40.3673 - val_mae: 4.9243\n",
      "Epoch 37/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 31.3310 - mae: 4.1031 - val_loss: 38.7247 - val_mae: 4.8193\n",
      "Epoch 38/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 29.9206 - mae: 4.0018 - val_loss: 37.2963 - val_mae: 4.7320\n",
      "Epoch 39/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 28.7498 - mae: 3.9154 - val_loss: 36.0101 - val_mae: 4.6548\n",
      "Epoch 40/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 27.6514 - mae: 3.8355 - val_loss: 34.8849 - val_mae: 4.5832\n",
      "Epoch 41/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 26.7511 - mae: 3.7695 - val_loss: 33.8301 - val_mae: 4.5160\n",
      "Epoch 42/50\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 25.8317 - mae: 3.7048 - val_loss: 32.9782 - val_mae: 4.4604\n",
      "Epoch 43/50\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 25.1418 - mae: 3.6562 - val_loss: 32.1475 - val_mae: 4.4058\n",
      "Epoch 44/50\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 24.4319 - mae: 3.6026 - val_loss: 31.2991 - val_mae: 4.3470\n",
      "Epoch 45/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 23.7944 - mae: 3.5532 - val_loss: 30.6563 - val_mae: 4.3044\n",
      "Epoch 46/50\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 23.2665 - mae: 3.5159 - val_loss: 30.0919 - val_mae: 4.2672\n",
      "Epoch 47/50\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 22.7724 - mae: 3.4818 - val_loss: 29.4772 - val_mae: 4.2283\n",
      "Epoch 48/50\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 22.3118 - mae: 3.4576 - val_loss: 29.0922 - val_mae: 4.2028\n",
      "Epoch 49/50\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 21.8989 - mae: 3.4324 - val_loss: 28.6235 - val_mae: 4.1684\n",
      "Epoch 50/50\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 21.5179 - mae: 3.4108 - val_loss: 28.2870 - val_mae: 4.1476\n"
     ]
    }
   ],
   "source": [
    "model1.compile(optimizer=\"adam\",\n",
    "               loss='mse', metrics=['mae'])\n",
    "\n",
    "history = model1.fit(x_train, y_train, epochs=epochs,\n",
    "                     validation_data=(x_test,y_test), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "id": "XR4B-d4Nyqqt",
    "outputId": "3d376c26-f27e-43fe-a554-097ae4f3bb8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_44 (Dense)             (None, 16)                224       \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 369\n",
      "Trainable params: 369\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "seed(1)\n",
    "set_seed(2)\n",
    "\n",
    "model2 = keras.Sequential()\n",
    "model2.add(keras.layers.Dense(16, activation=\"relu\", input_shape=(x_train.shape[1],)))\n",
    "model2.add(keras.layers.Dense(8, activation=\"relu\"))\n",
    "model2.add(keras.layers.Dense(1, activation=\"relu\"))\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "HUrW9Q8Dyqqz",
    "outputId": "66e51e8c-f9bf-4449-fcd2-75cfe59826ce",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taxa atual = 0.02000\n",
      "Epoch 1/50\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 546.3033 - mae: 21.0637 - val_loss: 502.7121 - val_mae: 19.6967\n",
      "Taxa atual = 0.02000\n",
      "Epoch 2/50\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 364.7381 - mae: 16.3605 - val_loss: 209.1097 - val_mae: 12.1302\n",
      "Taxa atual = 0.02000\n",
      "Epoch 3/50\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 88.3731 - mae: 7.2638 - val_loss: 78.3675 - val_mae: 6.6225\n",
      "Taxa atual = 0.02000\n",
      "Epoch 4/50\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 41.4213 - mae: 4.5843 - val_loss: 40.2851 - val_mae: 5.1418\n",
      "Taxa atual = 0.02000\n",
      "Epoch 5/50\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 28.1764 - mae: 4.0251 - val_loss: 28.4457 - val_mae: 4.1727\n",
      "Taxa atual = 0.02000\n",
      "Epoch 6/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 21.8347 - mae: 3.3420 - val_loss: 24.9516 - val_mae: 3.9363\n",
      "Taxa atual = 0.02000\n",
      "Epoch 7/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 19.0849 - mae: 3.2354 - val_loss: 26.3385 - val_mae: 3.9648\n",
      "Taxa atual = 0.02000\n",
      "Epoch 8/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 17.1346 - mae: 3.0490 - val_loss: 25.4212 - val_mae: 3.7553\n",
      "Taxa atual = 0.02000\n",
      "Epoch 9/50\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 15.2533 - mae: 2.8776 - val_loss: 24.7372 - val_mae: 3.6326\n",
      "Taxa atual = 0.02000\n",
      "Epoch 10/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 13.8437 - mae: 2.7122 - val_loss: 25.8824 - val_mae: 3.6150\n",
      "Taxa atual = 0.02000\n",
      "Epoch 11/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 12.6995 - mae: 2.6255 - val_loss: 24.6819 - val_mae: 3.4543\n",
      "Taxa atual = 0.01900\n",
      "Epoch 12/50\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 12.2264 - mae: 2.5415 - val_loss: 23.9559 - val_mae: 3.3889\n",
      "Taxa atual = 0.01810\n",
      "Epoch 13/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 11.4202 - mae: 2.4677 - val_loss: 25.0335 - val_mae: 3.3759\n",
      "Taxa atual = 0.01720\n",
      "Epoch 14/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 10.9026 - mae: 2.4250 - val_loss: 24.0920 - val_mae: 3.3163\n",
      "Taxa atual = 0.01640\n",
      "Epoch 15/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 10.9228 - mae: 2.3915 - val_loss: 25.3525 - val_mae: 3.3452\n",
      "Taxa atual = 0.01560\n",
      "Epoch 16/50\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 10.5868 - mae: 2.3339 - val_loss: 22.8815 - val_mae: 3.2061\n",
      "Taxa atual = 0.01480\n",
      "Epoch 17/50\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 10.1835 - mae: 2.3254 - val_loss: 24.8257 - val_mae: 3.2882\n",
      "Taxa atual = 0.01410\n",
      "Epoch 18/50\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 9.7149 - mae: 2.2557 - val_loss: 24.9670 - val_mae: 3.2495\n",
      "Taxa atual = 0.01340\n",
      "Epoch 19/50\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 9.6145 - mae: 2.2383 - val_loss: 24.2749 - val_mae: 3.2392\n",
      "Taxa atual = 0.01270\n",
      "Epoch 20/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 9.6776 - mae: 2.2196 - val_loss: 24.1411 - val_mae: 3.2410\n",
      "Taxa atual = 0.01210\n",
      "Epoch 21/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 9.1538 - mae: 2.1823 - val_loss: 25.4822 - val_mae: 3.2217\n",
      "Taxa atual = 0.01150\n",
      "Epoch 22/50\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 9.1131 - mae: 2.1562 - val_loss: 24.0012 - val_mae: 3.1864\n",
      "Taxa atual = 0.01090\n",
      "Epoch 23/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 9.0302 - mae: 2.1679 - val_loss: 24.2062 - val_mae: 3.1824\n",
      "Taxa atual = 0.01040\n",
      "Epoch 24/50\n",
      "13/13 [==============================] - ETA: 0s - loss: 6.0845 - mae: 1.841 - 0s 2ms/step - loss: 8.8143 - mae: 2.1191 - val_loss: 24.5451 - val_mae: 3.1889\n",
      "Taxa atual = 0.00990\n",
      "Epoch 25/50\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 8.7356 - mae: 2.1085 - val_loss: 23.9083 - val_mae: 3.1614\n",
      "Taxa atual = 0.00940\n",
      "Epoch 26/50\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 8.6935 - mae: 2.1340 - val_loss: 25.3673 - val_mae: 3.2476\n",
      "Taxa atual = 0.00890\n",
      "Epoch 27/50\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 8.6450 - mae: 2.1152 - val_loss: 24.8235 - val_mae: 3.1728\n",
      "Taxa atual = 0.00850\n",
      "Epoch 28/50\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 8.2366 - mae: 2.0541 - val_loss: 24.2383 - val_mae: 3.1798\n",
      "Taxa atual = 0.00810\n",
      "Epoch 29/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 8.1566 - mae: 2.0214 - val_loss: 23.2697 - val_mae: 3.1009\n",
      "Taxa atual = 0.00770\n",
      "Epoch 30/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 8.1561 - mae: 2.0512 - val_loss: 23.9299 - val_mae: 3.1568\n",
      "Taxa atual = 0.00730\n",
      "Epoch 31/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 8.0103 - mae: 2.0198 - val_loss: 23.9747 - val_mae: 3.1116\n",
      "Taxa atual = 0.00690\n",
      "Epoch 32/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 8.0539 - mae: 2.0009 - val_loss: 24.1620 - val_mae: 3.1396\n",
      "Taxa atual = 0.00660\n",
      "Epoch 33/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 7.8383 - mae: 2.0024 - val_loss: 23.7048 - val_mae: 3.1033\n",
      "Taxa atual = 0.00630\n",
      "Epoch 34/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 7.9419 - mae: 2.0194 - val_loss: 23.4670 - val_mae: 3.0881\n",
      "Taxa atual = 0.00600\n",
      "Epoch 35/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 7.7736 - mae: 1.9632 - val_loss: 23.0987 - val_mae: 3.0756\n",
      "Taxa atual = 0.00570\n",
      "Epoch 36/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 7.6836 - mae: 1.9679 - val_loss: 24.5179 - val_mae: 3.1361\n",
      "Taxa atual = 0.00540\n",
      "Epoch 37/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 7.7157 - mae: 1.9988 - val_loss: 23.3377 - val_mae: 3.0777\n",
      "Taxa atual = 0.00510\n",
      "Epoch 38/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 7.6302 - mae: 1.9679 - val_loss: 23.2640 - val_mae: 3.0926\n",
      "Taxa atual = 0.00490\n",
      "Epoch 39/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 7.6193 - mae: 1.9748 - val_loss: 24.0490 - val_mae: 3.0977\n",
      "Taxa atual = 0.00470\n",
      "Epoch 40/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 7.5694 - mae: 1.9438 - val_loss: 23.3482 - val_mae: 3.0718\n",
      "Taxa atual = 0.00450\n",
      "Epoch 41/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 7.4949 - mae: 1.9470 - val_loss: 23.3571 - val_mae: 3.0678\n",
      "Taxa atual = 0.00430\n",
      "Epoch 42/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 7.5751 - mae: 1.9698 - val_loss: 24.0104 - val_mae: 3.1203\n",
      "Taxa atual = 0.00410\n",
      "Epoch 43/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 7.5106 - mae: 1.9379 - val_loss: 22.9066 - val_mae: 3.0254\n",
      "Taxa atual = 0.00390\n",
      "Epoch 44/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 7.3981 - mae: 1.9424 - val_loss: 23.3354 - val_mae: 3.0664\n",
      "Taxa atual = 0.00370\n",
      "Epoch 45/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 7.3400 - mae: 1.9366 - val_loss: 23.5260 - val_mae: 3.0674\n",
      "Taxa atual = 0.00350\n",
      "Epoch 46/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 7.3797 - mae: 1.9237 - val_loss: 23.2982 - val_mae: 3.0590\n",
      "Taxa atual = 0.00330\n",
      "Epoch 47/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 7.3028 - mae: 1.9392 - val_loss: 23.4771 - val_mae: 3.0693\n",
      "Taxa atual = 0.00310\n",
      "Epoch 48/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 7.3255 - mae: 1.9405 - val_loss: 23.7739 - val_mae: 3.0792\n",
      "Taxa atual = 0.00290\n",
      "Epoch 49/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 7.3127 - mae: 1.9238 - val_loss: 23.1459 - val_mae: 3.0436\n",
      "Taxa atual = 0.00280\n",
      "Epoch 50/50\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 7.2454 - mae: 1.9142 - val_loss: 23.3194 - val_mae: 3.0701\n"
     ]
    }
   ],
   "source": [
    "def scheduler(epoch, lr):\n",
    "    print(\"Taxa atual = %.5f\" % (lr))\n",
    "    if epoch < 10:\n",
    "        return lr\n",
    "    else:\n",
    "        return np.round(lr * tf.math.exp(-0.05),4)\n",
    "\n",
    "callbacklr = keras.callbacks.LearningRateScheduler(scheduler)\n",
    "\n",
    "model2.compile(optimizer=keras.optimizers.Adam(0.02),\n",
    "              loss='mse', metrics=['mae'])\n",
    "\n",
    "history = model2.fit(x_train, y_train, epochs=epochs,\n",
    "                     callbacks=[callbacklr],  validation_data=(x_test,y_test),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J_F2ODAmyqq4"
   },
   "source": [
    "---\n",
    "\n",
    "### Exercício 8)\n",
    "\n",
    "\n",
    "Utilizando ainda a biblioteca Keras, investige o impacto do uso de parâmetros padrão de batchsize na base de dados Boston Housing, agora utilizando a mesma arquitetura da atividade anterior, com otimizador Adam, iniciando com learning rate 0.02 e decaimento exponencial de 0.1 a partir da época 6.\n",
    "\n",
    "Investige valores de batch = 2, 4, 8, 16, 32, 64, 128 e 256 executando por um número de épocas proporcional ao tamanho do batch atual, calculado por:\n",
    "$epocas = \\lfloor \\log_2(512 \\cdot batchsize) \\rfloor$, valor deve ser convertido para inteiro. Esse número de épocas proporcional permite equilibrar um pouco a relação entre velocidade e quantidade de vezes que os parâmetros são adaptados.\n",
    "\n",
    "Antes de projetar, compilar e treinar cada rede, defina as sementes do numpy para 1 e do tensorflow para 2.\n",
    "\n",
    "Não passe dados de validação durante o treinamento. Após o treinamento, avalie MSE nos dados de teste e imprima para comparar os valores para diferentes batchsizes.\n",
    "\n",
    "Quais foram os dois piores e os dois melhores valores de tamanho de batch em termos do MSE de teste?\n",
    "\n",
    "<font color='red'>(a) Piores: 128 e 256; Melhores: 4 e 16<br></font>\n",
    "(b) Piores: 16 e 64; Melhores: 32 e 128<br>\n",
    "(c) Piores: 2 e 4; Melhores: 32 e 64<br>\n",
    "(d) Piores: 4 e 256; Melhores: 2 e 128<br>\n",
    "\n",
    "**Justificativa**: Notar no código abaixo a comparação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "-29F6a8kyqq5"
   },
   "outputs": [],
   "source": [
    "def my_dnn():\n",
    "    model2 = keras.Sequential()\n",
    "    model2 = keras.Sequential()\n",
    "    model2.add(keras.layers.Dense(16, activation=\"relu\", input_shape=(x_train.shape[1],)))\n",
    "    model2.add(keras.layers.Dense(8, activation=\"relu\"))\n",
    "    model2.add(keras.layers.Dense(1, activation=\"relu\"))\n",
    "    return model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "mOGqhWieyqq_"
   },
   "outputs": [],
   "source": [
    "def scheduler(epoch, lr):\n",
    "    if epoch < 6:\n",
    "        return lr\n",
    "    else:\n",
    "        return np.round(lr * tf.math.exp(-0.1),4)\n",
    "\n",
    "callbacklr = keras.callbacks.LearningRateScheduler(scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 171
    },
    "id": "61grHxROyqrD",
    "outputId": "6f52adfd-a53e-4a03-a3f4-02acb829d707"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size = 2, Erro de Validação MSE = 22.4658 (Epocas=10)\n",
      "Batch size = 4, Erro de Validação MSE = 19.1443 (Epocas=11)\n",
      "Batch size = 8, Erro de Validação MSE = 23.9285 (Epocas=12)\n",
      "Batch size = 16, Erro de Validação MSE = 21.6256 (Epocas=13)\n",
      "Batch size = 32, Erro de Validação MSE = 25.3148 (Epocas=14)\n",
      "Batch size = 64, Erro de Validação MSE = 23.7474 (Epocas=15)\n",
      "Batch size = 128, Erro de Validação MSE = 30.6913 (Epocas=16)\n",
      "Batch size = 256, Erro de Validação MSE = 129.7601 (Epocas=17)\n"
     ]
    }
   ],
   "source": [
    "batches = [2, 4, 8, 16, 32, 64, 128, 256]\n",
    "\n",
    "for batch_size in batches:\n",
    "    seed(1)\n",
    "    set_seed(2)\n",
    "    model = my_dnn()\n",
    "    model.compile(optimizer=keras.optimizers.Adam(0.02),\n",
    "                  loss='mse', metrics=['mae'])\n",
    "    \n",
    "    epochs = int(np.log2((batch_size*512)))\n",
    "    history = model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size,\n",
    "                         callbacks=[callbacklr], verbose=0)\n",
    "    \n",
    "    score = model.evaluate(x_test, y_test, verbose = 0)\n",
    "    print(\"Batch size = %d, Erro de Validação MSE = %.4f (Epocas=%d)\" % (batch_size, score[0], epochs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q3zlC0i0yqrX"
   },
   "source": [
    "---\n",
    "\n",
    "### Exercício 9)\n",
    "\n",
    "O que podemos concluir dos dois exercícios anteriores (7 e 8)?\n",
    "\n",
    "(a) Os valores padrão para os hiperparâmetros geram bons resultados. A busca por outros parâmetros pode não valer a pena pois a diferença alcançada observada é pequena.<br>\n",
    "(b) Devemos sempre utilizar Adam com decaimento de taxa de aprendizado e batch size de tamanho entre 8 e 64, sendo que o uso do padrão (32) é normalmente suficiente.<br>\n",
    "(c) Batchs de tamanho muito grande são prejudiciais ao treinamento, e o otimizador Adam é sempre melhor com decaimento de taxa de aprendizado.<br>\n",
    "<font color='red'>(d) O uso de hiperparâmetros com valores padrão pode gerar resultados subótimos, sendo importante uma busca de parâmetros para melhor otimizar modelos<br></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9OLlz13JyqrX"
   },
   "source": [
    "---\n",
    "\n",
    "### Exercício 10)\n",
    "\n",
    "Carregue a base de dados Fashion MNIST\n",
    "\n",
    "Crie duas redes neurais utilizando os blocos Residuais e módulos Inception conforme visto em aula.\n",
    "\n",
    "* InceptionNet\n",
    "    * Módulo Inception V1 com número de filtros: 32, 32, 32, 32, 32, 16\n",
    "    * Maxpooling com pool=2, stride=2\n",
    "    * Módulo Inception V1 com número de filtros: 32, 64, 64, 64, 64, 16\n",
    "    * Maxpooling com pool=2, stride=2\n",
    "* ResNet\n",
    "    * 3 blocos residuais com 64 filtros, cada um seguido por camada Maxpooling com pool=2, stride=2\n",
    "\n",
    "Ambos devem possuir uma camada `GlobalAveragePooling2D` antes da camada de predição.\n",
    "\n",
    "Treine ambas com SGD, learning rate 0.05 e momentum 0.8, utilizando batchsize 64, e apenas as 800 primeiras imagens do dataset de treinamento (use :800), por 100 épocas.\n",
    "\n",
    "Ao final compute a perda e a acurácia no treinamento (800 imagens) e teste (todas as imagens do teste), e exiba o gráfico da perda ao longo das épocas para as duas arquiteturas.\n",
    "\n",
    "Marque a alternativa que melhor se encaixa no resultado observado e sua conclusão.\n",
    "\n",
    "<font color='red'>(a) A ResNet conseguiu ajustar perfeitamente aos dados de treinamento, mas com perda mais alta calculada no teste, indicando overfitting, enquanto a Inception tem espaço para melhorias<br></font>\n",
    "(b) As duas arquiteturas obtiveram boa generalização, mas a Inception possui claras vantagens frente à ResNet, com maior acurácia no teste<br>\n",
    "(c) A Inception teve melhor generalização mas não convergiu para perda próxima a zero com 100 épocas, portanto a taxa de aprendizado escolhida poderia ser reduzida para obter resultados melhores, enquanto a ResNet poderia ser treinada por mais épocas para melhorar a acurácia no teste.<br>\n",
    "(d) Os resultados de acurácia e perda no teste da ResNet são muito diferentes daqueles obtidos no treinamento, indicando underfitting, ou seja, uma incapacidade do modelo de se ajustar aos dados utilizados no processo de aprendizado<br>\n",
    "\n",
    "**Justificativa**: Nenhuma arquitetura apresenta boa generalização, visto que tanto a perda quanto a acurácia diferem quando computadas no treinamento e no teste. O caso dessa diferença é um sinal de overfitting, e não de underfitting, já que o modelo foi capaz de se ajustar perfeitamente (com perda 0) para o treinamento. Além disso, reduzir a taxa de aprendizado torna a convergência mais lenta, e não mais rápida. Finalmente, quando um modelo alcança custo zero no treinamento, o gradiente também vai para zero e não há mais benefícios em treinar por mais épocas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "id": "RVYNMoriyqrZ",
    "outputId": "b866fc9c-2d4e-425b-e849-29222f900bdd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "Shape:  (28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "# carregando datasets do keras\n",
    "#from tensorflow.keras.datasets import mnist\n",
    "\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "# obtendo informações das imagens (resolucao) e dos rótulos (número de classes)\n",
    "img_lin, img_col = x_train.shape[1], x_train.shape[2]\n",
    "num_classes = len(np.unique(y_train))\n",
    "\n",
    "print(x_train.shape)\n",
    "\n",
    "# dividir por 255 para obter normalizacao\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "# transformar categorias em one-hot-encoding\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "# verifica imagens da base de dados tem 3 canais (RGB) ou apenas 1 (escala de cinza)\n",
    "if (len(x_train.shape) == 3):\n",
    "      n_channels = 1\n",
    "else:\n",
    "      n_channels = x_train.shape[3]\n",
    "\n",
    "# re-formata o array de forma a encontrar o formato da entrada (input_shape)\n",
    "# se a dimensão dos canais vem primeiro ou após a imagem\n",
    "if keras.backend.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], n_channels, img_lin, img_col)\n",
    "    x_test = x_test.reshape(x_test.shape[0], n_channels, img_lin, img_col)\n",
    "    input_shape = (n_channels, img_lin, img_col)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_lin, img_col, n_channels)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_lin, img_col, n_channels)\n",
    "    input_shape = (img_lin, img_col, n_channels)\n",
    "\n",
    "print(\"Shape: \", input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ZJ3ZeShByqrd",
    "outputId": "df1e6005-26e5-4c85-87b2-53d45dfc8354"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 28, 28, 1)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 28, 28, 32)   64          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 28, 28, 32)   64          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 28, 28, 1)    0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 28, 28, 32)   64          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 28, 28, 32)   9248        conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 28, 28, 32)   25632       conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 28, 28, 16)   32          max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 28, 28, 112)  0           conv2d[0][0]                     \n",
      "                                                                 conv2d_2[0][0]                   \n",
      "                                                                 conv2d_4[0][0]                   \n",
      "                                                                 conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 14, 14, 112)  0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 14, 14, 64)   7232        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 14, 14, 64)   7232        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 14, 14, 112)  0           max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 14, 14, 32)   3616        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 14, 14, 64)   36928       conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 14, 14, 64)   102464      conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 14, 14, 16)   1808        max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 14, 14, 176)  0           conv2d_6[0][0]                   \n",
      "                                                                 conv2d_8[0][0]                   \n",
      "                                                                 conv2d_10[0][0]                  \n",
      "                                                                 conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 7, 7, 176)    0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d (Globa (None, 176)          0           max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_30 (Dense)                (None, 10)           1770        global_average_pooling2d[0][0]   \n",
      "==================================================================================================\n",
      "Total params: 196,154\n",
      "Trainable params: 196,154\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.layers import add\n",
    "\n",
    "def inception_module(layer_in, f1_out, f2_in, f2_out, f3_in, f3_out, f4_out):\n",
    "    # 1x1 conv\n",
    "    conv1 = Conv2D(f1_out, (1,1), padding='same', activation='relu')(layer_in)\n",
    "    # 3x3 conv\n",
    "    conv3 = Conv2D(f2_in, (1,1), padding='same', activation='relu')(layer_in)\n",
    "    conv3 = Conv2D(f2_out, (3,3), padding='same', activation='relu')(conv3)\n",
    "    # 5x5 conv\n",
    "    conv5 = Conv2D(f3_in, (1,1), padding='same', activation='relu')(layer_in)\n",
    "    conv5 = Conv2D(f3_out, (5,5), padding='same', activation='relu')(conv5)\n",
    "    # 3x3 max pooling\n",
    "    pool = MaxPooling2D((3,3), strides=(1,1), padding='same')(layer_in)\n",
    "    pool = Conv2D(f4_out, (1,1), padding='same', activation='relu')(pool)\n",
    "    layer_out = concatenate([conv1, conv3, conv5, pool], axis=-1)\n",
    "    return layer_out\n",
    " \n",
    "# define model input\n",
    "input_layer = Input(shape=input_shape)\n",
    "# add inception blocks\n",
    "layer1 = inception_module(input_layer, 32, 32, 32, 32, 32, 16)\n",
    "pool1 = MaxPooling2D((2,2), strides=(2,2), padding='same')(layer1) #\n",
    "layer2 = inception_module(pool1, 32, 64, 64, 64, 64, 16) # rem?\n",
    "pool2 = MaxPooling2D((2,2), strides=(2,2), padding='same')(layer2)\n",
    "flatt = keras.layers.GlobalAveragePooling2D()(pool2)\n",
    "\n",
    "softmax = keras.layers.Dense(num_classes, activation='softmax')(flatt)\n",
    "\n",
    "# create model\n",
    "Inception = keras.models.Model(inputs=input_layer, outputs=softmax)\n",
    "# summarize model\n",
    "Inception.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 959
    },
    "id": "caop8lv8yqrh",
    "outputId": "1120a3ed-15e2-4a99-ea26-d61fcb8a4a70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 28, 28, 1)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 28, 28, 64)   640         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 28, 28, 64)   36928       conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 28, 28, 64)   128         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 28, 28, 64)   0           conv2d_14[0][0]                  \n",
      "                                                                 conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 28, 28, 64)   0           add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 14, 14, 64)   0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 14, 14, 64)   36928       max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 14, 14, 64)   36928       conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 14, 14, 64)   0           conv2d_16[0][0]                  \n",
      "                                                                 max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 14, 14, 64)   0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 7, 7, 64)     0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 7, 7, 64)     36928       max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 7, 7, 64)     36928       conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 7, 7, 64)     0           conv2d_18[0][0]                  \n",
      "                                                                 max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 7, 7, 64)     0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 4, 4, 64)     0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 64)           0           max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_31 (Dense)                (None, 10)           650         global_average_pooling2d_1[0][0] \n",
      "==================================================================================================\n",
      "Total params: 186,058\n",
      "Trainable params: 186,058\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def residual_block(layer_in, n_filters):\n",
    "    merge_input = layer_in\n",
    "    #verifica se é necessária uma primeira camada para deixar o número de filtros iguais para adição\n",
    "    if layer_in.shape[-1] != n_filters:\n",
    "        merge_input = Conv2D(n_filters, (1,1), padding='same', activation='relu', kernel_initializer='he_normal')(layer_in)\n",
    "    # conv1\n",
    "    conv1 = Conv2D(n_filters, (3,3), padding='same', activation='relu', kernel_initializer='he_normal')(layer_in)\n",
    "    # conv2\n",
    "    conv2 = Conv2D(n_filters, (3,3), padding='same', activation='linear', kernel_initializer='he_normal')(conv1)\n",
    "    # soma entrada com saída (pulou 2 camadas)\n",
    "    layer_out = add([conv2, merge_input])\n",
    "    # função de ativação da saída do bloco\n",
    "    layer_out = keras.layers.Activation('relu')(layer_out)\n",
    "    return layer_out\n",
    " \n",
    "# define model input\n",
    "visible = Input(shape=input_shape)\n",
    "\n",
    "layer1 = residual_block(visible, 64)\n",
    "pool1 = MaxPooling2D((2,2), strides=(2,2), padding='same')(layer1)\n",
    "layer2 = residual_block(pool1, 64)\n",
    "pool2 = MaxPooling2D((2,2), strides=(2,2), padding='same')(layer2)\n",
    "layer3 = residual_block(pool2, 64)\n",
    "pool3 = MaxPooling2D((2,2), strides=(2,2), padding='same')(layer3)\n",
    "flatt = keras.layers.GlobalAveragePooling2D()(pool3)\n",
    "softmax = keras.layers.Dense(num_classes, activation='softmax')(flatt)\n",
    "\n",
    "# create model\n",
    "ResNet = keras.models.Model(inputs=visible, outputs=softmax)\n",
    "# summarize model\n",
    "ResNet.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "ey3iyaJFyqrk"
   },
   "outputs": [],
   "source": [
    "x_sub = x_train[:800]\n",
    "y_sub = y_train[:800]\n",
    "batch_size = 64\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "ZsboR6ZTyqrp"
   },
   "outputs": [],
   "source": [
    "# as sementes ajudam a ter resultados reproduzíveis\n",
    "#tf.keras.backend.clear_session()\n",
    "seed(1)\n",
    "set_seed(2)\n",
    "\n",
    "Inception.compile(loss='categorical_crossentropy',\n",
    "              optimizer=keras.optimizers.SGD(lr=0.05, momentum=0.8),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "histInc = Inception.fit(x_sub, y_sub,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "InGNAeBOyqrs"
   },
   "outputs": [],
   "source": [
    "# as sementes ajudam a ter resultados reproduzíveis\n",
    "#tf.keras.backend.clear_session()\n",
    "seed(1)\n",
    "set_seed(2)\n",
    "\n",
    "ResNet.compile(loss='categorical_crossentropy',\n",
    "              optimizer=keras.optimizers.SGD(lr=0.05, momentum=0.8),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "histResNet = ResNet.fit(x_sub, y_sub,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "ICUijS3gyqrw"
   },
   "outputs": [],
   "source": [
    "score1T = Inception.evaluate(x_sub, y_sub, verbose = 0)\n",
    "score2T = ResNet.evaluate(x_sub, y_sub, verbose = 0)\n",
    "\n",
    "score1 = Inception.evaluate(x_test, y_test, verbose = 0)\n",
    "score2 = ResNet.evaluate(x_test, y_test, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 94
    },
    "id": "VK0Nrv5kyqr1",
    "outputId": "87adf110-15f4-4be3-cc72-ea64519a48b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inception Treinamento = Loss 0.345, Accuracy 0.858\n",
      "Inception Teste       = Loss 0.715, Accuracy 0.758\n",
      "ResNet Treinamento    = Loss 0.000, Accuracy 1.000\n",
      "ResNet Teste          = Loss 1.515, Accuracy 0.800\n"
     ]
    }
   ],
   "source": [
    "print(\"Inception Treinamento = Loss %.3f, Accuracy %.3f\" % (score1T[0], score1T[1]))\n",
    "print(\"Inception Teste       = Loss %.3f, Accuracy %.3f\" % (score1[0], score1[1]))\n",
    "\n",
    "print(\"ResNet Treinamento    = Loss %.3f, Accuracy %.3f\" % (score2T[0], score2T[1]))\n",
    "print(\"ResNet Teste          = Loss %.3f, Accuracy %.3f\" % (score2[0], score2[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 284
    },
    "id": "0bJI1XO5XI2Y",
    "outputId": "7da316e7-be37-45cb-a1f7-363a758b25d1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f4bd82f4790>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(histResNet.history['loss'])\n",
    "plt.plot(histInc.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SZr1QQF2XYEJ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "RNAP-03-Exercicios_solucoes.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
