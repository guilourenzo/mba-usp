{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "flekT6GFDN6m"
   },
   "source": [
    "# <span style=\"color:blue\">MBA em Ciência de Dados</span>\n",
    "# <span style=\"color:blue\">Análise de Dados com Base em Processamento Massivo em Paralelo</span>\n",
    "\n",
    "## <span style=\"color:blue\">Avaliação Final</span>\n",
    "\n",
    "**Material Produzido por:**<br>\n",
    ">**Profa. Dra. Cristina Dutra de Aguiar Ciferri**<br>\n",
    ">**André Perez**<br> \n",
    ">**Guilherme Muzzi da Rocha**<br> \n",
    ">**Jadson José Monteiro Oliveira**<br>\n",
    ">**João Pedro de Carvalho Castro**<br> \n",
    ">**Leonardo Mauro Pereira Moraes**<br> \n",
    ">**Piero Lima Capelo**<br>\n",
    "\n",
    "\n",
    "**CEMEAI - ICMC/USP São Carlos**\n",
    "\n",
    "**A avaliação final contém 7 questões, as quais estão espalhadas ao longo do texto. Por favor, procurem por Questão para encontrar a especificação das questões. Também é possível localizar as questões utilizando o menu de navegação. O *notebook* contém a constelação de fatos da BI Solutions que deve ser utilizada para responder às questões e também toda a obtenção dos dados e a respectiva geração dos DataFrames e das visões temporárias.** \n",
    "\n",
    "**Desejamos uma boa avaliação!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3o3dN_WLQcyD"
   },
   "source": [
    "#1 Constelação de Fatos da BI Solutions\n",
    "\n",
    "A aplicação de *data warehousing* da BI Solutions utiliza como base uma contelação de fatos, conforme descrita a seguir.\n",
    "\n",
    "**Tabelas de dimensão**\n",
    "\n",
    "- data (dataPK, dataCompleta, dataDia, dataMes, dataBimestre, dataTrimestre, dataSemestre, dataAno)\n",
    "- funcionario (funcPK, funcMatricula, funcNome, funcSexo, funcDataNascimento, funcDiaNascimento, funcMesNascimento, funcAnoNascimento, funcCidade, funcEstadoNome, funcEstadoSigla, funcRegiaoNome, funcRegiaoSigla, funcPaisNome, funcPaisSigla)\n",
    "- equipe (equipePK, equipeNome, filialNome, filialCidade, filialEstadoNome, filialEstadoSigla, filialRegiaoNome, filialRegiaoSigla, filialPaisNome, filialPaisSigla)\n",
    "- cargo (cargoPK, cargoNome, cargoRegimeTrabalho, cargoEscolaridadeMinima, cargoNivel)\n",
    "- cliente (clientePK, clienteNomeFantasia, clienteSetor, clienteCidade, clienteEstadoNome, clienteEstadoSigla, clienteRegiaoNome, clienteRegiaoSigla, clientePaisNome, clientePaisSigla)\n",
    "\n",
    "**Tabelas de fatos**\n",
    "- pagamento (dataPK, funcPK, equipePK, cargoPK, salario, quantidadeLancamentos)\n",
    "- negociacao (dataPK, equipePK, clientePK, receita, quantidadeNegociacoes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BGeh8KdXwVCQ"
   },
   "source": [
    "#2 Obtenção dos Dados da BI Solutions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CCCNC64AzBG0"
   },
   "source": [
    "## 2.1 Baixando o Módulo wget\n",
    "\n",
    "Para baixar os dados referentes ao esquema relacional da constelação de fatos da BI Solutions, é utilizado o módulo  **wget**. O comando a seguir realiza a instalação desse módulo. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3e0Eao1K0EYG"
   },
   "outputs": [],
   "source": [
    "#instalando o módulo wget\n",
    "# %%capture\n",
    "!pip install -q wget\n",
    "!mkdir data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j56pVJ2hZ2i5"
   },
   "source": [
    "## 2.2 Obtenção dos Dados das Tabelas de Dimensão\n",
    "\n",
    "Os comandos a seguir baixam os dados que povoam as tabelas de dimensão. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "46QzTpLJwfkW",
    "outputId": "0fc40ddf-6989-44b7-e033-1ee63d6db1ce"
   },
   "outputs": [],
   "source": [
    "#baixando os dados das tabelas de dimensão\n",
    "import wget\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/GuiMuzziUSP/Data_Mart_BI_Solutions/main/data.csv\"\n",
    "wget.download(url, \"data/data.csv\")\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/GuiMuzziUSP/Data_Mart_BI_Solutions/main/funcionario.csv\"\n",
    "wget.download(url, \"data/funcionario.csv\")\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/GuiMuzziUSP/Data_Mart_BI_Solutions/main/equipe.csv\"\n",
    "wget.download(url, \"data/equipe.csv\")\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/GuiMuzziUSP/Data_Mart_BI_Solutions/main/cargo.csv\"\n",
    "wget.download(url, \"data/cargo.csv\")\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/GuiMuzziUSP/Data_Mart_BI_Solutions/main/cliente.csv\"\n",
    "wget.download(url, \"data/cliente.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0o-dC7feszRc"
   },
   "source": [
    "## 2.3 Obtenção dos Dados Tabelas de Fatos\n",
    "\n",
    "Os comandos a seguir baixam os dados que povoam as tabelas de fatos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "XWM-CUFgBl_8",
    "outputId": "575ff48d-5284-4aeb-ff8b-f4766b2691ea"
   },
   "outputs": [],
   "source": [
    "#baixando os dados das tabelas de fatos\n",
    "url = \"https://raw.githubusercontent.com/GuiMuzziUSP/Data_Mart_BI_Solutions/main/pagamento.csv\"\n",
    "wget.download(url, \"data/pagamento.csv\")\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/GuiMuzziUSP/Data_Mart_BI_Solutions/main/negociacao.csv\"\n",
    "wget.download(url, \"data/negociacao.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sO16-7-jOioq"
   },
   "source": [
    "# 3 Apache Spark Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YVEgY9qKflBV"
   },
   "source": [
    "## 3.1 Instalação\n",
    "\n",
    "Neste *notebook* é criado um *cluster* Spark composto apenas por um **nó mestre**. Ou seja, o *cluster* não possui um ou mais **nós de trabalho** e o **gerenciador de cluster**. Nessa configuração, as tarefas (*tasks*) são realizadas no próprio *driver* localizado no **nó mestre**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KaM-OnIjgLS2"
   },
   "source": [
    "Para que o cluster possa ser criado, primeiramente é instalado o Java Runtime Environment (JRE) versão 8. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NXls3bfoglKW"
   },
   "outputs": [],
   "source": [
    "#instalando Java Runtime Environment (JRE) versão 8\n",
    "%%capture\n",
    "!apt-get remove openjdk*\n",
    "!apt-get update --fix-missing\n",
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7BQzZfDYhb4j"
   },
   "source": [
    "Na sequência, é feito o *download* do Apache Spark versão 3.0.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8a_Yv59zg3gm"
   },
   "outputs": [],
   "source": [
    "#baixando Apache Spark versão 3.0.0\n",
    "%%capture\n",
    "!wget -q https://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop2.7.tgz\n",
    "!tar xf spark-3.0.0-bin-hadoop2.7.tgz && rm spark-3.0.0-bin-hadoop2.7.tgz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RETWX6wqhkLf"
   },
   "source": [
    "Na sequência, são configuradas as variáveis de ambiente JAVA_HOME e SPARK_HOME. Isto permite que tanto o Java quanto o Spark possam ser encontrados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iZpR7NwOh2EB"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "#configurando a variável de ambiente JAVA_HOME\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "#configurando a variável de ambiente SPARK_HOME\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-bin-hadoop2.7\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ql0z7Ro1iHQb"
   },
   "source": [
    "Por fim, são instalados dois pacotes da linguagem de programação Python, cujas funcionalidades são descritas a seguir.\n",
    "\n",
    "> **Pacote findspark:** Usado para ler a variável de ambiente SPARK_HOME e armazenar seu valor na variável dinâmica de ambiente PYTHONPATH. Como resultado, Python pode encontrar a instalação do Spark. \n",
    "\n",
    "> **Pacote pyspark:** PySpark é a API do Python para Spark. Ela possibilita o uso de Python, considerando que o *framework* Apache Spark encontra-se desenvolvido na linguagem de programação Scala. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T18:07:21.635133Z",
     "start_time": "2020-11-16T18:07:11.327300Z"
    },
    "id": "5oSYOwKljPf5"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "#instalando o pacote findspark\n",
    "!pip install -q findspark==1.4.2\n",
    "#instalando o pacote pyspark\n",
    "!pip install -q pyspark==3.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eAaLyjPzmIwZ"
   },
   "source": [
    "## 3.2 Conexão\n",
    "\n",
    "PySpark não é adicionado ao *sys.path* por padrão. Isso significa que não é possível importá-lo, pois o interpretador da linguagem Python não sabe onde encontrá-lo. \n",
    "\n",
    "Para resolver esse aspecto, é necessário instalar o módulo `findspark`. Esse módulo mostra onde PySpark está localizado. Os comandos a seguir têm essa finalidade.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T21:15:51.653161Z",
     "start_time": "2020-11-16T21:15:51.621887Z"
    },
    "id": "-zm1pBTEmjp4"
   },
   "outputs": [],
   "source": [
    "#importando o módulo findspark\n",
    "import findspark\n",
    "#carregando a variávels SPARK_HOME na variável dinâmica PYTHONPATH\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZDqfefF7YUab"
   },
   "source": [
    "Depois de configurados os pacotes e módulos e inicializadas as variáveis de ambiente, é possível iniciar o uso do Spark na aplicação de `data warehousing`. Para tanto, é necessário importar o comando `SparkSession` do módulo `pyspark.sql`. São utilizados os seguintes conceitos: <br>\n",
    "\n",
    "- `SparkSession`: permite a criação de `DataFrames`. Como resultado, as tabelas relacionais podem ser manipuladas por meio de `DataFrames` e é possível realizar consultas OLAP por meio de comandos SQL. <br>\n",
    "- `builder`: cria uma instância de SparkSession. <br>\n",
    "- `appName`: define um nome para a aplicação, o qual pode ser visto na interface de usuário web do Spark. <br> \n",
    "- `master`: define onde está o nó mestre do *cluster*. Como a aplicação é executada localmente e não em um *cluster*, indica-se isso pela *string* `local` seguida do parâmetro `[*]`. Ou seja, define-se que apenas núcleos locais são utilizados. \n",
    "- `getOrCreate`: cria uma SparkSession. Caso ela já exista, retorna a instância existente. \n",
    "\n",
    "\n",
    "**Observação**: A lista completa de todos os parâmetros que podem ser utilizados na inicialização do *cluster* pode ser encontrada neste [link](https://spark.apache.org/docs/latest/spark-standalone.html#cluster-launch-scripts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T21:15:59.490028Z",
     "start_time": "2020-11-16T21:15:53.428274Z"
    },
    "id": "9TxljJ_cwBCy"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"pyspark-notebook\").master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5qL9SiR_pQE2"
   },
   "source": [
    "# 4 Geração dos DataFrames em Spark da BI Solutions\n",
    "\n",
    "Um `DataFrame` em Spark é equivalente a uma tabela relacional. Portanto, um `DataFrame` possui um esquema, uma ou mais linhas (ou tuplas) e uma ou mais colunas (ou atributos).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DRVoz-SGt87W"
   },
   "source": [
    "## 4.1 Criação dos DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T21:16:04.487158Z",
     "start_time": "2020-11-16T21:15:59.490028Z"
    },
    "cellView": "both",
    "id": "FNR-3dV6oYk4"
   },
   "outputs": [],
   "source": [
    "#criando os DataFrames em Spark \n",
    "cargo = spark.read.csv(path=\"data/cargo.csv\", header=True, sep=\",\")\n",
    "cliente = spark.read.csv(path=\"data/cliente.csv\", header=True, sep=\",\")\n",
    "data = spark.read.csv(path=\"data/data.csv\", header=True, sep=\",\")\n",
    "equipe = spark.read.csv(path=\"data/equipe.csv\", header=True, sep=\",\")\n",
    "funcionario = spark.read.csv(path=\"data/funcionario.csv\", header=True, sep=\",\")\n",
    "negociacao = spark.read.csv(path=\"data/negociacao.csv\", header=True, sep=\",\")\n",
    "pagamento = spark.read.csv(path=\"data/pagamento.csv\", header=True, sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mrch9vLgjl_H"
   },
   "source": [
    "## 4.2 Atualização dos Tipos de Dados "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9A_ot2pOjsWB"
   },
   "source": [
    "Nos comandos a seguir, primeiro são identificados quais colunas de quais `DataFrames` devem ser do tipo de dado inteiro. Na sequência, ocorre a conversão. Por fim, são exibidos os esquemas dos `DataFrames`, possibilitando visualizar a mudança de tipo de dados das colunas especificadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T21:16:04.502779Z",
     "start_time": "2020-11-16T21:16:04.487158Z"
    },
    "id": "jmCV6Mur__z6"
   },
   "outputs": [],
   "source": [
    "# identificando quais colunas de quais DataFrames devem ser do tipo de dado inteiro\n",
    "colunas_cargo = [\"cargoPK\"]\n",
    "colunas_cliente = [\"clientePK\"]\n",
    "colunas_data = [\"dataPk\", \"dataDia\", \"dataMes\", \"dataBimestre\", \"dataTrimestre\", \"dataSemestre\", \"dataAno\"]\n",
    "colunas_equipe = [\"equipePK\"]\n",
    "colunas_funcionario = [\"funcPK\", \"funcDiaNascimento\", \"funcMesNascimento\", \"funcAnoNascimento\"]\n",
    "colunas_negociacao = [\"equipePK\", \"clientePK\", \"dataPK\", \"quantidadeNegociacoes\"]\n",
    "colunas_pagamento = [\"funcPK\", \"equipePK\", \"dataPK\", \"cargoPK\", \"quantidadeLancamentos\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T21:16:04.734502Z",
     "start_time": "2020-11-16T21:16:04.502779Z"
    },
    "id": "yPNnDJcG9R5H"
   },
   "outputs": [],
   "source": [
    "# importando o tipo de dado desejado\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "\n",
    "# atualizando o tipo de dado das colunas especificadas \n",
    "# substituindo as colunas já existentes \n",
    "\n",
    "for coluna in colunas_cargo:\n",
    "    cargo = cargo.withColumn(coluna, cargo[coluna].cast(IntegerType()))\n",
    "\n",
    "for coluna in colunas_cliente:\n",
    "    cliente = cliente.withColumn(coluna, cliente[coluna].cast(IntegerType()))\n",
    "\n",
    "for coluna in colunas_data:\n",
    "    data = data.withColumn(coluna, data[coluna].cast(IntegerType()))\n",
    "\n",
    "for coluna in colunas_equipe:\n",
    "    equipe = equipe.withColumn(coluna, equipe[coluna].cast(IntegerType()))\n",
    "\n",
    "for coluna in colunas_funcionario:\n",
    "    funcionario = funcionario.withColumn(coluna, funcionario[coluna].cast(IntegerType()))\n",
    "\n",
    "for coluna in colunas_negociacao:\n",
    "    negociacao = negociacao.withColumn(coluna, negociacao[coluna].cast(IntegerType()))\n",
    "\n",
    "for coluna in colunas_pagamento:\n",
    "    pagamento = pagamento.withColumn(coluna, pagamento[coluna].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J0dX_7U_AzIY"
   },
   "source": [
    "Nos comandos a seguir, primeiro são identificados quais colunas de quais `DataFrames` devem ser do tipo de dado número de ponto flutuante. Na sequência, ocorre a conversão. Por fim, são exibidos os esquemas dos `DataFrames`, possibilitando visualizar a mudança de tipo de dados das colunas especificadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T21:16:04.750127Z",
     "start_time": "2020-11-16T21:16:04.734502Z"
    },
    "id": "RBcQ7Ep7AWqN"
   },
   "outputs": [],
   "source": [
    "# identificando quais colunas de quais DataFrames devem ser do tipo de dado número de ponto flutuante\n",
    "colunas_negociacao = [\"receita\"]\n",
    "colunas_pagamento = [\"salario\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T21:16:04.772262Z",
     "start_time": "2020-11-16T21:16:04.750127Z"
    },
    "id": "rcfvkIK1BRSp"
   },
   "outputs": [],
   "source": [
    "# importando o tipo de dado desejado\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "\n",
    "# atualizando o tipo de dado das colunas especificadas \n",
    "# substituindo as colunas já existentes \n",
    "\n",
    "for coluna in colunas_negociacao:\n",
    "    negociacao = negociacao.withColumn(coluna, negociacao[coluna].cast(FloatType()))\n",
    "\n",
    "for coluna in colunas_pagamento:\n",
    "    pagamento = pagamento.withColumn(coluna, pagamento[coluna].cast(FloatType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9wN5iOGKwnHG"
   },
   "source": [
    "## 4.3 Criação de Visões Temporárias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T21:16:05.066923Z",
     "start_time": "2020-11-16T21:16:04.772262Z"
    },
    "id": "xJsqRI3TwsjS"
   },
   "outputs": [],
   "source": [
    "#criando as visões temporárias \n",
    "cargo.createOrReplaceTempView(\"cargo\")\n",
    "cliente.createOrReplaceTempView(\"cliente\")\n",
    "data.createOrReplaceTempView(\"data\")\n",
    "equipe.createOrReplaceTempView(\"equipe\")\n",
    "funcionario.createOrReplaceTempView(\"funcionario\")\n",
    "negociacao.createOrReplaceTempView(\"negociacao\")\n",
    "pagamento.createOrReplaceTempView(\"pagamento\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WkL4w2MMudL7"
   },
   "source": [
    "# 5 Instruções Importantes sobre a Avaliação\n",
    "\n",
    "Esta avaliação é composta por 7 questões referentes a diferentes consultas OLAP. O valor de cada questão encontra-se especificado juntamente com a definição da questão. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uKdDsHnDwlaH"
   },
   "source": [
    "## 5.1 Especificação das Consultas\n",
    "\n",
    "As consultas OLAP devem ser respondidas de acordo com o solicitado em cada questão. As seguintes solicitações podem ser feitas:\n",
    "\n",
    "- Resolva a questão especificando a consulta OLAP na **linguagem SQL**. Neste caso, a consulta deve ser respondida usando os conceitos apresentados na Aula 07 da disciplina. Ou seja, a consulta deve ser respondida usando a linguagem SQL textual e o método `spark.sql()`. Não é possível usar os demais métodos do módulo `pyspark.sql` para especificar a consulta, com exceção do método `show()` para listar o resultado da consulta.  \n",
    "\n",
    "- Resolva a questão especificando a consulta OLAP usando os **métodos de pyspark.sql**. Neste caso, a consulta deve ser respondida usando os conceitos apresentados na Aula 08 da disciplina. Ou seja, a consulta deve ser respondida usando os demais métodos do módulo `pyspark.sql`. Não é possível usar o método `spark.sql()` para especificar a consulta.\n",
    "\n",
    "Caso a consulta seja especificada de forma diferente do que foi solicitado, a resposta não será considerada, mesmo que ela esteja correta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EsrYR2SJw-x3"
   },
   "source": [
    "## 5.2 Ordem das Colunas e das Linhas\n",
    "\n",
    "A resolução das questões deve seguir estritamente as especificações definidas em cada consulta. Isto significa que:\n",
    "\n",
    "- As **colunas** solicitadas devem ser exibidas exatamente na mesma ordem que a definida na questão. Note que todas as colunas a serem exibidas como resposta da consulta, bem como a ordem na qual elas devem aparecer são sempre definidas na questão. \n",
    "\n",
    "- As **linhas** retornadas como respostas devem ser exibidas exatamente na mesma ordem que a definida na questão. Note que a ordem na qual as linhas devem aparecer são sempre definidas na questão. \n",
    "\n",
    "- Os **nomes das colunas** renomeadas devem seguir estritamente os nomes definidos na questão. Para evitar possíveis erros, os nomes das colunas renomeadas não possuem acentos e espaços em branco, além de serem escritos utilizando apenas letras maiúsculas. Note que os nomes das colunas renomeadas são sempre definidos na questão.\n",
    "\n",
    "Essas orientações devem ser seguidas uma vez que a correção da avaliação será realizada de forma automática. Caso a consulta retorne resultados de forma diferente do que foi solicitado, a resposta não será considerada, mesmo que ela esteja correta.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3AZ0475X4L59"
   },
   "source": [
    "## 5.3 Listagem das Respostas das Consultas\n",
    "\n",
    "A resposta de cada consulta deve ser listada usando o método `show()`. Nenhum outro método pode ser utilizado com essa finalidade.  \n",
    "\n",
    "Devem ser listadas apenas as `20` primeiras linhas de resposta de cada consulta. Adicionalmente, devem ser listadas *strings* com tamanho maior do que 20 caracteres, ou seja, o parâmetro `truncate` do método `show()` deve ser inicializado como `false`.\n",
    "\n",
    "Portanto, a listagem das respostas deve ser feita utilizando o método `show()` como especificado a seguir. \n",
    "\n",
    "- Quando a consulta OLAP for especificada usando a **linguagem SQL**. Utilize o comando `spark.sql(consultaSQL).show(20,truncate=False)` para exibir o resultado da consulta. \n",
    "\n",
    "- Quando a consulta OLAP for especificada usando os demais **métodos de pyspark.sql**. Utilize o comando `nomeDoDataFrame.show(20,truncate=False)` para exibir o resultado da consulta.\n",
    "\n",
    "Por padrão, o método `show()` exibe as `20` primeiras linhas. Mesmo assim, defina o valor `20` como parâmetro."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VzIcWYeOVOsN"
   },
   "source": [
    "## 5.4 Arredondamento dos Dados\n",
    "\n",
    "Deve ser realizado o arredondamento dos dados todas as vezes que uma função de agregação for aplicada às medidas numéricas `salario` da tabela de dimensão `pagamento` e `receita` da tabela de dimensão `negociacao`. \n",
    "\n",
    "O arredondamento deve ser realizado usando a função `round()` na linguagem SQL e o método `round()` em `pyspark.sql` e deve arredondar os dados até duas casas decimais. Por exemplo, podem ser produzidos resultados da forma `112233.4` e `112233.44`. \n",
    "\n",
    "Portanto, o arredondamento dos dados deve ser feito como especificado a seguir.\n",
    "\n",
    "- Quando a consulta OLAP for especificada usando a **linguagem SQL**. Utilize a função `ROUND(funçãoDeAgregação,2)` para arredondar o dado até duas casas decimais.\n",
    "\n",
    "- Quando a consulta OLAP for especificada usando os demais **métodos de pyspark.sql**. Utilize o método `round(funçãoDeAgregação,2)` para arredondar o dado até duas casas decimais."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DQVQqm0pNDS1"
   },
   "source": [
    "## 5.5 Comentários Explicativos\n",
    "\n",
    "Devem ser colocados comentários no código que expliquem o passo a passo da resolução da questão. Os comentários explicativos devem ser realizados como especificado a seguir. \n",
    "\n",
    "- Quando a consulta OLAP for especificada usando a **linguagem SQL**. Utilize `#` para colocar comentários gerais (conforme explicado para os demais métodos de `pyspark.sql`) ou utilize `--` para colocar comentários no comando SQL. Por exemplo:\n",
    "\n",
    "```\n",
    "-- na cláusula SELECT são listadas as colunas a serem exibidas\n",
    "SELECT funcNome\n",
    "-- na cláusula FROM são especificadas as relações temporárias\n",
    "FROM funcionario\n",
    "-- na cláusula WHERE são definidas as condições de seleção\n",
    "WHERE funcPK = 1\n",
    "```\n",
    "\n",
    "- Quando a consulta OLAP for especificada usando os demais **métodos de pyspark.sql**. Utilize `#` para colocar comentário. Por exemplo:\n",
    "\n",
    "```\n",
    "# no comando a seguir, é aplicado o método select() sobre o DataFrame \n",
    "# para listar as colunas a serem exibidas, depois é aplicado o método\n",
    "# filter() para listar as condições de seleção  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6IcbpvZHPn3S"
   },
   "source": [
    "## 5.6 Indentação e Organização\n",
    "\n",
    "As consultas e os comandos que respondem às questões dessa avaliação devem ser escritos de forma indentada. Em caso de dúvida, observem os *notebooks* da Aula 07 e da Aula 08 e verifiquem como as consultas e os comandos foram indentados.\n",
    "\n",
    "Com relação à organização, é necessário que as respostas às questões sejam localizadas aonde especificado no *notebook*. Por favor, procurem por \"Resposta da Questão\" para encontrar o local no qual as respostas devem ser especificadas. Também é possível localizar o local das respostas utilizando o menu de navegação."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uFjCivy1Mg-A"
   },
   "source": [
    "## 5.7 Critério de Avaliação\n",
    "\n",
    "Na correção da avaliação, serão ponderados os seguintes aspectos:\n",
    "\n",
    "- Corretude da execução das consultas OLAP.\n",
    "\n",
    "- Atendimento às especificações definidas nas seções 5.1, 5.2, 5.3 e 5.4.\n",
    "\n",
    "- Atendimento às especificações da sintaxe das cláusulas e dos métodos utilizados para resolver cada questão.\n",
    "\n",
    "- Qualidade da documentação entregue, de acordo com as especificações definidas nas seções 5.5 e 5.6. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ss0pmgplPAL3"
   },
   "source": [
    "# 6 Questões"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_emI31_-uYc5"
   },
   "source": [
    "O mercado de trabalho brasileiro usualmente mostra que as mulheres ainda não possuem o mesmo reconhecimento que os homens. Existem diversas pesquisas que mostram que as mulheres ganham menos que homens em todos os cargos, áreas de atuação e níveis de escolaridade. Além disso, mulheres ainda são minoria quando consideradas posições nos principais cargos de gestão. Adicionalmente, existem estudos que indicam que a participação feminina no mercado de trabalho brasileiro aumenta a produtividade. \n",
    "\n",
    "O objetivo da avaliação é investigar se existe disparidade entre o sexo feminino e masculino na BI Solutions. São considerados os seguintes aspectos nas análises a serem realizadas: temporalidade e regionalidade. \n",
    "\n",
    "Os resultados obtidos na avaliação poderão ser posteriormente utilizados para definir estratégias que a BI Solutions deve executar para resolver essa disparidade, caso necessário. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nLD9FAdWK-Sp"
   },
   "source": [
    "## 6.1 Visão Comparativa Relacionada aos Sexos\n",
    "\n",
    "O objetivo das análises desta seção é obter uma visão relacionada aos sexos, por meio da comparação da média dos salários recebidos por mulheres e homens. Podem ser realizadas diferentes análises, sendo que duas delas são solicitadas a seguir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3kUBwA4WKmQ_"
   },
   "source": [
    "### Questão 1 \n",
    "\n",
    "**(valor: 1,0)** Liste, para cada `dataAno` e para cada sexo do funcionário, a média dos salários. Arredonde a média dos salários para até duas casas decimais. Devem ser exibidas as colunas na ordem e com os nomes especificados a seguir: \"ANO\", \"SEXO\" e \"MEDIASALARIO\". Ordene as linhas exibidas primeiro por ano e depois por sexo, todos em ordem ascendente. Liste as primeiras 20 linhas da resposta, sem truncamento das *strings*.\n",
    "\n",
    "**Resolva a questão especificando a consulta OLAP na linguagem SQL**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PsWybehcoEvc"
   },
   "source": [
    "### Resposta da Questão 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T21:16:12.968420Z",
     "start_time": "2020-11-16T21:16:10.538032Z"
    },
    "id": "KLOmAqmOoMvJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+------------+\n",
      "|ANO |SEXO|MEDIASALARIO|\n",
      "+----+----+------------+\n",
      "|2016|F   |9169.65     |\n",
      "|2016|M   |6906.46     |\n",
      "|2017|F   |8336.19     |\n",
      "|2017|M   |7131.79     |\n",
      "|2018|F   |8334.27     |\n",
      "|2018|M   |7605.94     |\n",
      "|2019|F   |7585.33     |\n",
      "|2019|M   |7789.65     |\n",
      "|2020|F   |7585.33     |\n",
      "|2020|M   |7789.65     |\n",
      "+----+----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Resposta da Questão 1 \n",
    "query = \"\"\"\n",
    "-- na cláusula SELECT são listadas as colunas a serem exibidas na resposta\n",
    "SELECT\n",
    "    dataAno AS ANO,\n",
    "    funcSexo AS SEXO,\n",
    "    ROUND(MEAN(salario),2) AS MEDIASALARIO\n",
    "-- na cláusula FROM são especificadas as relações temporárias realizadas\n",
    "-- entre data, pagamento e funcionario\n",
    "FROM data JOIN pagamento ON data.dataPK = pagamento.dataPK\n",
    "          JOIN funcionario ON funcionario.funcPK = pagamento.funcPK\n",
    "-- na cláusula GROUP BY são especificadas as relações de agrupamento\n",
    "GROUP BY ANO, SEXO\n",
    "-- na cláusula ORDER BY são especificadas as relações de ordenação\n",
    "ORDER BY ANO, SEXO\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query).show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ddwc0fdWQpL1"
   },
   "source": [
    "### Questão 2\n",
    "\n",
    "**(valor: 1,0)** Liste todas as agregações que podem ser geradas a partir da média dos salários dos funcionários por `dataAno` por `funcSexo` por `funcRegiaoNome`. Arredonde a média dos salários para até duas casas decimais. Devem ser exibidas as colunas na ordem e com os nomes especificados a seguir: \"ANO\", \"SEXO\", \"REGIAO\", \"MEDIASALARIO\". Ordene as linhas exibidas primeiro por ano, depois por sexo, depois por nome da região, todos em ordem ascendente. Liste as primeiras 20 linhas da resposta, sem truncamento das *strings*.\n",
    "\n",
    "**Resolva a questão especificando a consulta OLAP usando os métodos de pyspark.sql**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-tfKPSOfolLI"
   },
   "source": [
    "### Resposta da Questão 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T21:16:14.604022Z",
     "start_time": "2020-11-16T21:16:13.186945Z"
    },
    "id": "FHxd4JUlopoE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+--------+------------+\n",
      "|ANO |SEXO|REGIAO  |MEDIASALARIO|\n",
      "+----+----+--------+------------+\n",
      "|2016|F   |SUDESTE |8587.4      |\n",
      "|2016|F   |SUL     |14992.14    |\n",
      "|2016|M   |NORDESTE|9351.25     |\n",
      "|2016|M   |SUDESTE |5903.65     |\n",
      "|2016|M   |SUL     |14343.41    |\n",
      "|2017|F   |NORDESTE|1797.28     |\n",
      "|2017|F   |SUDESTE |8575.92     |\n",
      "|2017|F   |SUL     |8837.75     |\n",
      "|2017|M   |NORDESTE|8484.34     |\n",
      "|2017|M   |SUDESTE |6763.65     |\n",
      "|2017|M   |SUL     |9169.53     |\n",
      "|2018|F   |NORDESTE|5917.48     |\n",
      "|2018|F   |SUDESTE |8333.57     |\n",
      "|2018|F   |SUL     |9546.04     |\n",
      "|2018|M   |NORDESTE|6920.48     |\n",
      "|2018|M   |SUDESTE |7345.4      |\n",
      "|2018|M   |SUL     |9540.1      |\n",
      "|2019|F   |NORDESTE|7762.09     |\n",
      "|2019|F   |SUDESTE |7329.74     |\n",
      "|2019|F   |SUL     |8857.84     |\n",
      "+----+----+--------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Resposta da Questão 2\n",
    "\n",
    "# Import do pyspark.sql.functions para manipulação correta do Dataframe\n",
    "from pyspark.sql.functions import round\n",
    "\n",
    "# no comando a seguir, é aplicado o método join() sobre os DataFrames\n",
    "#     data,\n",
    "#     pagamento (dataPK como condição) e\n",
    "#     funcionario (funcPK como condição)\n",
    "# para obtenção das informações corretas do problema proposto\n",
    "df = data\\\n",
    "   .join(pagamento, [\"dataPK\"], \"inner\")\\\n",
    "   .join(funcionario, [\"funcPK\"], \"inner\")\n",
    "\n",
    "# no comando a seguir, é aplicado o método groupBy() sobre o DataFrame \n",
    "# para aplicar a operação de avg() na coluna de salario\n",
    "df = df\\\n",
    "   .groupBy(\"dataAno\", \"funcSexo\", \"funcRegiaoNome\")\\\n",
    "   .avg(\"salario\")\n",
    "\n",
    "# posteriormente o resultado de avg() é arredondado com o método\n",
    "# round() na coluna MEDIASALARIO\n",
    "df = df\\\n",
    "   .withColumn(\"MEDIASALARIO\", round(\"avg(salario)\",2))\n",
    "\n",
    "# no comando a seguir, é aplicado o método withColumnRenamed() sobre\n",
    "# o DataFrame para listar as colunas de acordo com o requisitado nas\n",
    "# condições de seleção  \n",
    "df = df\\\n",
    "   .withColumnRenamed(\"dataAno\", \"ANO\")\\\n",
    "   .withColumnRenamed(\"funcSexo\", \"SEXO\")\\\n",
    "   .withColumnRenamed(\"funcRegiaoNome\", \"REGIAO\")\n",
    "\n",
    "# no comando a seguir, é aplicado o método select() sobre o DataFrame \n",
    "# para listar as colunas a serem exibidas, depois é aplicado o método\n",
    "# orderBy() para ordernar os resultados na condição especificada  \n",
    "df = df\\\n",
    "   .select([\"ANO\", \"SEXO\", \"REGIAO\", \"MEDIASALARIO\"])\\\n",
    "   .orderBy(\"ANO\", \"SEXO\", \"REGIAO\")\n",
    "\n",
    "# Os primeiros 20 resultados das operação são exibidos pelo método show()\n",
    "# e sem truncamento das strings\n",
    "df.show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oBBSsa9WlJ2f"
   },
   "source": [
    "## 6.2 Visão Específica da Atuação Feminina\n",
    "\n",
    "O objetivo das análises desta seção é obter uma visão direcionada especificamente à atuação feminina, considerando aspectos individuais referentes a salários e receitas. Podem ser realizadas diferentes análises, sendo que duas delas são solicitadas a seguir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T6QZzSzGbXQI"
   },
   "source": [
    "### Questão 3\n",
    "\n",
    "**(valor 1,5)** Liste, para cada `dataAno`, a soma dos salários das funcionárias do sexo feminino que nasceram entre os anos de 1970 (inclusive) e 1990 (inclusive) e que moram na região \"SUDESTE\" (\"SE\") ou \"NORDESTE\" (\"NE\"). Arredonde a soma dos salários para até duas casas decimais. Devem ser exibidas as colunas na ordem e com os nomes especificados a seguir: \"ANO\", \"IDADE\", \"REGIAO\" e \"TOTALSALARIO\". Ano corresponde ao atributo `dataAno` da tabela de dimensão `data`, idade corresponde ao cálculo feito considerando o ano atual de 2020 e o atributo `funcAnoNascimento` da tabela de dimensão `funcionario`, região corresponde ao atributo `funcRegiaoNome` da tabela de dimensão `funcionario`. Ordene as linhas exibidas primeiro por ano, depois por idade e depois por região, todos em ordem ascendente. Liste as primeiras 20 linhas da resposta, sem truncamento das *strings*.\n",
    "\n",
    "**Resolva a questão especificando a consulta OLAP na linguagem SQL**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eqsvVdf9pATO"
   },
   "source": [
    "### Resposta da Questão 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T21:16:27.003018Z",
     "start_time": "2020-11-16T21:16:25.923841Z"
    },
    "id": "KLOmAqmOoMvJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+--------+------------+\n",
      "|ANO |IDADE|REGIAO  |TOTALSALARIO|\n",
      "+----+-----+--------+------------+\n",
      "|2016|30   |SUDESTE |342165.96   |\n",
      "|2016|47   |SUDESTE |172363.32   |\n",
      "|2017|30   |NORDESTE|21567.36    |\n",
      "|2017|30   |SUDESTE |1330655.03  |\n",
      "|2017|46   |SUDESTE |53737.44    |\n",
      "|2017|47   |SUDESTE |196181.64   |\n",
      "|2018|30   |NORDESTE|70918.68    |\n",
      "|2018|30   |SUDESTE |1768666.66  |\n",
      "|2018|35   |SUDESTE |113653.8    |\n",
      "|2018|46   |SUDESTE |53737.44    |\n",
      "|2018|47   |SUDESTE |196181.64   |\n",
      "|2019|30   |NORDESTE|416759.88   |\n",
      "|2019|30   |SUDESTE |1927991.98  |\n",
      "|2019|34   |SUDESTE |59745.6     |\n",
      "|2019|35   |SUDESTE |113653.8    |\n",
      "|2019|46   |SUDESTE |53737.44    |\n",
      "|2019|47   |SUDESTE |196181.64   |\n",
      "|2020|30   |NORDESTE|416759.88   |\n",
      "|2020|30   |SUDESTE |1927991.98  |\n",
      "|2020|34   |SUDESTE |59745.6     |\n",
      "+----+-----+--------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Resposta da Questão 3\n",
    "query = \"\"\"\n",
    "-- na cláusula SELECT são listadas as colunas a serem exibidas na resposta\n",
    "SELECT\n",
    "    dataAno AS ANO,\n",
    "    (2020 - funcAnoNascimento) AS IDADE,\n",
    "    funcRegiaoNome AS REGIAO,\n",
    "    ROUND(SUM(salario),2) AS TOTALSALARIO\n",
    "-- na cláusula FROM são especificadas as relações temporárias realizadas\n",
    "-- entre data, pagamento e funcionario\n",
    "FROM data JOIN pagamento ON data.dataPK = pagamento.dataPK\n",
    "          JOIN funcionario ON funcionario.funcPK = pagamento.funcPK\n",
    "-- na cláusula WHERE são definidas as condições de seleção:\n",
    "--     funcionárias do sexo feminino\n",
    "--     que nasceram entre os anos de 1970 (inclusive) e 1990 (inclusive)\n",
    "--     e que moram na região \"SUDESTE\" (\"SE\") ou \"NORDESTE\" (\"NE\")\n",
    "WHERE funcSexo = 'F'\n",
    "      AND (funcAnoNascimento >= 1970)\n",
    "      AND (funcAnoNascimento <= 1990)\n",
    "      AND (funcRegiaoNome IN ('SUDESTE', 'NORDESTE') OR funcRegiaoSigla IN ('SE', 'NE'))\n",
    "-- na cláusula GROUP BY são especificadas as relações de agrupamento\n",
    "GROUP BY ANO, IDADE, REGIAO\n",
    "-- na cláusula ORDER BY são especificadas as relações de ordenação\n",
    "ORDER BY ANO, IDADE, REGIAO\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query).show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZE-uEzVVm5Pk"
   },
   "source": [
    "### Questão 4 \n",
    "\n",
    "**(valor 1,5)** Considere que as equipes cujos valores de `equipePK` são iguais a `1, 3 e 5` possuem a maior quantidade de funcionárias do sexo feminino. Liste, para cada `dataAno`, a soma das receitas recebidas por essas equipes, o nome da equipe, o nome da filial e o setor do cliente, considerando apenas os clientes localizados na cidade de \"SAO CARLOS\". Arredonde a média dos salários para até duas casas decimais. Devem ser exibidas as colunas na ordem e com os nomes especificados a seguir: \"ANO\", \"NOMEEQUIPE\", \"NOMEFILIAL\", \"SETORCLIENTE\", \"TOTALRECEITA\". Ordene as linhas exibidas primeiro por ano, depois por nome da equipe, depois por nome da filial e depois por setor do cliente, todos em ordem ascendente. Liste as primeiras 20 linhas da resposta, sem truncamento das *strings*.\n",
    "\n",
    "**Resolva a questão especificando a consulta usando os métodos de pyspark.sql**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v5h4J6hCpf79"
   },
   "source": [
    "### Resposta da Questão 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T21:17:02.390918Z",
     "start_time": "2020-11-16T21:17:01.153837Z"
    },
    "id": "TPABkaULpiYv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------+------------------------+-------------------+------------+\n",
      "|ANO |NOMEEQUIPE   |NOMEFILIAL              |SETORCLIENTE       |TOTALRECEITA|\n",
      "+----+-------------+------------------------+-------------------+------------+\n",
      "|2016|APP - DESKTOP|SAO PAULO - AV. PAULISTA|BEBIDAS E ALIMENTOS|82203.8     |\n",
      "|2016|APP - DESKTOP|SAO PAULO - AV. PAULISTA|VESTUARIO          |71010.0     |\n",
      "|2017|APP - DESKTOP|SAO PAULO - AV. PAULISTA|BEBIDAS E ALIMENTOS|11256.35    |\n",
      "|2017|APP - DESKTOP|SAO PAULO - AV. PAULISTA|VESTUARIO          |103029.5    |\n",
      "|2017|WEB          |CAMPO GRANDE - CENTRO   |TECNOLOGIA         |57953.4     |\n",
      "|2017|WEB          |CAMPO GRANDE - CENTRO   |VESTUARIO          |12275.45    |\n",
      "|2017|WEB          |SAO PAULO - AV. PAULISTA|TECNOLOGIA         |3602.75     |\n",
      "|2017|WEB          |SAO PAULO - AV. PAULISTA|VESTUARIO          |37813.15    |\n",
      "|2018|APP - DESKTOP|SAO PAULO - AV. PAULISTA|BEBIDAS E ALIMENTOS|12383.8     |\n",
      "|2018|APP - DESKTOP|SAO PAULO - AV. PAULISTA|VESTUARIO          |76043.15    |\n",
      "|2018|WEB          |CAMPO GRANDE - CENTRO   |TECNOLOGIA         |26722.1     |\n",
      "|2018|WEB          |CAMPO GRANDE - CENTRO   |VESTUARIO          |970.1       |\n",
      "|2018|WEB          |SAO PAULO - AV. PAULISTA|TECNOLOGIA         |31092.85    |\n",
      "|2018|WEB          |SAO PAULO - AV. PAULISTA|VESTUARIO          |16979.95    |\n",
      "|2019|APP - DESKTOP|SAO PAULO - AV. PAULISTA|BEBIDAS E ALIMENTOS|37909.9     |\n",
      "|2019|APP - DESKTOP|SAO PAULO - AV. PAULISTA|VESTUARIO          |32699.5     |\n",
      "|2019|WEB          |CAMPO GRANDE - CENTRO   |TECNOLOGIA         |8521.45     |\n",
      "|2019|WEB          |CAMPO GRANDE - CENTRO   |VESTUARIO          |514.35      |\n",
      "|2019|WEB          |SAO PAULO - AV. PAULISTA|TECNOLOGIA         |16136.5     |\n",
      "|2019|WEB          |SAO PAULO - AV. PAULISTA|VESTUARIO          |1201.05     |\n",
      "+----+-------------+------------------------+-------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reposta da Questão 4\n",
    "\n",
    "# Import do pyspark.sql.functions para manipulação correta do Dataframe\n",
    "from pyspark.sql.functions import round\n",
    "\n",
    "# no comando a seguir, é aplicado o método join() sobre os DataFrames\n",
    "#     data,\n",
    "#     negociacao (dataPK como condição),\n",
    "#     equipe (equipePK como condição) e\n",
    "#     cliente (clientePK como condição)\n",
    "# para obtenção das informações corretas do problema proposto\n",
    "df = data\\\n",
    "   .join(negociacao, [\"dataPK\"], \"inner\")\\\n",
    "   .join(equipe, [\"equipePK\"], \"inner\")\\\n",
    "   .join(cliente, [\"clientePK\"], \"inner\")\n",
    "\n",
    "# no comando a seguir, é aplicado o método filter() sobre o DataFrame \n",
    "# para considerar as equipes cujos valores de equipePK são iguais a 1, 3 e 5\n",
    "# pois possuem a maior quantidade de funcionárias do sexo feminino\n",
    "# e filtrar os clientes localizados na cidade de \"SAO CARLOS\"\n",
    "df = df\\\n",
    "   .filter(df.equipePK.isin([1, 3, 5]) & df.clienteCidade.like(\"%SAO CARLOS%\"))\n",
    "\n",
    "# no comando a seguir, é aplicado o método groupBy() sobre o DataFrame \n",
    "# para aplicar a operação de sum() na coluna de receita\n",
    "df = df\\\n",
    "   .groupBy(\"dataAno\", \"equipeNome\", \"filialNome\", \"clienteSetor\")\\\n",
    "   .sum(\"receita\")\n",
    "\n",
    "# posteriormente o resultado de sum() é arredondado com o método\n",
    "# round() na coluna TOTALRECEITA\n",
    "df = df\\\n",
    "   .withColumn(\"TOTALRECEITA\", round(\"sum(receita)\",2))\n",
    "\n",
    "# no comando a seguir, é aplicado o método withColumnRenamed() sobre\n",
    "# o DataFrame para listar as colunas de acordo com o requisitado nas\n",
    "# condições de seleção  \n",
    "df = df\\\n",
    "   .withColumnRenamed(\"dataAno\", \"ANO\")\\\n",
    "   .withColumnRenamed(\"equipeNome\", \"NOMEEQUIPE\")\\\n",
    "   .withColumnRenamed(\"filialNome\", \"NOMEFILIAL\")\\\n",
    "   .withColumnRenamed(\"clienteSetor\", \"SETORCLIENTE\")\n",
    "\n",
    "# no comando a seguir, é aplicado o método select() sobre o DataFrame \n",
    "# para listar as colunas a serem exibidas, depois é aplicado o método\n",
    "# orderBy() para ordernar os resultados na condição especificada\n",
    "df = df\\\n",
    "   .select([\"ANO\", \"NOMEEQUIPE\", \"NOMEFILIAL\", \"SETORCLIENTE\", \"TOTALRECEITA\"])\\\n",
    "   .orderBy(\"ANO\", \"NOMEEQUIPE\", \"NOMEFILIAL\", \"SETORCLIENTE\")\n",
    "\n",
    "# Os primeiros 20 resultados das operação são exibidos pelo método show()\n",
    "# e sem truncamento das strings\n",
    "df.show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cif1-kov8bym"
   },
   "source": [
    "## 6.3 Visão Geral da Atuação Feminina\n",
    "\n",
    "O objetivo das análises desta seção é obter uma visão direcionada especificamente à atuação feminina, considerando aspectos conjuntos referentes a salários e receitas. Podem ser realizadas diferentes análises, dentre as quais destaca-se a análise base descrita a seguir.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wRagvNvcWX3Q"
   },
   "source": [
    "### **Análise Base** \n",
    "\n",
    "Liste, para cada `dataAno`, a soma dos salários das funcionárias de sexo feminino que moram no estado do \"RIO DE JANEIRO\" (\"RJ\") e as somas das receitas recebidas pelas equipes localizadas no estado do \"RIO DE JANEIRO\" (\"RJ\"). O estado no qual as funcionárias moram pode ser identificado pelos atributos `funcEstadoNome` ou `funcEstadoSigla` da tabela de dimensão `funcionario`, enquanto que o estado nos quais as equipes estão localizadas pode ser identificado pelos atributos `filialEstadoNome` ou `filialEstadoSigla` da tabela de dimensão `equipe`. Arredonde a soma dos salários e a soma das receitas para até duas casas decimais. Devem ser exibidas as colunas na ordem e com os nomes especificados a seguir: \"ANO\", \"TOTALSALARIO\", \"TOTALRECEITA\". Ordene as linhas exibidas por ano em ordem ascendente. Liste as primeiras 20 linhas da resposta, sem truncamento das *strings*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xr4W91rckYJJ"
   },
   "source": [
    "### Questão 5\n",
    "**(valor: 1,5) Resolva a \"Análise Base\" especificando a consulta OLAP na linguagem SQL**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZYjwVTL3p1Jc"
   },
   "source": [
    "### Resposta da Questão 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T21:18:08.840269Z",
     "start_time": "2020-11-16T21:18:07.329839Z"
    },
    "id": "_B7n3riYp9bj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------+------------+\n",
      "|ANO |TOTALSALARIO|TOTALRECEITA|\n",
      "+----+------------+------------+\n",
      "|2016|30061.2     |2205042.91  |\n",
      "|2017|30061.2     |3484981.8   |\n",
      "|2018|48108.36    |4741199.75  |\n",
      "|2019|70794.36    |5100984.61  |\n",
      "|2020|70794.36    |4192420.2   |\n",
      "+----+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Resposta da Questão 5\n",
    "query = \"\"\"\n",
    "-- na cláusula SELECT são listadas as colunas a serem exibidas na resposta\n",
    "SELECT\n",
    "    funcionarias_salario_ano.ANO,\n",
    "    TOTALSALARIO,\n",
    "    TOTALRECEITA\n",
    "-- na cláusula FROM são especificadas as relações temporárias realizadas\n",
    "FROM ( \n",
    "    -- a soma dos salários das funcionárias de sexo feminino que moram no estado do \"RIO DE JANEIRO\" (\"RJ\")\n",
    "    SELECT\n",
    "        dataAno AS ANO,\n",
    "        ROUND(SUM(salario), 2) AS TOTALSALARIO\n",
    "    FROM data JOIN pagamento ON data.dataPK = pagamento.dataPK\n",
    "              JOIN funcionario ON funcionario.funcPK = pagamento.funcPK\n",
    "    -- na cláusula WHERE são definidas as condições de seleção:\n",
    "    --     funcionárias do sexo feminino\n",
    "    --     que moram no estado do \"RIO DE JANEIRO\" (\"RJ\")\n",
    "    WHERE funcSexo = 'F'\n",
    "          AND (funcEstadoNome LIKE 'RIO DE JANEIRO' OR funcEstadoSigla LIKE 'RJ')\n",
    "    GROUP BY ANO\n",
    "    ORDER BY ANO\n",
    "    ) AS funcionarias_salario_ano,\n",
    "    (\n",
    "    -- somas das receitas recebidas pelas equipes localizadas no estado do \"RIO DE JANEIRO\" (\"RJ\")  \n",
    "    SELECT\n",
    "        dataAno AS ANO,\n",
    "        ROUND(SUM(receita), 2) AS TOTALRECEITA\n",
    "    FROM data JOIN negociacao ON data.dataPK = negociacao.dataPK\n",
    "              JOIN equipe ON negociacao.equipePK = equipe.equipePK\n",
    "    -- na cláusula WHERE são definidas as condições de seleção:\n",
    "    --     equipes localizadas no estado do \"RIO DE JANEIRO\" (\"RJ\")  \n",
    "    WHERE (filialEstadoNome LIKE 'RIO DE JANEIRO' OR filialEstadoSigla LIKE 'RJ')\n",
    "    GROUP BY ANO\n",
    "    ORDER BY ANO\n",
    "    ) AS equipes_receita_ano\n",
    "-- na cláusula WHERE são definidas as condições de seleção\n",
    "WHERE funcionarias_salario_ano.ANO = equipes_receita_ano.ANO  \n",
    "-- na cláusula ORDER BY são especificadas as relações de ordenação\n",
    "ORDER BY ANO\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "spark.sql(query).show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H6NnIh3qE3zE"
   },
   "source": [
    "###  Questão 6 \n",
    "\n",
    "**(valor: 1,5) Resolva a \"Análise Base\" especificando a consulta OLAP usando os métodos de pyspark.sql**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mbMGkFxAqEwP"
   },
   "source": [
    "### Resposta da Questão 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Análise Base** \n",
    "\n",
    "Liste, para cada `dataAno`, a soma dos salários das funcionárias de sexo feminino que moram no estado do \"RIO DE JANEIRO\" (\"RJ\") e as somas das receitas recebidas pelas equipes localizadas no estado do \"RIO DE JANEIRO\" (\"RJ\"). O estado no qual as funcionárias moram pode ser identificado pelos atributos `funcEstadoNome` ou `funcEstadoSigla` da tabela de dimensão `funcionario`, enquanto que o estado nos quais as equipes estão localizadas pode ser identificado pelos atributos `filialEstadoNome` ou `filialEstadoSigla` da tabela de dimensão `equipe`. Arredonde a soma dos salários e a soma das receitas para até duas casas decimais. Devem ser exibidas as colunas na ordem e com os nomes especificados a seguir: \"ANO\", \"TOTALSALARIO\", \"TOTALRECEITA\". Ordene as linhas exibidas por ano em ordem ascendente. Liste as primeiras 20 linhas da resposta, sem truncamento das *strings*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T21:18:21.526933Z",
     "start_time": "2020-11-16T21:18:21.511311Z"
    },
    "id": "at7o5_tbqHMa"
   },
   "outputs": [],
   "source": [
    "# Resposta da Questão 6\n",
    "\n",
    "# Import do pyspark.sql.functions para manipulação correta do Dataframe\n",
    "from pyspark.sql.functions import round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T21:18:21.965395Z",
     "start_time": "2020-11-16T21:18:21.896397Z"
    },
    "id": "at7o5_tbqHMa"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Parte 1 - Receitas recebidas pelas equipes localizadas\n",
    "# no estado do \"RIO DE JANEIRO\" (\"RJ\")\n",
    "#\n",
    "#\n",
    "# no comando a seguir, é aplicado o método join() sobre os DataFrames\n",
    "#     data,\n",
    "#     negociacao (dataPK como condição),\n",
    "#     equipe (equipePK como condição) e\n",
    "# para obtenção das informações corretas do problema proposto\n",
    "df_1 = data\\\n",
    "   .join(negociacao, [\"dataPK\"], \"inner\")\\\n",
    "   .join(equipe, [\"equipePK\"], \"inner\")\n",
    "\n",
    "# no comando a seguir, é aplicado o método filter() sobre o DataFrame \n",
    "# para considerar as equipes localizadas no estado do \"RIO DE JANEIRO\" (\"RJ\")\n",
    "df_1 = df_1\\\n",
    "   .filter(df_1.filialEstadoNome.like(\"RIO DE JANEIRO\") | df_1.filialEstadoSigla.like(\"RJ\"))\n",
    "\n",
    "# no comando a seguir, é aplicado o método groupBy() sobre o DataFrame \n",
    "# para aplicar a operação de sum() na coluna de receita\n",
    "df_1 = df_1\\\n",
    "   .groupBy(\"dataAno\")\\\n",
    "   .sum(\"receita\")\n",
    "\n",
    "# posteriormente o resultado de sum() é arredondado com o método\n",
    "# round() na coluna TOTALRECEITA\n",
    "df_1 = df_1\\\n",
    "   .withColumn(\"TOTALRECEITA\", round(\"sum(receita)\",2))\n",
    "\n",
    "# no comando a seguir, é aplicado o método withColumnRenamed() sobre\n",
    "# o DataFrame para listar as colunas de acordo com o requisitado nas\n",
    "# condições de seleção  \n",
    "df_1 = df_1\\\n",
    "   .withColumnRenamed(\"dataAno\", \"ANO\")\n",
    "\n",
    "# no comando a seguir, é aplicado o método select() sobre o DataFrame \n",
    "# para listar as colunas a serem exibidas\n",
    "df_1 = df_1\\\n",
    "   .select([\"ANO\", \"TOTALRECEITA\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T21:18:24.518788Z",
     "start_time": "2020-11-16T21:18:24.456323Z"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Parte 2 - Soma dos salários das funcionárias de sexo feminino\n",
    "# que moram no estado do \"RIO DE JANEIRO\" (\"RJ\") \n",
    "#\n",
    "#\n",
    "# no comando a seguir, é aplicado o método join() sobre os DataFrames\n",
    "#     data,\n",
    "#     pagamento (dataPK como condição) e\n",
    "#     funcionario (funcPK como condição)\n",
    "# para obtenção das informações corretas do problema proposto\n",
    "df_2 = data\\\n",
    "   .join(pagamento, [\"dataPK\"], \"inner\")\\\n",
    "   .join(funcionario, [\"funcPK\"], \"inner\")\n",
    "\n",
    "# no comando a seguir, é aplicado o método filter() sobre o DataFrame \n",
    "# para considerar as funcionárias de sexo feminino  que moram no \n",
    "# estado do \"RIO DE JANEIRO\" (\"RJ\") \n",
    "df_2 = df_2\\\n",
    "   .filter(df_2.funcSexo.like(\"F\") & (df_2.funcEstadoNome.like(\"RIO DE JANEIRO\") | df_2.funcEstadoSigla.like(\"RJ\")))\n",
    "\n",
    "# no comando a seguir, é aplicado o método groupBy() sobre o DataFrame \n",
    "# para aplicar a operação de sum() na coluna de salario\n",
    "df_2 = df_2\\\n",
    "   .groupBy(\"dataAno\")\\\n",
    "   .sum(\"salario\")\n",
    "\n",
    "# posteriormente o resultado de sum() é arredondado com o método\n",
    "# round() na coluna MEDIASALARIO\n",
    "df_2 = df_2\\\n",
    "   .withColumn(\"TOTALSALARIO\", round(\"sum(salario)\",2))\n",
    "\n",
    "# no comando a seguir, é aplicado o método withColumnRenamed() sobre\n",
    "# o DataFrame para listar as colunas de acordo com o requisitado nas\n",
    "# condições de seleção  \n",
    "df_2 = df_2\\\n",
    "   .withColumnRenamed(\"dataAno\", \"ANO\")\n",
    "\n",
    "# no comando a seguir, é aplicado o método select() sobre o DataFrame \n",
    "# para listar as colunas a serem exibidas\n",
    "df_2 = df_2\\\n",
    "   .select([\"ANO\", \"TOTALSALARIO\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T21:18:25.996472Z",
     "start_time": "2020-11-16T21:18:24.919854Z"
    },
    "id": "at7o5_tbqHMa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------+------------+\n",
      "|ANO |TOTALSALARIO|TOTALRECEITA|\n",
      "+----+------------+------------+\n",
      "|2016|30061.2     |2205042.91  |\n",
      "|2017|30061.2     |3484981.8   |\n",
      "|2018|48108.36    |4741199.75  |\n",
      "|2019|70794.36    |5100984.61  |\n",
      "|2020|70794.36    |4192420.2   |\n",
      "+----+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# no comando a seguir, é aplicado o método join() sobre os DataFrames\n",
    "#     df_1 (parte 1) e\n",
    "#     df_2 (parte 2)\n",
    "df = df_1\\\n",
    "   .join(df_2, [\"ANO\"])\n",
    "\n",
    "# no comando a seguir, é aplicado o método select() sobre o DataFrame \n",
    "# para listar as colunas a serem exibidas, depois é aplicado o método\n",
    "# orderBy() para ordernar os resultados na condição especificada\n",
    "df = df\\\n",
    "   .select([\"ANO\", \"TOTALSALARIO\", \"TOTALRECEITA\"])\\\n",
    "   .orderBy(\"ANO\")\n",
    "\n",
    "# Os primeiros 20 resultados das operação são exibidos pelo método show()\n",
    "# e sem truncamento das strings\n",
    "df.show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PWCX859nDwG3"
   },
   "source": [
    "## 6.4 Visão Comparativa Final\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oadu9w10TfFq"
   },
   "source": [
    "O objetivo da análise desta seção é obter uma visão relacionada aos sexos, por meio da comparação do total anual de gastos em salários para o pagamento das mulheres e dos homens em comparação ao total anual de receitas recebidas. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D9ccL9R-QPuT"
   },
   "source": [
    "### Questão 7\n",
    "\n",
    "**(valor 2,0)** Liste, para cada `dataAno`, a soma dos salários das funcionárias de sexo feminino, a soma dos salários dos funcionários do sexo masculino e as somas das receitas recebidas. Arredonde a soma dos salários e a soma das receitas para até duas casas decimais. Devem ser exibidas as colunas na ordem e com os nomes especificados a seguir: \"ANO\", \"TOTALSALARIOMULHERES\", \"TOTALSALARIOHOMENS\", \"TOTALRECEITA\". Ordene as linhas exibidas por ano em ordem ascendente. Liste as primeiras 20 linhas da resposta, sem truncamento das *strings*.\n",
    "\n",
    "**Resolva a questão especificando a consulta OLAP na linguagem SQL**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DFOQQmdbqbft"
   },
   "source": [
    "### Resposta da Questão 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T21:18:28.997167Z",
     "start_time": "2020-11-16T21:18:27.230721Z"
    },
    "id": "MrJNZCS2qeE7",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+------------------+-------------+\n",
      "|ANO |TOTALSALARIOMULHERES|TOTALSALARIOHOMENS|TOTALRECEITA |\n",
      "+----+--------------------+------------------+-------------+\n",
      "|2016|1210393.21          |3232223.89        |4614246.97   |\n",
      "|2017|2500857.97          |7274421.87        |7200423.35   |\n",
      "|2018|3800427.49          |1.113509898E7     |1.159353966E7|\n",
      "|2019|4733247.25          |1.38344192E7      |3.535331833E7|\n",
      "|2020|4733247.25          |1.38344192E7      |3.022217587E7|\n",
      "+----+--------------------+------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Resposta da Questão 7\n",
    "query = \"\"\"\n",
    "-- na cláusula SELECT são listadas as colunas a serem exibidas na resposta\n",
    "SELECT\n",
    "    funcionarias_salario_ano.ANO,\n",
    "    TOTALSALARIOMULHERES,\n",
    "    TOTALSALARIOHOMENS,\n",
    "    TOTALRECEITA\n",
    "-- na cláusula FROM são especificadas as relações temporárias realizadas\n",
    "FROM ( \n",
    "    -- a soma dos salários das funcionárias de sexo feminino\n",
    "    SELECT\n",
    "        dataAno AS ANO,\n",
    "        ROUND(SUM(salario), 2) AS TOTALSALARIOMULHERES\n",
    "    FROM data JOIN pagamento ON data.dataPK = pagamento.dataPK\n",
    "              JOIN funcionario ON funcionario.funcPK = pagamento.funcPK\n",
    "    -- na cláusula WHERE são definidas as condições de seleção:\n",
    "    --     funcionárias do sexo feminino\n",
    "    WHERE funcSexo = 'F'\n",
    "    GROUP BY ANO\n",
    "    ORDER BY ANO\n",
    "    ) AS funcionarias_salario_ano,\n",
    "    (\n",
    "    -- a soma dos salários dos funcionários de sexo masculino\n",
    "    SELECT\n",
    "        dataAno AS ANO,\n",
    "        ROUND(SUM(salario), 2) AS TOTALSALARIOHOMENS\n",
    "    FROM data JOIN pagamento ON data.dataPK = pagamento.dataPK\n",
    "              JOIN funcionario ON funcionario.funcPK = pagamento.funcPK\n",
    "    -- na cláusula WHERE são definidas as condições de seleção:\n",
    "    --     funcionárias do sexo masculino\n",
    "    WHERE funcSexo = 'M'\n",
    "    GROUP BY ANO\n",
    "    ORDER BY ANO\n",
    "    ) AS funcionarios_salario_ano,\n",
    "    (\n",
    "    -- somas das receitas recebidas pelas equipes\n",
    "    SELECT\n",
    "        dataAno AS ANO,\n",
    "        ROUND(SUM(receita), 2) AS TOTALRECEITA\n",
    "    FROM data JOIN negociacao ON data.dataPK = negociacao.dataPK\n",
    "              JOIN equipe ON negociacao.equipePK = equipe.equipePK\n",
    "    GROUP BY ANO\n",
    "    ORDER BY ANO\n",
    "    ) AS equipes_receita_ano\n",
    "-- na cláusula WHERE são definidas as condições de seleção\n",
    "WHERE funcionarias_salario_ano.ANO = funcionarios_salario_ano.ANO\n",
    "      AND funcionarias_salario_ano.ANO = equipes_receita_ano.ANO\n",
    "-- na cláusula ORDER BY são especificadas as relações de ordenação\n",
    "ORDER BY ANO\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "spark.sql(query).show(20, truncate=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Avaliação Final.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
