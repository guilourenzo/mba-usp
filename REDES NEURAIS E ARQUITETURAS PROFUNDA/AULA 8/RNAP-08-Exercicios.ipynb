{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K8CDQUj8yqpq"
   },
   "source": [
    "## MBA em Ciência de Dados\n",
    "# Redes Neurais e Arquiteturas Profundas\n",
    "\n",
    "### <span style=\"color:darkred\">Módulo VIII -  Tópicos em Deep Learning</span>\n",
    "\n",
    "\n",
    "### <span style=\"color:darkred\">Exercícios</span>\n",
    "\n",
    "Moacir Antonelli Ponti\n",
    "\n",
    "CeMEAI - ICMC/USP São Carlos\n",
    "\n",
    "---\n",
    "\n",
    "#### <span style=\"color:red\">Recomenda-se fortemente que os exercícios sejam feitos sem consultar as respostas antecipadamente.</span>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xMJ4IFd7yqpt"
   },
   "source": [
    "### Exercício 1)\n",
    "\n",
    "Leia sobre o dataset e desafio COCO Keypoints em https://cocodataset.org/#keypoints-2020, no qual o objetivo é detectar pessoas numa cena e localizar seus pontos chave.\n",
    "\n",
    "Qual abordagem, dentre as abaixo, seria adequada para esse cenário?\n",
    "\n",
    " (a) Rede neural convolucional treinada para detectar a presença de uma ou mais pessoas, bem como realizar regressão dos pontos chave de cada pessoa por meio do projeto de uma camada de saída com essas informações<br>\n",
    " (b) Rede neural convolucional para classificar entre imagens que possuem pessoas e imagens que não possuem pessoas, aprendendo a detecção de pessoas por meio de uma função de entropia cruzada a qual deverá computar a probabilidade de uma ou mais pessoas estarem na cena.<br>\n",
    " (c) Rede neural convolucional com função de perda triplet aproximar pontos chave de uma pessoa independente de sua pose<br>\n",
    " (d) Rede neural densa com emprego de uma função de custo que combine classificação e intersecção sobre união para gerar como saída a sobreposição entre áreas ocupadas por pessoas preditas e reais<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "038CuS5syqqL"
   },
   "source": [
    "---\n",
    "\n",
    "### Exercício 2)\n",
    "\n",
    "Assuma um cenário em que temos uma grande base de dados com documentos, em formato texto, escrito por 200 autores de uma editora. Para fins de detecção de plágio queremos aprender uma representação que seja capaz de projetar um texto num espaço de características (espaço latente) no qual seja possível medir a similaridade entre esse texto de entrada e um texto da base de dados, identificando de qual autor possui maior similaridade. Como principal problema, o texto possui alta sobreposição de conteúdo com trechos em comum entre autores arbitrários, sendo que os trechos que caracterizam cada autor representa parte significativamente menor do texto, o que é difícil de detectar e pré-processar (impedindo por exemplo remover esses trechos de confusão).\n",
    "\n",
    "Tendo em mãos um word-embedding adequado, qual seria a melhor opção para aprender uma representação útil, considerando as abordagens:\n",
    "\n",
    "I - Camada GRU<br>\n",
    "II - Camada densa de projeção em k dimensões<br>\n",
    "III - Camada densa de classificação com 200 neurônios<br>\n",
    "IV - Função entropia cruzada<br>\n",
    "V - Função contrastiva<br>\n",
    "\n",
    " (a) Treinar rede com camadas I, II, III e função V<br>\n",
    " (b) Treinar rede com camada II e função IV<br>\n",
    " (c) Treinar rede com camadas I, II e III com função IV, e depois novamente as camadas I e II e a função V, retirando a IV<br>\n",
    " (d) Treinar rede com camadas I e III com função V, e depois acrescentar a camadas II treinando com a função IV<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Exercício 3)\n",
    "\n",
    "Para qual cenário a auto-supervisão é uma técnica útil?\n",
    "\n",
    " (a) Processar dados não estruturados de qualquer origem criando um classificador diretamente a partir dos dados sem a necessidade de rotulação, e posteriormente classificar novas imagens utilizando esse modelo.<br>\n",
    " (b) Pré-processar grandes bases de dados de forma a eliminar outliers, ruído e outros efeitos indesejados nessa base de dados, garantindo maior acurácia para dados futuros.<br>\n",
    " (c) Coletar e utilizar grandes quantidades de dados não anotados, e obter um modelo inicial que possa ser utilizado como base para adaptar modelos para problemas específicos, vencendo a barreira de anotar dados massivamente.<br>\n",
    " (d) Para base de dados pequenas em que não se dispõe de grande quantidades de exemplos para treinar redes neurais profundas para tarefas de classificação e regressão, pois essa técnica realiza aumento de dados de forma nativa.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nJQ0-S3myqqL"
   },
   "source": [
    "---\n",
    "### Exercício 4)\n",
    "\n",
    "Vamos implementar as funções contrastive e triplet para observar se essas funções são minimizadas após o treinamento de uma CNN com a camada softmax e entropia cruzada. Em resumo, não iremos treinar a rede com as funções contrastive/triplet, mas apenas avaliar o espaço de características.\n",
    "\n",
    "Estude e utilize as implementações abaixo. Elas foram feitas para maior entendimento e não para velocidade. Caso necessário, o ponto principal de melhoria é a função `np.where()` que poderia ser substituída por algo mais eficiente.\n",
    "\n",
    "Carregue a base de dados Fashion MNIST e monte uma CNN com a seguinte arquitetura:\n",
    "* Convolucional com 32 filtros 3x3 e strides (2,2), sem zero pading, ativação relu\n",
    "* Convolucional com 64 filtros 3x3 e strides (2,2), sem zero pading, ativação relu\n",
    "* Convolucional com 64 filtros 3x3 e strides (2,2), sem zero pading, ativação relu\n",
    "* Densa com 64 neurônios e ativação relu (embedding)\n",
    "* Dropout 0.25\n",
    "* Densa softmax com 10 neurônios\n",
    "\n",
    "Defina as sementes seed(1), set_seed(2) e compile o modelo com o otimizador Adam, taxa de aprendizado 0.001. Após isso, monte um modelo que gere como saída a camada Densa de 64 neurônios (embedding). Obtenha as representações do embedding (antes do treinamento) para os 50 primeiros exemplos de treinamento. Depois:\n",
    "1. Defina random.seed(1)\n",
    "2. Compute e armazene as funções contrastive e triplet nesses 50 exemplos: note que é preciso passar as classes (em formato numérico) e as representações para a função. \n",
    "\n",
    "Depois, com 20 épocas e batchsize = 32, treine com o otimizador Adam, taxa de aprendizado 0.001, e usando apenas as 10000 primeiras instâncias da base [:10000].\n",
    "\n",
    "Obtenha as representações do embedding após o treinamento para os 50 primeiros exemplos de treinamento. A seguir:\n",
    "1. Defina random.seed(1)\n",
    "2. Compute e armazene as funções contrastive e triplet nas representações dos mesmos 50 exemplos usando o modelo treinado\n",
    "\n",
    "Considerando os valores das perdas arredondados para 4 casas decimais e o efeito do treinamento nas perdas contrastive e triplet, qual foi o resultado?\n",
    "\n",
    "OBS: para que essas funções possam ser usadas para treinamento no Keras é preciso utilizar funções do tensorflow ao invés de numpy, além de garantir que o `shape` de `y_true` é igual a de `y_pred`. Caso necessário, adapte e procure como passar essa função de perda personalizada.\n",
    "\n",
    " (a) Valor da triplet é maior do que o da contrastiva, e apenas a triplet reduziu seu valor significativamente, a contrastive teve redução na 4.a casa decimal<br>\n",
    " (b) Valor da triplet é menor do que o da contrastiva, e ambas foram minimizadas significativamente (na ordem da 2.a casa decimal ou maior) após o treinamento<br>\n",
    " (c) Valor da triplet é maior do que o da contrastiva, e ambas foram minimizadas significativamente (na ordem da 2.a casa decimal ou maior) após o treinamento<br>\n",
    " (d) Valor da triplet é menor do que o da contrastiva, e apenas a contrastive reduziu seu valor significativamente, a triplet teve redução na 4.a casa decimal<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_loss(y_true, y_pred):\n",
    "    \"\"\" Triplet loss\n",
    "        y_true: classes dos exemplos de forma numérica (não one-hot-encoding)\n",
    "        y_pred: predicao da rede em termos das *representacoes*/embedding aprendida\n",
    "        Moacir A. Ponti/2020\n",
    "    \"\"\"\n",
    "    L = 0 # loss acumulada\n",
    "    m = 1 # margem\n",
    "    size = y_true.shape[0] # tamanho do conjunto a avaliar\n",
    "    \n",
    "    # normalizar dados 0-1\n",
    "    y_pred = (y_pred-np.min(y_pred))/(np.max(y_pred)-np.min(y_pred))\n",
    "    \n",
    "    # seleciona uma ancora por exemplo no conjunto\n",
    "    for i in range(size):\n",
    "        a = y_pred[i] # representacao da ancora\n",
    "        y = y_true[i] # classe da ancora\n",
    "        \n",
    "        # seleciona exemplo aleatorio, positivo e negativo\n",
    "        pos_ind = np.where(y_true == y)\n",
    "        neg_ind = np.where(y_true != y)\n",
    "        p = y_pred[pos_ind[0][random.randint(0,len(pos_ind[0])-1)]]\n",
    "        n = y_pred[neg_ind[0][random.randint(0,len(neg_ind[0])-1)]]\n",
    "        \n",
    "        # computa triplet loss\n",
    "        lapn = np.max([0, m + np.sum(np.power(a-p,2)) - np.sum(np.power(a-n,2))])/2.0\n",
    "        # acumula\n",
    "        L += lapn\n",
    "\n",
    "    return L/float(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss(y_true, y_pred):\n",
    "    \"\"\" Contrastive loss\n",
    "        y_true: classes dos exemplos de forma numérica (não one-hot-encoding)\n",
    "        y_pred: predicao da rede em termos das *representacoes*/embedding aprendida\n",
    "        Moacir A. Ponti/2020\n",
    "    \"\"\"    \n",
    "    L = 0\n",
    "    m = 1\n",
    "    size = y_true.shape[0]\n",
    "\n",
    "    # normalizar dados\n",
    "    y_pred = (y_pred-np.min(y_pred))/(np.max(y_pred)-np.min(y_pred))\n",
    "\n",
    "    # para cada exemplo \"ancora\"\n",
    "    for i in range(size):\n",
    "        a = y_pred[i] # representacao a\n",
    "        y_a = y_true[i] # classe de a\n",
    "        \n",
    "        # sorteia exemplo (nao ancora)\n",
    "        ind = i \n",
    "        while (ind == i):\n",
    "            ind = random.randint(0,size-1)\n",
    "        \n",
    "        p = y_pred[ind] # representacao p\n",
    "        \n",
    "        # calcula contrastive loss (com base na classe)\n",
    "        if (y_true[ind] == y_a):\n",
    "            lapn = np.sum(np.power(a-p,2))/2.0\n",
    "        else: \n",
    "            lapn = np.max([0, m - np.sum(np.power(a-p, 2)) ])/2.0\n",
    "            \n",
    "        L += lapn\n",
    "        \n",
    "    return L/float(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from numpy.random import seed\n",
    "from tensorflow.random import set_seed\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# carregando datasets do keras\n",
    "#from tensorflow.keras.datasets import mnist\n",
    "\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "Classes:  10\n",
      "Tamanho da entrada:  (28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "# obtendo informações das imagens (resolucao) e dos rótulos (número de classes)\n",
    "img_lin, img_col = x_train.shape[1], x_train.shape[2]\n",
    "if (len(x_train.shape) == 3):\n",
    "      n_channels = 1\n",
    "else:\n",
    "      n_channels = x_train.shape[3]\n",
    "        \n",
    "num_classes = len(np.unique(y_train))\n",
    "print(x_train.shape)\n",
    "print('Classes: ', num_classes)\n",
    "\n",
    "# dividir por 255 para obter normalizacao\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "# transformar categorias em one-hot-encoding\n",
    "y_train_one_hot = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test_one_hot = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "# formatar para treinamento\n",
    "x_train = x_train.reshape(x_train.shape[0], img_lin, img_col, n_channels)\n",
    "x_test = x_test.reshape(x_test.shape[0], img_lin, img_col, n_channels)\n",
    "\n",
    "# formato da entrada\n",
    "input_shape = (img_lin, img_col, n_channels)\n",
    "print('Tamanho da entrada: ', input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yfnm0YgLyqqU"
   },
   "source": [
    "---\n",
    "\n",
    "### Exercício 5)\n",
    "\n",
    "Suponha uma base de dados que possua 32 características relacionadas a transações financeiras realizadas por clientes de um banco. Temos disponível 10 milhões de transações não anotadas de diferentes clientes, e transações anotadas para 20 mil clientes: em número girando entre 100 e 2 mil transações anotadas em \"normais\", \"suspeitas\" e \"anomalias\" por cliente. \n",
    "\n",
    "Desejamos criar modelos de detecção de anomalias em transações para clientes recentes do banco, para os quais temos nenhum ou poucos dados de transações. Para evitar que as primeiras predições sejam aleatórias, considere as seguintes abordagens para aprender uma representação com 32 características que possa ser usada como modelo inicial para posterior ajuste com dados futuros rotulados:\n",
    "\n",
    "I - denoising autoencoder overcomplete com restrição de esparsidade no código de 32 dimensões<br>\n",
    "II - rede neural que receba por entrada as 32 dimensões embaralhadas e que aprenda a detectar as posições corretas das características<br>\n",
    "III - rede neural treinada com transações anotadas de clientes com perfil semelhante ao novo cliente<br>\n",
    "\n",
    "Essas abordagens dizem respeito a:\n",
    "\n",
    "(a) I - auto-supervisão, II - análise de permutações, III - aprendizado ativo<br>\n",
    "(b) I - aprendizado não-supervisionado, II e III - auto-supervisão<br>\n",
    "(c) I - redução de dimensionalidade, II - auto-supervisão, III - transferência de aprendizado<br>\n",
    "(d) I e II - auto-supervisão, III - transferência de aprendizado<br>"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "RNAP-04-Exercicios_solucoes.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
