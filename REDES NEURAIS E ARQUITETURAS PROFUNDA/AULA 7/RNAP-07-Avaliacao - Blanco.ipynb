{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K8CDQUj8yqpq"
   },
   "source": [
    "## MBA em Ciência de Dados\n",
    "# Redes Neurais e Arquiteturas Profundas\n",
    "\n",
    "### <span style=\"color:darkred\">Módulo VII -  Introdução ao Aprendizado por Reforço</span>\n",
    "\n",
    "\n",
    "### <span style=\"color:darkred\">Avaliação</span>\n",
    "\n",
    "Moacir Antonelli Ponti\n",
    "\n",
    "CeMEAI - ICMC/USP São Carlos\n",
    "\n",
    "---\n",
    "\n",
    "As respostas devem ser dadas no Moodle, use esse notebook apenas para gerar o código necessário para obter as respostas\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xMJ4IFd7yqpt"
   },
   "source": [
    "### Questão 1)\n",
    "\n",
    "Considere a afirmação: o aprendizado por reforço é supervisionado. Em que sentido essa afirmação estaria correta?\n",
    "\n",
    "(a) Problemas de aprendizado por reforço recebem por entrada pares de exemplo e rótulo<br>\n",
    "<span style=\"color:red\">***(b) O aprendizado por reforço é supervisionado por meio da observação do ambiente e retorno obtido na forma de uma recompensa após a realização de uma ação***</span><br>\n",
    "(c) Problemas de aprendizado por reforço recebem por entrada muitos exemplos, alguns rótulos ligados a esse exemplos e executam por grande número de épocas<br>\n",
    "(d) O aprendizado por reforço é apenas semi-supervisionado, pois um agente observa parcialmente o estado do ambiente, portanto essa afirmação está incorreta.<br>\n",
    "\n",
    "***Aos 12:24 do video 1 ele fala sobre***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "038CuS5syqqL"
   },
   "source": [
    "---\n",
    "\n",
    "### Questão 2)\n",
    "\n",
    "Quais os passos fundamentais em uma execução de um algoritmo de aprendizado por reforço em um dado instante t dentro um episódio?\n",
    "\n",
    " (a) Executar uma ação selecionada por meio de uma política atual, recebimento de uma recompensa e observação que potencialmente modifica o estado do agente.<br>\n",
    " (b) Obter um exemplo de treinamento, predizer uma ação com base nesse exemplo e comparar com a ação que deveria ter sido realizada<br>\n",
    " (c) Executar uma sequência de ações e, após atingir um estado terminal, computar a recompensa total<br>\n",
    "<span style=\"color:red\">***(d) Amostrar uma ação dentre as possíveis segundo uma política e fornecer essa ação como entrada para uma rede neural que irá classificá-la entre uma ação correta e incorreta.***</span><br>\n",
    "\n",
    "***No video 4 ele comeca a falar sobre aos 26 minutos***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nJQ0-S3myqqL"
   },
   "source": [
    "---\n",
    "### Questão 3)\n",
    "\n",
    "Quais os algoritmos de treinamento que representam as duas principais abordagens no aprendizado por reforço?\n",
    "\n",
    " (a) Backpropagation e Stochastic Gradient Descent<br>\n",
    " (b) Deep Q-Network e Entropia Cruzada<br>\n",
    "<span style=\"color:red\">***(c) Policy Learning e Value Learning***</span><br>\n",
    " (d) Actor-Critic e Feed-forward network<br>\n",
    " \n",
    "***Slide 55***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C1sh5GgYyqqY"
   },
   "source": [
    "---\n",
    "\n",
    "### Questão 4)\n",
    "\n",
    "Na biblioteca `gym` carregue o ambiente `Blackjack-v0` que representa o Jogo 21, no qual se pode pedir mais cartas ou parar, o ambiente `Pendulum-v0` no qual o objetivo é equilibrar uma alavanca giratória num ângulo de 90 graus, o `CartPole-v1`, no qual o objetivo é equilibrar um poste sobre um carro, movimentando esse carro, e finalmente `Acrobot-v1` que é um pêndulo bi-articulado com dois segmentos, com objetivo de levantar a ponta do segundo segmento a uma altura ao menos igual ao tamanho dos segmento por cima da base.\n",
    "\n",
    "Como é o espaço de ações possíeis desses ambientes?\n",
    "\n",
    "(a) Blackjack: discreto 2 ações, Pendulum: discreto 2 ações, CartPole: contínuo entre -1 e 1, Acrobot: contínuo entre -1 e 1<br>\n",
    "(b) Blackjack: discreto 3 ações, Pendulum: discreto 2 ações, CartPole: contínuo entre -2 e 2, Acrobot: discreto 2 ações<br>\n",
    "<span style=\"color:red\">***(c) Blackjack: discreto 2 ações, Pendulum:  contínuo entre -2 e 2, CartPole: discreto 2 ações, Acrobot: discreto 3 ações***</span><br>\n",
    "(d) Blackjack: discreto 3 ações, Pendulum: contínuo entre -2 e 2, CartPole: discreto 3 ações, Acrobot: contínuo entre -1 e 1<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(2)\n",
      "Box(-2.0, 2.0, (1,), float32)\n",
      "Discrete(2)\n",
      "Discrete(3)\n"
     ]
    }
   ],
   "source": [
    "env1 = gym.make(\"Blackjack-v0\")\n",
    "print(env1.action_space)\n",
    "\n",
    "env2 = gym.make(\"Pendulum-v0\")\n",
    "print(env2.action_space)\n",
    "\n",
    "env3 = gym.make(\"CartPole-v1\")\n",
    "print(env3.action_space)\n",
    "\n",
    "env4 = gym.make(\"Acrobot-v1\")\n",
    "print(env4.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Questão 5)\n",
    "\n",
    "Carregue o ambiente `Blackjack-v0`. Esse problema gera recompensa +1 para vitória, 0 para pedir uma carta, e -1 para derrota. Inicialize o ambiente, execute 100 mil episódios (cada um até o estado terminal) e calcule a média de recompensas totais (MR), a média de ações por episódio (MA), a taxa de vitórias (TV) e a taxa de derrotas (TD), sendo vitórias e derrotas medidas após o final de cada episódio. Arredonde os valores para 1 casas decimal.\n",
    "\n",
    "(a) MR é negativo, MP está entre 2 e 3, TV é igual a TD<br>\n",
    "(b) MR é 0.0, MP é 1.4, TV é menor do que TD<br>\n",
    "(c) MR é positivo, MP está entre 1 e 2, TV é igual a TD<br>\n",
    "<span style=\"color:red\">***(d) MR é negativo, MP está entre 1 e 2, TV é menor do que TD***</span><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Site legal para ver:https://www.kaggle.com/angps95/blackjack-strategy-using-reinforcement-learning\n",
    "import gym\n",
    "import numpy as np\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Blackjack-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(2)\n",
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(18, 10, False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(env.action_space)\n",
    "print(env.action_space.n)\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((23, 10, False), -1.0, True, {})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tuple(Discrete(32), Discrete(11), Discrete(2))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reward_range\n",
    "env.natural\n",
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = env.action_space.sample()\n",
    "# realizar acao\n",
    "# observation, reward, done, info\n",
    "s_n, r, fim, info = env.step(a)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 4, False)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episódio:  0\n",
      "Epoch: 3\n",
      "Vitoria: 1\n",
      "Derrota: 0\n",
      "Recompensa: 1.0\n",
      "--------------------------------------\n",
      "\n",
      "Episódio:  20000\n",
      "Epoch: 36787\n",
      "Vitoria: 5634\n",
      "Derrota: 13545\n",
      "Recompensa: -7911.0\n",
      "--------------------------------------\n",
      "\n",
      "Episódio:  40000\n",
      "Epoch: 73276\n",
      "Vitoria: 11217\n",
      "Derrota: 27158\n",
      "Recompensa: -15941.0\n",
      "--------------------------------------\n",
      "\n",
      "Episódio:  60000\n",
      "Epoch: 110003\n",
      "Vitoria: 16734\n",
      "Derrota: 40783\n",
      "Recompensa: -24049.0\n",
      "--------------------------------------\n",
      "\n",
      "Episódio:  80000\n",
      "Epoch: 146448\n",
      "Vitoria: 22278\n",
      "Derrota: 54365\n",
      "Recompensa: -32087.0\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "episodios = 100000\n",
    "total_epochs = 0\n",
    "total_recs = 0\n",
    "vitoria = 0\n",
    "derrota = 0\n",
    "\n",
    "# episodios\n",
    "for t in range(episodios):\n",
    "    s = env.reset()\n",
    "    \n",
    "    #setado epoch e recompensa no inicio igual a zero\n",
    "    epochs, recompensas = 0, 0\n",
    "    fim = False\n",
    "    \n",
    "    # episodio atual\n",
    "    while not fim:\n",
    "#         print('Zica')\n",
    "        # explorar espaco de acao\n",
    "        a = env.action_space.sample()\n",
    "        \n",
    "        # realizar acao\n",
    "        s_n, r, fim, info = env.step(a)\n",
    "        # estado subsequente s_n\n",
    "        \n",
    "        epochs += 1\n",
    "        recompensas += r\n",
    "            \n",
    "        #após realziar ação, computamos o ganho ou perda:\n",
    "        if recompensas > 0:\n",
    "            vitoria += 1\n",
    "\n",
    "        if recompensas < 0:\n",
    "            derrota += 1 \n",
    "        \n",
    "       \n",
    "        # atualiza estado\n",
    "        total_epochs += epochs\n",
    "        total_recs += recompensas        \n",
    "        \n",
    "    if (t % 20000 == 0):\n",
    "        print(\"\\nEpisódio: \", t)\n",
    "        print(\"Epoch:\", total_epochs)\n",
    "        print(\"Vitoria:\", vitoria)\n",
    "        print(\"Derrota:\", derrota)\n",
    "        print(\"Recompensa:\", total_recs)\n",
    "        print(\"--------------------------------------\")\n",
    "        \n",
    "        \n",
    "        \n",
    "#         print(\"Média de recompensas totais (MR): %.2f\" % (total_recs/episodios))\n",
    "#         print(\"Média de passos por episódio (MP): %.2f\" % (total_epochs/episodios))\n",
    "#         print(\"Taxa de vitórias (TV): %.2f\" % (vitoria/episodios))\n",
    "#         print(\"Taxa de derrotas (TD): %.2f\" % (derrota/episodios))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episódio:  99999\n",
      "Média de recompensas totais (MR): -0.40\n",
      "Média de passos por episódio (MP): 1.83\n",
      "Taxa de vitórias (TV): 0.28\n",
      "Taxa de derrotas (TD): 0.68\n"
     ]
    }
   ],
   "source": [
    "print(\"Episódio: \", t)\n",
    "print(\"Média de recompensas totais (MR): %.2f\" % (total_recs/episodios))\n",
    "print(\"Média de passos por episódio (MP): %.2f\" % (total_epochs/episodios))\n",
    "print(\"Taxa de vitórias (TV): %.2f\" % (vitoria/episodios))\n",
    "print(\"Taxa de derrotas (TD): %.2f\" % (derrota/episodios))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "RNAP-04-Avaliacao_solucoes.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
