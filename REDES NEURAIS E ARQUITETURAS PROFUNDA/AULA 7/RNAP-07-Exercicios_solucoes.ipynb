{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K8CDQUj8yqpq"
   },
   "source": [
    "## MBA em Ciência de Dados\n",
    "# Redes Neurais e Arquiteturas Profundas\n",
    "\n",
    "### <span style=\"color:darkred\">Módulo VII -  Introdução ao Aprendizado por Reforço</span>\n",
    "\n",
    "\n",
    "### <span style=\"color:darkred\">Exercícios (com soluções)</span>\n",
    "\n",
    "Moacir Antonelli Ponti\n",
    "\n",
    "CeMEAI - ICMC/USP São Carlos\n",
    "\n",
    "---\n",
    "\n",
    "#### <span style=\"color:red\">Recomenda-se fortemente que os exercícios sejam feitos sem consultar as respostas antecipadamente.</span>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xMJ4IFd7yqpt"
   },
   "source": [
    "### Exercício 1)\n",
    "\n",
    "Qual alternativa descreve a comparação entre os objetivos da análise de agrupamentos e o aprendizado por reforço?\n",
    "\n",
    " <font color='red'>(a) A análise de agrupamentos visa encontrar estrutura nos dados com base na similaridade ou diferença em suas características, enquanto que o aprendizado por reforço objetiva maximizar recompensa futura mapeando observações em ações por meio de uma política<br></font>\n",
    " (b) A análise de agrupamentos e aprendizado por reforço não possuem supervisão, possuindo algoritmos para treinamento similar, sua diferença está apenas na formulação do problema<br>\n",
    " (c) A análise de agrupamentos é não supervisionada, enquanto que o aprendizado por reforço é semi-supervisionado<br>\n",
    " (d) A análise de agrupamentos visa encontrar estrutura nos dados com base em suas características, enquanto que o por reforço visa encontrar um mapeamento entre características e as melhores ações possíveis segundo previamente rotuladas por um especialista<br>\n",
    " \n",
    " **Justificativa**: A análise de agrupamento é um método não supervisionado para encontrar (dis)similaridade entre exemplos. O aprendizado por reforço também pode ser considerado sem supervisão no sentido de não haver anotações ou rotulos manualmente definidos - assim não é supervisionado nem semi-supervisionado. Apesar de ambas não supervisionadas, aprendizado por reforço e agrupamentos são tarefas distintas e requerem algoritmos diferentes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "038CuS5syqqL"
   },
   "source": [
    "---\n",
    "\n",
    "### Exercício 2)\n",
    "\n",
    "São componentes fundamentais do aprendizado por reforço:\n",
    "\n",
    " (a) Policy learning, estados e redes neurais profundas<br>\n",
    " (b) Histórico, camadas convolucionais, agente e inicialização<br>\n",
    " <font color='red'>(c) Agente, ambiente, estado, política de ação e função valor</font><br>\n",
    " (d) Value learning, estados e redes neurais profundas<br>\n",
    " \n",
    " **Justificativa**: Redes neurais profundas e camadas convolucionais podem fazer parte de uma solução para aprendizado por reforço, mas não são compontentes fundamentais. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nJQ0-S3myqqL"
   },
   "source": [
    "---\n",
    "### Exercício 3)\n",
    "\n",
    "Qual os passos básicos de um algoritmo de aprendizado de políticas (policy learning)?\n",
    "\n",
    " (a) Inicializar agente, com a política atual executar uma ação, obter uma recompensa, reforçar a política atual se essa produziu recompensa positiva nessa iteração.<br>\n",
    " (b) Inicializar agente, executar política até estado terminal, e repetir esse processo múltiplas vezes, selecionando o episódio com a maior recompensa total<br>\n",
    " <font color='red'>(c) Inicializar agente, executar política até estado terminal, armazenar: estados, ações e recompensas, reduzir probabilidade de ações com baixa recompensa, Aumentar probabilidade de ações com alta recompensa</font><br>\n",
    " (d) Inicializar agente, utilizar rede neural para otimizar a melhor política em cada ação realizada.<br>\n",
    " \n",
    " **Justificativa**: O Policy learning executa política até o estado terminal para determinar as recompensas, não sendo um método que adapta por passo/iteração, mesmo quando usando uma rede neural. O processo é repetido múltiplas vezes mas todos os episódios são usados para adaptação do modelo, não apenas o com maior recompensa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6exl-MrVyqqT"
   },
   "source": [
    "---\n",
    "### Exercício 4)\n",
    "\n",
    "Dado um problema, como projetá-lo para ser resolvido com aprendizado por reforço?\n",
    "\n",
    "(a) Organizar os dados em pares $(x,y)$ sendo $x$ os dados de entrada e $y$ o espaço de saída ou alvo para que seja aprendido um mapeamento $X \\rightarrow Y$<br>\n",
    "<font color='red'>(b) Formular o problema como o de um agente que executa ações e maximiza a recompensa dessas ações com base na solução encontrada<br></font>\n",
    "(c) Coletar dados e organizá-los em uma base dividida em instâncias $x \\in X$ para que sejam inspecionadas por funções de distância e particionar o espaço $X$<br>\n",
    "(d) Projetar o problema para funcionar com um agente que explora um ambiente e encontra o resultado por tentativa e erro com backtracking<br>\n",
    "\n",
    "**Justificativa**: O problema é sempre colocado no sentido de ações realizadas por um agente a partir de uma política. Não há um espaço de saída explícito, nem divisão em instâncias para particionamento. Ainda, backtracking não é uma estratégia típica de aprendizado por reforço, sendo baseada em múltiplos episódios, ao invés de retornar a um ponto da execução."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yfnm0YgLyqqU"
   },
   "source": [
    "---\n",
    "\n",
    "### Exercício 5)\n",
    "\n",
    "Instale o pacote Box2D e carregue os ambientes `Reverse-v0` e `CarRacing-v0` da biblioteca Gym. Procure sobre esses ambientes em https://gym.openai.com/envs, caso necessário.\n",
    "\n",
    "Como é formulado o espaço de ações desses problemas?\n",
    "\n",
    " a) Reverse: Contínuo com 3 valores discretos; CarRacing: Discreto com valores de 0-255 <br>\n",
    " b) Reverse: Uma tripla em que cada elemento possui 2 valores discretos; CarRacing: Contínuo entre -1 e 1 e um vetor de 3 posições float32<br>\n",
    " c) Reverse: Contínuo valores entre -1 e 1 mais um valor discreto; CarRacing: Discreto com valores de 0-255<br>\n",
    " <span style=\"color: red\"> d) Reverse: Uma tripla em que cada elemento possui 2 valores discretos; CarRacing: 3 valores contínuos: entre -1 e 1 para uma das ações e entre 0 e 1 para as outras duas</span><br>\n",
    "\n",
    "  **Justificativa**: ver abaixo o código. Para mais detalhes do CarRacing ver: https://github.com/openai/gym/blob/master/gym/envs/box2d/car_racing.py e procurar por `action_space`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuple(Discrete(2), Discrete(2), Discrete(2))\n",
      "Box(-1.0, 1.0, (3,), float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maponti/.virtualenvs/rn/lib/python3.8/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env1 = gym.make(\"Reverse-v0\")\n",
    "print(env1.action_space)\n",
    "\n",
    "env2 = gym.make(\"CarRacing-v0\")\n",
    "print(env2.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J6VfUMk8yqqW"
   },
   "source": [
    "---\n",
    "\n",
    "### Exercício 6)\n",
    "\n",
    "Retome o problema do taxi visto em aula, utilizando o mesmo algoritmo de Value Learning, no qual a cada passo utilizamos um método de \"exploration\" obtendo amostras do espaço de ações por meio do `env.action_space.sample()`. Nesse exercício, vamos executar também \"exploitation\". Para isso modifique o treinamento conforme abaixo:\n",
    "1. Crie uma nova variável `tau` que definirá a chance do algoritmo realizar \"exploration\". \n",
    "2. Carregue o pacote `random` e antes dos episódios defina `random.seed(1)`\n",
    "3. Substitua a linha em que a ação é selecionada por um condicional:\n",
    "    * Se `random.uniform(0, 1)` for menor ou igual a `tau`, então realize \"exploration\" (da mesma forma como estava no algoritmo dado em aula)\n",
    "    * Caso contrário, então realize \"exploitation\", isso é, obtendo a ação não aleatória, mas a partir da tabela Q aprendida até agora com `np.argmax(q_table[s])`\n",
    "    \n",
    "Execute dois treinamentos, 1) com `tau=0.9`, 2) com `tau=0.3`, por 3000 episódios, e logo após teste com 50 episódios novos, medindo a média de recompensas totais e média de passos por episódio. Qual foi o resultado, comparativamente?\n",
    "\n",
    "OBS: lembre-se de definer `random.seed(1)` antes de iniciar cada experimento. Use alpha = 0.1 e gamma = 0.4\n",
    "    \n",
    " a) Maior taxa de *exploitation* beneficiou o treinamento, o agente alcançou uma política que resultou em menos passos e maior recompensa média, mas mesmo usando mais **exploration** o agente também aprendeu uma política significativamente melhor do que aleatória.<br>\n",
    " b) Maior taxa de **exploration** beneficiou o treinamento, tendo o agente alcançado uma política que resultou em menos passos e maior recompensa média, enquanto que com maior *exploitation* o agente não foi capaz de aprender uma política significativamente melhor do que aleatória.<br>\n",
    " c) Os resultados foram muito similares, não sendo possível dizer qual abordagem é melhor, ambas obtiveram alguma melhoria no sentido da política aprendida<br>\n",
    " <span style=\"color: red\">d) Maior taxa de **exploration** beneficiou o treinamento, o agente alcançou uma política que resultou em menos passos e maior recompensa média, mas mesmo usando mais *exploitation* o agente também aprendeu uma política significativamente melhor do que aleatória.<br></span>\n",
    " \n",
    " **Justificativa**: Notar que, ao menos nos primeiros 3000 episódios uma taxa baixa de exploration impede o agente de explorar novas ações, sendo benéfico uma taxa alta de exploration e baixa de exploitation. Uma opção é aumentar a taxa de exploitation conforme aumenta o número de episódios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "\n",
    "env = gym.make(\"Taxi-v3\")\n",
    "\n",
    "n_episodios = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episódio:  3000\n"
     ]
    }
   ],
   "source": [
    "# tabela Q\n",
    "q_table1 = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "\n",
    "q_table1.shape\n",
    "\n",
    "# hiperparametros\n",
    "alpha = 0.1 # taxa de aprendizado\n",
    "gamma = 0.4 # desconto de recompensas futuras\n",
    "tau = 0.9 # taxa de exploration\n",
    "\n",
    "# historico\n",
    "episodios = []\n",
    "\n",
    "random.seed(1)\n",
    "\n",
    "# episodios\n",
    "for t in range(1, n_episodios+1):\n",
    "    s = env.reset()\n",
    "    \n",
    "    epochs, recompensas = 0, 0\n",
    "    fim = False\n",
    "    \n",
    "    # episodio atual\n",
    "    while not fim:\n",
    "        if random.uniform(0, 1) <= tau:\n",
    "            a = env.action_space.sample() # exploration\n",
    "        else:\n",
    "            a = np.argmax(q_table1[s]) # exploitation\n",
    "        \n",
    "        # realizar acao\n",
    "        s_n, r, fim, info = env.step(a)\n",
    "        # estado subsequente s_n\n",
    "\n",
    "        # salvo o valor atual para (s,a) - Q(s,a)\n",
    "        valor_ant = q_table1[s,a]\n",
    "        \n",
    "        # verifica proximo valor\n",
    "        prox_max = np.max(q_table1[s_n])\n",
    "        \n",
    "        # combina com desconto na recompensa futura\n",
    "        novo_valor = (1-alpha)*valor_ant + alpha*(r+gamma*prox_max)\n",
    "        q_table1[s,a] = novo_valor\n",
    "        \n",
    "        # atualiza estado\n",
    "        s = s_n\n",
    "        epochs += 1\n",
    "        \n",
    "    if (t % 100 == 0):\n",
    "        clear_output(wait=True)\n",
    "        print(\"Episódio: \", t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Média de recompensas totais: 8.24\n",
      "Média de passos por episódio: 12.76\n"
     ]
    }
   ],
   "source": [
    "n_episodios_teste = 50\n",
    "total_epochs = 0\n",
    "total_recs = 0\n",
    "\n",
    "for i in range(n_episodios_teste):\n",
    "    s = env.reset()\n",
    "    epochs, rec_total_i = 0,0\n",
    "    fim = False\n",
    "    while not fim:\n",
    "        a = np.argmax(q_table1[s])\n",
    "        s, r, fim, info = env.step(a)\n",
    "        epochs += 1\n",
    "        rec_total_i += r\n",
    "        \n",
    "    total_epochs += epochs\n",
    "    total_recs += rec_total_i\n",
    "\n",
    "print(\"Média de recompensas totais: %.2f\" % (total_recs/n_episodios_teste))\n",
    "print(\"Média de passos por episódio: %.2f\" % (total_epochs/n_episodios_teste))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episódio:  3000\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Taxi-v3\")\n",
    "\n",
    "# tabela Q\n",
    "q_table2 = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "\n",
    "q_table2.shape\n",
    "\n",
    "# hiperparametros\n",
    "alpha = 0.1 # taxa de aprendizado\n",
    "gamma = 0.4 # desconto de recompensas futuras\n",
    "tau = 0.3 # taxa de exploration\n",
    "\n",
    "# historico\n",
    "episodios = []\n",
    "\n",
    "random.seed(1)\n",
    "\n",
    "# episodios\n",
    "for t in range(1, n_episodios+1):\n",
    "    s = env.reset()\n",
    "    \n",
    "    epochs, recompensas = 0, 0\n",
    "    fim = False\n",
    "    \n",
    "    # episodio atual\n",
    "    while not fim:\n",
    "        if random.uniform(0, 1) <= tau:\n",
    "            a = env.action_space.sample() # exploration\n",
    "        else:\n",
    "            a = np.argmax(q_table2[s]) # exploitation\n",
    "\n",
    "        # realizar acao\n",
    "        s_n, r, fim, info = env.step(a)\n",
    "        # estado subsequente s_n\n",
    "\n",
    "        # salvo o valor atual para (s,a) - Q(s,a)\n",
    "        valor_ant = q_table2[s,a]\n",
    "        \n",
    "        # verifica proximo valor\n",
    "        prox_max = np.max(q_table2[s_n])\n",
    "        \n",
    "        # combina com desconto na recompensa futura\n",
    "        novo_valor = (1-alpha)*valor_ant + alpha*(r+gamma*prox_max)\n",
    "        q_table2[s,a] = novo_valor\n",
    "        \n",
    "        # atualiza estado\n",
    "        s = s_n\n",
    "        epochs += 1\n",
    "        \n",
    "    if (t % 100 == 0):\n",
    "        clear_output(wait=True)\n",
    "        print(\"Episódio: \", t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Média de recompensas totais: -87.04\n",
      "Média de passos por episódio: 98.38\n"
     ]
    }
   ],
   "source": [
    "n_episodios_teste = 50\n",
    "total_epochs = 0\n",
    "total_recs = 0\n",
    "\n",
    "for i in range(n_episodios_teste):\n",
    "    s = env.reset()\n",
    "    epochs, rec_total_i = 0,0\n",
    "    fim = False\n",
    "    while not fim:\n",
    "        a = np.argmax(q_table2[s])\n",
    "        s, r, fim, info = env.step(a)\n",
    "        epochs += 1\n",
    "        rec_total_i += r\n",
    "        \n",
    "    total_epochs += epochs\n",
    "    total_recs += rec_total_i\n",
    "\n",
    "print(\"Média de recompensas totais: %.2f\" % (total_recs/n_episodios_teste))\n",
    "print(\"Média de passos por episódio: %.2f\" % (total_epochs/n_episodios_teste))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "def animacao_episodio(frames):\n",
    "    for i, frame in enumerate(frames):\n",
    "        clear_output(wait=True)\n",
    "        print(frame['frame'])\n",
    "        print(\"t: \", (i + 1))\n",
    "        print(\"Estado: \", frame['state'])\n",
    "        print(\"Ação: \", frame['action'])\n",
    "        print(\"Recompensa: \", frame['reward'])\n",
    "        sleep(.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :\u001b[35m\u001b[34;1m\u001b[43mG\u001b[0m\u001b[0m\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "\n",
      "t:  7\n",
      "Estado:  85\n",
      "Ação:  5\n",
      "Recompensa:  20\n",
      "\n",
      "Recompensa total:  14\n",
      "Passos até o estado terminal:  7\n"
     ]
    }
   ],
   "source": [
    "# inicializacao\n",
    "env.reset()\n",
    "frames = [] # animacao\n",
    "rec_total = 0\n",
    "epochs = 0\n",
    "\n",
    "s = env.reset()\n",
    "epochs, rec_total_i = 0,0\n",
    "fim = False\n",
    "while not fim:\n",
    "    a = np.argmax(q_table2[s])\n",
    "    s, r, fim, info = env.step(a)\n",
    "    epochs += 1\n",
    "    rec_total += r\n",
    "    \n",
    "    frames.append({\n",
    "        'frame': env.render(mode='ansi'),\n",
    "        'state': s,\n",
    "        'action': a,\n",
    "        'reward': r\n",
    "        }\n",
    "    )\n",
    "    \n",
    "env.close()\n",
    "\n",
    "animacao_episodio(frames)\n",
    "print(\"\\nRecompensa total: \", rec_total)\n",
    "print(\"Passos até o estado terminal: \", epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Exercício 7)\n",
    "\n",
    "Tente utilizar o mesmo algoritmo anterior, agora para o ambiente `MountainCar-v0`. Logo ao definir a tabela Q surge um erro. Como interpretar esse erro?\n",
    "\n",
    " a) Esse problema é muito simples e contem poucas ações assim não conseguimos definir a tabela Q<br>\n",
    " b) O espaço de ações desse problema não é discreto. Assim, não é possível definir diretamente um número de colunas para a tabela<br>\n",
    " <span style=\"color: red\">c) O espaço de observações (ou quantidade de estados) desse problema não é discreto. Assim, não é possível definir diretamente um número de linhas para a tabela<br></span>\n",
    " d) O espaço de ações e de observações (ou quantidade de estados) são contínuos não permitindo encontrar diretamente um número de elementos para a tabela<br>\n",
    " \n",
    " **Justificativa**: O espaço de observações do problema tem valores contínuos, e portanto há um grande número de possíveis estados, não sendo possível definir uma tabela Q discreta diretamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(-1.2000000476837158, 0.6000000238418579, (2,), float32) Discrete(3)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Box' object has no attribute 'n'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-4de9415252e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# tabela Q\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mq_table2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mq_table2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Box' object has no attribute 'n'"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"MountainCar-v0\")\n",
    "\n",
    "n_episodios = 1000\n",
    "\n",
    "print(env.observation_space, env.action_space)\n",
    "\n",
    "# tabela Q\n",
    "q_table2 = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "\n",
    "q_table2.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do espaço de ações:  3\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Box' object has no attribute 'n'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-4bea9f5900d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tamanho do espaço de ações: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tamanho do espaço de observacoes: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Box' object has no attribute 'n'"
     ]
    }
   ],
   "source": [
    "print(\"Tamanho do espaço de ações: \", env.action_space.n)\n",
    "print(\"Tamanho do espaço de observacoes: \", env.observation_space.n) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C1sh5GgYyqqY"
   },
   "source": [
    "---\n",
    "\n",
    "### Exercício 8)\n",
    "\n",
    "Para o caso em que não conseguimos definir uma tabela Q diretamente, considere as seguintes opções:\n",
    "\n",
    "I - Projetar uma Deep Q-Network que receba o estado e dê como saída os valores preditos para cada ação<br>\n",
    "II - Projetar um mecanismo basedo em Policy Learning, aprendendo diretamente probabilidades de selecionar ações a partir dos estados<br>\n",
    "III - Criar múltiplas tabelas Q, uma para cada ação<br>\n",
    "IV - Projetar um algoritmo de Value learning que aprenda as distribuições dos valores ao invés dos valores diretamente<br>\n",
    "\n",
    "São viáveis as opções:\n",
    "\n",
    " <span style=\"color: red\">a) I e II <br></span>\n",
    " b) I e IV<br>\n",
    " c) I e III <br>\n",
    " d) II e IV<br>\n",
    " \n",
    " **Justificativa**: A opção I é válida pois uma rede neural é capaz de receber valores contínos e gerar como saída a predição do valor. Já a opção II também é válida pois Policy Learning não precisa de uma tabela Q, apenas otimizar probabilidades relacionadas à política. Finalmente III é inválida pois múltiplas tabelas Q também não dão conta do espaço contínuo, e um algoritmo de Value Learning para aprender distribuições também não resolveria o problema, já que o problema está ao estimar o valor de um estado que é contínuo. "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "RNAP-04-Exercicios_solucoes.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
