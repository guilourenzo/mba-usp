{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fvaEK6Bn-wwG"
   },
   "source": [
    "## MBA em Ciência de Dados\n",
    "# Redes Neurais e Arquiteturas Profundas\n",
    "\n",
    "### <span style=\"color:darkred\">Módulo 7 - Introdução ao Aprendizado por Reforço</span>\n",
    "\n",
    "#### <span style=\"color:darkred\">**Parte 2: Algoritmo de Aprendizado por Reforço \"Value Learning\"**</span>\n",
    "\n",
    "Moacir Antonelli Ponti\n",
    "\n",
    "CeMEAI - ICMC/USP São Carlos\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problema Taxi\n",
    "\n",
    "Relembrando:\n",
    "* Temos 4 localizações relevantes indicadas por 0:R, 1:G, 2:Y e 3:B.\n",
    "* Azul é o passageiro e magenta o destino.\n",
    "* O taxi é amarelo quando livre e verde quando ocupado\n",
    "* Grid contém 25 posições para o táxi, 5 para o passageiro, e 4 destinos\n",
    "\n",
    "**Ações**:<br>\n",
    "0 : mover para norte<br>\n",
    "1 : mover para sul<br>\n",
    "2 : mover para leste<br>\n",
    "3 : mover para oeste<br>\n",
    "4 : pegar passageiro (pickup)<br>\n",
    "5 : deixar passageiro (dropoff)\n",
    "\n",
    "**Observação/estado**:<br>\n",
    "Posições do taxi, passageiro e destino codificada numericamente\n",
    "\n",
    "**Recompensas**:<br>\n",
    "* -1 : por passo<br>\n",
    "* 20 : deixar passageiro<br>\n",
    "* -10: executar \"pegar\" ou \"deixar\" ilegalmente\n",
    "\n",
    "**Término**:\n",
    "* Passageiro é deixado no destino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 6)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"Taxi-v3\")\n",
    "\n",
    "# tabela Q\n",
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "\n",
    "q_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "for i in range(301,305):\n",
    "    print(np.round(q_table[i],4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algoritmo de Value learning\n",
    "\n",
    "Tentaremos predizer o valor atual, ajustando com o melhor valor do estado subsequente.\n",
    "\n",
    "Iremos considerar uma taxa de aprendizado $\\alpha$ e um desconto para recompensas futuras $\\gamma$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episódio:  5000\n"
     ]
    }
   ],
   "source": [
    "# hiperparametros\n",
    "alpha = 0.1 # taxa de aprendizado\n",
    "gamma = 0.5 # desconto de recompensas futuras\n",
    "\n",
    "# historico\n",
    "episodios = []\n",
    "\n",
    "# episodios\n",
    "for t in range(1, 5001):\n",
    "    s = env.reset()\n",
    "    \n",
    "    epochs, recompensas = 0, 0\n",
    "    fim = False\n",
    "    \n",
    "    # episodio atual\n",
    "    while not fim:\n",
    "        # explorar espaco de acao\n",
    "        a = env.action_space.sample()\n",
    "        \n",
    "        # realizar acao\n",
    "        s_n, r, fim, info = env.step(a)\n",
    "        # estado subsequente s_n\n",
    "\n",
    "        # salvo o valor atual para (s,a) - Q(s,a)\n",
    "        valor_ant = q_table[s,a]\n",
    "        \n",
    "        # verifica proximo valor\n",
    "        prox_max = np.max(q_table[s_n])\n",
    "        \n",
    "        # combina com desconto na recompensa futura\n",
    "        novo_valor = (1-alpha)*valor_ant + alpha*(r+gamma*prox_max)\n",
    "        q_table[s,a] = novo_valor\n",
    "        \n",
    "        # atualiza estado\n",
    "        s = s_n\n",
    "        epochs += 1\n",
    "        \n",
    "    if (t % 250 == 0):\n",
    "        clear_output(wait=True)\n",
    "        print(\"Episódio: \", t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao consultar a tabela, teremos *valores* diferentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -1.9988  -1.9951  -1.9976  -1.9976 -10.9976 -10.9976]\n",
      "[ -1.9799  -1.9181  -1.9595  -1.9593 -10.9594 -10.9593]\n",
      "[ -1.9975  -1.9897  -1.9949  -1.9949 -10.9949 -10.9949]\n",
      "[ -1.9999  -1.9997  -1.9999  -1.9999 -10.9999 -10.9999]\n"
     ]
    }
   ],
   "source": [
    "for i in range(301,305):\n",
    "    print(np.round(q_table[i],4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avaliando a performance do agente\n",
    "\n",
    "Após obter a função Q, armazenada na tabela, agora podemos avaliar a performance do agente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Média de recompensas totais: 7.96\n",
      "Média de passos por episódio: 13.04\n"
     ]
    }
   ],
   "source": [
    "n_episodios_teste = 50\n",
    "total_epochs = 0\n",
    "total_recs = 0\n",
    "\n",
    "for i in range(n_episodios_teste):\n",
    "    s = env.reset()\n",
    "    epochs, rec_total_i = 0,0\n",
    "    fim = False\n",
    "    while not fim:\n",
    "        a = np.argmax(q_table[s])\n",
    "        s, r, fim, info = env.step(a)\n",
    "        epochs += 1\n",
    "        rec_total_i += r\n",
    "        \n",
    "    total_epochs += epochs\n",
    "    total_recs += rec_total_i\n",
    "\n",
    "print(\"Média de recompensas totais: %.2f\" % (total_recs/n_episodios_teste))\n",
    "print(\"Média de passos por episódio: %.2f\" % (total_epochs/n_episodios_teste))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Executando uma vez para visualizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "def animacao_episodio(frames):\n",
    "    for i, frame in enumerate(frames):\n",
    "        clear_output(wait=True)\n",
    "        print(frame['frame'])\n",
    "        print(\"t: \", (i + 1))\n",
    "        print(\"Estado: \", frame['state'])\n",
    "        print(\"Ação: \", frame['action'])\n",
    "        print(\"Recompensa: \", frame['reward'])\n",
    "        sleep(.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "\n",
      "t:  14\n",
      "Estado:  475\n",
      "Ação:  5\n",
      "Recompensa:  20\n",
      "\n",
      "Recompensa total:  7\n",
      "Passos até o estado terminal:  14\n"
     ]
    }
   ],
   "source": [
    "# inicializacao\n",
    "env.reset()\n",
    "frames = [] # animacao\n",
    "rec_total = 0\n",
    "epochs = 0\n",
    "\n",
    "s = env.reset()\n",
    "epochs, rec_total_i = 0,0\n",
    "fim = False\n",
    "while not fim:\n",
    "    a = np.argmax(q_table[s])\n",
    "    s, r, fim, info = env.step(a)\n",
    "    epochs += 1\n",
    "    rec_total += r\n",
    "    \n",
    "    frames.append({\n",
    "        'frame': env.render(mode='ansi'),\n",
    "        'state': s,\n",
    "        'action': a,\n",
    "        'reward': r\n",
    "        }\n",
    "    )\n",
    "    \n",
    "env.close()\n",
    "\n",
    "animacao_episodio(frames)\n",
    "print(\"\\nRecompensa total: \", rec_total)\n",
    "print(\"Passos até o estado terminal: \", epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "RNAP-06-Aula-notebook1_lstm.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
