{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K8CDQUj8yqpq"
   },
   "source": [
    "## MBA em Ciência de Dados\n",
    "# Redes Neurais e Arquiteturas Profundas\n",
    "\n",
    "### <span style=\"color:darkred\">Módulo VII -  Introdução ao Aprendizado por Reforço</span>\n",
    "\n",
    "\n",
    "### <span style=\"color:darkred\">Exercícios</span>\n",
    "\n",
    "Moacir Antonelli Ponti\n",
    "\n",
    "CeMEAI - ICMC/USP São Carlos\n",
    "\n",
    "---\n",
    "\n",
    "#### <span style=\"color:red\">Recomenda-se fortemente que os exercícios sejam feitos sem consultar as respostas antecipadamente.</span>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xMJ4IFd7yqpt"
   },
   "source": [
    "### Exercício 1)\n",
    "\n",
    "Qual alternativa descreve a comparação entre os objetivos da análise de agrupamentos e o aprendizado por reforço?\n",
    "\n",
    " (a) A análise de agrupamentos visa encontrar estrutura nos dados com base na similaridade ou diferença em suas características, enquanto que o aprendizado por reforço objetiva maximizar recompensa futura mapeando observações em ações por meio de uma política<br>\n",
    " (b) A análise de agrupamentos e aprendizado por reforço não possuem supervisão, possuindo algoritmos para treinamento similar, sua diferença está apenas na formulação do problema<br>\n",
    " (c) A análise de agrupamentos é não supervisionada, enquanto que o aprendizado por reforço é semi-supervisionado<br>\n",
    " (d) A análise de agrupamentos visa encontrar estrutura nos dados com base em suas características, enquanto que o por reforço visa encontrar um mapeamento entre características e as melhores ações possíveis segundo previamente rotuladas por um especialista<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "038CuS5syqqL"
   },
   "source": [
    "---\n",
    "\n",
    "### Exercício 2)\n",
    "\n",
    "São componentes fundamentais do aprendizado por reforço:\n",
    "\n",
    " (a) Policy learning, estados e redes neurais profundas<br>\n",
    " (b) Histórico, camadas convolucionais, agente e inicialização<br>\n",
    " (c) Agente, ambiente, estado, política de ação e função valor<br>\n",
    " (d) Value learning, estados e redes neurais profundas<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nJQ0-S3myqqL"
   },
   "source": [
    "---\n",
    "### Exercício 3)\n",
    "\n",
    "Qual os passos básicos de um algoritmo de aprendizado de políticas (policy learning)?\n",
    "\n",
    " (a) Inicializar agente, com a política atual executar uma ação, obter uma recompensa, reforçar a política atual se essa produziu recompensa positiva nessa iteração.<br>\n",
    " (b) Inicializar agente, executar política até estado terminal, e repetir esse processo múltiplas vezes, selecionando o episódio com a maior recompensa total<br>\n",
    " (c) Inicializar agente, executar política até estado terminal, armazenar: estados, ações e recompensas, reduzir probabilidade de ações com baixa recompensa, Aumentar probabilidade de ações com alta recompensa<br>\n",
    " (d) Inicializar agente, utilizar rede neural para otimizar a melhor política numa determinada época<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6exl-MrVyqqT"
   },
   "source": [
    "---\n",
    "### Exercício 4)\n",
    "\n",
    "Dado um problema, como projetá-lo para ser resolvido com aprendizado por reforço?\n",
    "\n",
    "(a) Organizar os dados em pares $(x,y)$ sendo $x$ os dados de entrada e $y$ o espaço de saída ou alvo para que seja aprendido um mapeamento $X \\rightarrow Y$<br>\n",
    "(b) Formular o problema como o de um agente que executa ações e maximiza a recompensa dessas ações com base na solução encontrada<br>\n",
    "(c) Coletar dados e organizá-los em uma base dividida em instâncias $x \\in X$ para que sejam inspecionadas por funções de distância e particionar o espaço $X$<br>\n",
    "(d) Projetar o problema para funcionar com um agente que explora um ambiente e encontra o resultado por tentativa e erro com backtracking<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yfnm0YgLyqqU"
   },
   "source": [
    "---\n",
    "\n",
    "### Exercício 5)\n",
    "\n",
    "Instale o pacote Box2D e carregue os ambientes `Reverse-v0` e `CarRacing-v0` da biblioteca Gym. Procure sobre esses ambientes em https://gym.openai.com/envs, caso necessário.\n",
    "\n",
    "Como é formulado o espaço de ações desses problemas?\n",
    "\n",
    " a) Reverse: Contínuo com 3 valores discretos; CarRacing: Discreto com valores de 0-255 <br>\n",
    " b) Reverse: Uma tripla em que cada elemento possui 2 valores discretos; CarRacing: Contínuo entre -1 e 1<br>\n",
    " c) Reverse: Contínuo valores entre -1 e 1 mais um valor discreto; CarRacing: Discreto com valores de 0-255<br>\n",
    " d) Reverse: Uma tripla em que cada elemento possui 2 valores discretos; CarRacing: 3 valores contínuos: entre -1 e 1 para uma das ações e entre 0 e 1 para as outras duas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## carregar biblioteca e ambientes/problemas\n",
    "## inspecionar os espaços de ações"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J6VfUMk8yqqW"
   },
   "source": [
    "---\n",
    "\n",
    "### Exercício 6)\n",
    "\n",
    "Retome o problema do taxi visto em aula, utilizando o mesmo algoritmo de Value Learning, no qual a cada passo utilizamos um método de \"exploration\" obtendo amostras do espaço de ações por meio do `env.action_space.sample()`. Nesse exercício, vamos executar também \"exploitation\". Para isso modifique o treinamento conforme abaixo:\n",
    "1. Crie uma nova variável `tau` que definirá a chance do algoritmo realizar \"exploration\". \n",
    "2. Carregue o pacote `random` e antes dos episódios defina `random.seed(1)`\n",
    "3. Substitua a linha em que a ação é selecionada por um condicional:\n",
    "    * Se `random.uniform(0, 1)` for menor ou igual a `tau`, então realize \"exploration\" (da mesma forma como estava no algoritmo dado em aula)\n",
    "    * Caso contrário, então realize \"exploitation\", isso é, obtendo a ação não aleatória, mas a partir da tabela Q aprendida até agora com `np.argmax(q_table[s])`\n",
    "    \n",
    "Execute dois treinamentos, 1) com `tau=0.9`, 2) com `tau=0.1`, por 2000 episódios, e logo após teste com 50 episódios novos, medindo a média de recompensas totais e média de passos por episódio. Qual foi o resultado, comparativamente?\n",
    "\n",
    "OBS: lembre-se de definer `random.seed(1)` antes de iniciar cada experimento.\n",
    "    \n",
    " a) Maior taxa de *exploitation* beneficiou o treinamento, o agente alcançou uma política que resultou em menos passos e maior recompensa média, mas mesmo usando mais **exploration** o agente também aprendeu uma política significativamente melhor do que aleatória.<br>\n",
    " b) Maior taxa de **exploration** beneficiou o treinamento, tendo o agente alcançado uma política que resultou em menos passos e maior recompensa média, enquanto que com maior *exploitation* o agente não foi capaz de aprender uma política significativamente melhor do que aleatória.<br>\n",
    " c) Os resultados foram muito similares, não sendo possível dizer qual abordagem é melhor, ambas obtiveram alguma melhoria no sentido da política aprendida<br>\n",
    " c) Maior taxa de **exploration** beneficiou o treinamento, o agente alcançou uma política que resultou em menos passos e maior recompensa média, mas mesmo usando mais *exploitation* o agente também aprendeu uma política significativamente melhor do que aleatória.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "env = gym.make(\"Taxi-v3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configurar tabela Q\n",
    "# episodios caso 1\n",
    "# teste caso 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# configurar tabela Q\n",
    "# episodios caso 2\n",
    "# teste caso 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Exercício 7)\n",
    "\n",
    "Tente utilizar o mesmo algoritmo anterior, agora para o ambiente `MountainCar-v0`. Logo ao definir a tabela Q surge um erro. Como interpretar esse erro?\n",
    "\n",
    " a) Esse problema é muito simples e contem poucas ações assim não conseguimos definir a tabela Q<br>\n",
    " b) O espaço de ações desse problema não é discreto. Assim, não é possível definir diretamente um número de colunas para a tabela<br>\n",
    " c) O espaço de observações (ou quantidade de estados) desse problema não é discreto. Assim, não é possível definir diretamente um número de linhas para a tabela<br>\n",
    " d) O espaço de ações e de observações (ou quantidade de estados) são contínuos não permitindo encontrar diretamente um número de elementos para a tabela<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### tentativa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C1sh5GgYyqqY"
   },
   "source": [
    "---\n",
    "\n",
    "### Exercício 8)\n",
    "\n",
    "Para o caso em que não conseguimos definir uma tabela Q diretamente, considere as seguintes opções:\n",
    "\n",
    "I - Projetar uma Deep Q-Network que receba o estado e dê como saída os valores preditos para cada ação<br>\n",
    "II - Projetar um mecanismo basedo em Policy Learning, aprendendo diretamente probabilidades de selecionar ações a partir dos estados<br>\n",
    "III - Criar múltiplas tabelas Q, uma para cada ação<br>\n",
    "IV - Projetar um algoritmo de Value learning que aprenda as distribuições dos valores ao invés dos valores diretamente<br>\n",
    "\n",
    "São viáveis as opções:\n",
    "\n",
    " a) I e II <br>\n",
    " b) I e IV<br>\n",
    " c) I e III <br>\n",
    " d) II e IV<br>"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "RNAP-04-Exercicios_solucoes.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
