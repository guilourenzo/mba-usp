{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K8CDQUj8yqpq"
   },
   "source": [
    "## MBA em Ciência de Dados\n",
    "# Redes Neurais e Arquiteturas Profundas\n",
    "\n",
    "### <span style=\"color:darkred\">Módulo VII -  Introdução ao Aprendizado por Reforço</span>\n",
    "\n",
    "\n",
    "### <span style=\"color:darkred\">Avaliação</span>\n",
    "\n",
    "Moacir Antonelli Ponti\n",
    "\n",
    "CeMEAI - ICMC/USP São Carlos\n",
    "\n",
    "---\n",
    "\n",
    "As respostas devem ser dadas no Moodle, use esse notebook apenas para gerar o código necessário para obter as respostas\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xMJ4IFd7yqpt"
   },
   "source": [
    "### Questão 1)\n",
    "\n",
    "Considere a afirmação: o aprendizado por reforço é supervisionado. Em que sentido essa afirmação estaria correta?\n",
    "\n",
    "(a) Problemas de aprendizado por reforço recebem por entrada pares de exemplo e rótulo<br>\n",
    "(b) O aprendizado por reforço é supervisionado por meio da observação do ambiente e retorno obtido na forma de uma recompensa após a realização de uma ação<br>\n",
    "(c) Problemas de aprendizado por reforço recebem por entrada muitos exemplos, alguns rótulos ligados a esse exemplos e executam por grande número de épocas<br>\n",
    "(d) O aprendizado por reforço é apenas semi-supervisionado, pois um agente observa parcialmente o estado do ambiente, portanto essa afirmação está incorreta.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "038CuS5syqqL"
   },
   "source": [
    "---\n",
    "\n",
    "### Questão 2)\n",
    "\n",
    "Quais os passos fundamentais em uma execução de um algoritmo de aprendizado por reforço em um dado instante t dentro um episódio?\n",
    "\n",
    " (a) Executar uma ação selecionada por meio de uma política atual, recebimento de uma recompensa e observação que potencialmente modifica o estado do agente.<br>\n",
    " (b) Obter um exemplo de treinamento, predizer uma ação com base nesse exemplo e comparar com a ação que deveria ter sido realizada<br>\n",
    " (c) Executar uma sequência de ações e, após atingir um estado terminal, computar a recompensa total<br>\n",
    " (d) Amostrar uma ação dentre as possíveis segundo uma política e fornecer essa ação como entrada para uma rede neural que irá classificá-la entre uma ação correta e incorreta.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nJQ0-S3myqqL"
   },
   "source": [
    "---\n",
    "### Questão 3)\n",
    "\n",
    "Quais os algoritmos de treinamento que representam as duas principais abordagens no aprendizado por reforço?\n",
    "\n",
    " (a) Backpropagation e Stochastic Gradient Descent<br>\n",
    " (b) Deep Q-Network e Entropia Cruzada<br>\n",
    " (c) Policy Learning e Value Learning<br>\n",
    " (d) Actor-Critic e Feed-forward network<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C1sh5GgYyqqY"
   },
   "source": [
    "---\n",
    "\n",
    "### Questão 4)\n",
    "\n",
    "Na biblioteca `gym` carregue o ambiente `Blackjack-v0` que representa o Jogo 21, no qual se pode pedir mais cartas ou parar, o ambiente `Pendulum-v0` no qual o objetivo é equilibrar uma alavanca giratória num ângulo de 90 graus, o `CartPole-v1`, no qual o objetivo é equilibrar um poste sobre um carro, movimentando esse carro, e finalmente `Acrobot-v1` que é um pêndulo bi-articulado com dois segmentos, com objetivo de levantar a ponta do segundo segmento a uma altura ao menos igual ao tamanho dos segmento por cima da base.\n",
    "\n",
    "Como é o espaço de ações possíeis desses ambientes?\n",
    "\n",
    "(a) Blackjack: discreto 2 ações, Pendulum: discreto 2 ações, CartPole: contínuo entre -1 e 1, Acrobot: contínuo entre -1 e 1<br>\n",
    "(b) Blackjack: discreto 3 ações, Pendulum: discreto 2 ações, CartPole: contínuo entre -2 e 2, Acrobot: discreto 2 ações<br>\n",
    "(c) Blackjack: discreto 2 ações, Pendulum:  contínuo entre -2 e 2, CartPole: discreto 2 ações, Acrobot: discreto 3 ações<br>\n",
    "(d) Blackjack: discreto 3 ações, Pendulum: contínuo entre -2 e 2, CartPole: discreto 3 ações, Acrobot: contínuo entre -1 e 1<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Questão 5)\n",
    "\n",
    "Carregue o ambiente `Blackjack-v0`. Esse problema gera recompensa +1 para vitória, 0 para empate, e -1 para derrota. Inicialize o ambiente, execute 100 mil episódios (cada um até o estado terminal) e calcule a média de recompensas totais (MR), a média de ações por episódio (MA), a taxa de vitórias (TV) e a taxa de derrotas (TD), sendo vitórias e derrotas medidas após o final de cada episódio. Arredonde os valores para 1 casas decimal.\n",
    "\n",
    "(a) MR é negativo, MA está entre 2 e 3, TV é igual a TD<br>\n",
    "(b) MR é 0.0, MA é 1.4, TV é menor do que TD<br>\n",
    "(c) MR é positivo, MA está entre 1 e 2, TV é igual a TD<br>\n",
    "(d) MR é negativo, MA está entre 1 e 2, TV é menor do que TD<br>"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "RNAP-04-Avaliacao_solucoes.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
