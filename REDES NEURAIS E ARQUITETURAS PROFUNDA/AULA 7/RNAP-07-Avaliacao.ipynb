{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K8CDQUj8yqpq"
   },
   "source": [
    "## MBA em Ciência de Dados\n",
    "# Redes Neurais e Arquiteturas Profundas\n",
    "\n",
    "### <span style=\"color:darkred\">Módulo VII -  Introdução ao Aprendizado por Reforço</span>\n",
    "\n",
    "\n",
    "### <span style=\"color:darkred\">Avaliação</span>\n",
    "\n",
    "Moacir Antonelli Ponti\n",
    "\n",
    "CeMEAI - ICMC/USP São Carlos\n",
    "\n",
    "---\n",
    "\n",
    "As respostas devem ser dadas no Moodle, use esse notebook apenas para gerar o código necessário para obter as respostas\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xMJ4IFd7yqpt"
   },
   "source": [
    "### Questão 1)\n",
    "\n",
    "Considere a afirmação: o aprendizado por reforço é supervisionado. Em que sentido essa afirmação estaria correta?\n",
    "\n",
    "(a) Problemas de aprendizado por reforço recebem por entrada pares de exemplo e rótulo<br>\n",
    "(b) O aprendizado por reforço é supervisionado por meio da observação do ambiente e retorno obtido na forma de uma recompensa após a realização de uma ação<br>\n",
    "(c) Problemas de aprendizado por reforço recebem por entrada muitos exemplos, alguns rótulos ligados a esse exemplos e executam por grande número de épocas<br>\n",
    "(d) O aprendizado por reforço é apenas semi-supervisionado, pois um agente observa parcialmente o estado do ambiente, portanto essa afirmação está incorreta.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "038CuS5syqqL"
   },
   "source": [
    "---\n",
    "\n",
    "### Questão 2)\n",
    "\n",
    "Quais os passos fundamentais em uma execução de um algoritmo de aprendizado por reforço em um dado instante t dentro um episódio?\n",
    "\n",
    " (a) Executar uma ação selecionada por meio de uma política atual, recebimento de uma recompensa e observação que potencialmente modifica o estado do agente.<br>\n",
    " (b) Obter um exemplo de treinamento, predizer uma ação com base nesse exemplo e comparar com a ação que deveria ter sido realizada<br>\n",
    " (c) Executar uma sequência de ações e, após atingir um estado terminal, computar a recompensa total<br>\n",
    " (d) Amostrar uma ação dentre as possíveis segundo uma política e fornecer essa ação como entrada para uma rede neural que irá classificá-la entre uma ação correta e incorreta.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nJQ0-S3myqqL"
   },
   "source": [
    "---\n",
    "### Questão 3)\n",
    "\n",
    "Quais os algoritmos de treinamento que representam as duas principais abordagens no aprendizado por reforço?\n",
    "\n",
    " (a) Backpropagation e Stochastic Gradient Descent<br>\n",
    " (b) Deep Q-Network e Entropia Cruzada<br>\n",
    " (c) Policy Learning e Value Learning<br>\n",
    " (d) Actor-Critic e Feed-forward network<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C1sh5GgYyqqY"
   },
   "source": [
    "---\n",
    "\n",
    "### Questão 4)\n",
    "\n",
    "Na biblioteca `gym` carregue o ambiente `Blackjack-v0` que representa o Jogo 21, no qual se pode pedir mais cartas ou parar, o ambiente `Pendulum-v0` no qual o objetivo é equilibrar uma alavanca giratória num ângulo de 90 graus, o `CartPole-v1`, no qual o objetivo é equilibrar um poste sobre um carro, movimentando esse carro, e finalmente `Acrobot-v1` que é um pêndulo bi-articulado com dois segmentos, com objetivo de levantar a ponta do segundo segmento a uma altura ao menos igual ao tamanho dos segmento por cima da base.\n",
    "\n",
    "Como é o espaço de ações possíeis desses ambientes?\n",
    "\n",
    "(a) Blackjack: discreto 2 ações, Pendulum: discreto 2 ações, CartPole: contínuo entre -1 e 1, Acrobot: contínuo entre -1 e 1<br>\n",
    "(b) Blackjack: discreto 3 ações, Pendulum: discreto 2 ações, CartPole: contínuo entre -2 e 2, Acrobot: discreto 2 ações<br>\n",
    "(c) Blackjack: discreto 2 ações, Pendulum:  contínuo entre -2 e 2, CartPole: discreto 2 ações, Acrobot: discreto 3 ações<br>\n",
    "(d) Blackjack: discreto 3 ações, Pendulum: contínuo entre -2 e 2, CartPole: discreto 3 ações, Acrobot: contínuo entre -1 e 1<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Blackjack-v0\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "# Blackjack-v0\n",
    "print('Blackjack-v0')\n",
    "env = gym.make(\"Blackjack-v0\")\n",
    "# (env.observation_space, )\n",
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Pendulum-v0\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Box(-2.0, 2.0, (1,), float32)"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "# Pendulum-v0\n",
    "print('Pendulum-v0')\n",
    "env = gym.make(\"Pendulum-v0\")\n",
    "# (env.observation_space, )\n",
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CartPole-v1\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "# CartPole-v1\n",
    "print('CartPole-v1')\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "# (env.observation_space, )\n",
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Acrobot-v1\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Discrete(3)"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "# Acrobot-v1\n",
    "print('Acrobot-v1')\n",
    "env = gym.make(\"Acrobot-v1\")\n",
    "# (env.observation_space, )\n",
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Questão 5)\n",
    "\n",
    "Carregue o ambiente `Blackjack-v0`. Esse problema gera recompensa +1 para vitória, 0 para empate, e -1 para derrota. Inicialize o ambiente, execute 100 mil episódios (cada um até o estado terminal) e calcule a média de recompensas totais (MR), a média de ações por episódio (MA), a taxa de vitórias (TV) e a taxa de derrotas (TD), sendo vitórias e derrotas medidas após o final de cada episódio. Arredonde os valores para 1 casas decimal.\n",
    "\n",
    "(a) MR é negativo, MA está entre 2 e 3, TV é igual a TD<br>\n",
    "(b) MR é 0.0, MA é 1.4, TV é menor do que TD<br>\n",
    "(c) MR é positivo, MA está entre 1 e 2, TV é igual a TD<br>\n",
    "(d) MR é negativo, MA está entre 1 e 2, TV é menor do que TD<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Blackjack-v0\n",
    "env = gym.make(\"Blackjack-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Episódio:  0\n",
      "Episódio:  10000\n",
      "Episódio:  20000\n",
      "Episódio:  30000\n",
      "Episódio:  40000\n",
      "Episódio:  50000\n",
      "Episódio:  60000\n",
      "Episódio:  70000\n",
      "Episódio:  80000\n",
      "Episódio:  90000\n",
      "Média de recompensas totais (MR): -0.4\n",
      "Média de passos por episódio (MA): 1.4\n",
      "Taxa de vitórias (TV): 0.3\n",
      "Taxa de derrotas (TD): 0.7\n"
     ]
    }
   ],
   "source": [
    "n_episodios_teste = 100000\n",
    "total_epochs = 0\n",
    "total_recs = 0\n",
    "wins = 0\n",
    "loss = 0\n",
    "\n",
    "# episodios\n",
    "for t in range(n_episodios_teste):\n",
    "    s = env.reset()\n",
    "\n",
    "    fim = False\n",
    "    epochs, rec_total_i = 0,0\n",
    "    \n",
    "    # episodio atual\n",
    "    while not fim:\n",
    "        # explorar espaco de acao\n",
    "        a = env.action_space.sample()\n",
    "        \n",
    "        # realizar acao\n",
    "        s_n, r, fim, info = env.step(a)\n",
    "        \n",
    "        epochs += 1\n",
    "        rec_total_i += r\n",
    "    \n",
    "    if rec_total_i > 0:\n",
    "        wins += 1\n",
    "\n",
    "    if rec_total_i < 0:\n",
    "        loss += 1\n",
    "\n",
    "    total_epochs += epochs\n",
    "    total_recs += rec_total_i\n",
    "\n",
    "        \n",
    "    if (t % 10000 == 0):\n",
    "        print(\"Episódio: \", t)\n",
    "\n",
    "print(\"Média de recompensas totais (MR): %.1f\" % (total_recs/n_episodios_teste))\n",
    "print(\"Média de passos por episódio (MA): %.1f\" % (total_epochs/n_episodios_teste))\n",
    "print(\"Taxa de vitórias (TV): %.1f\" % (wins/n_episodios_teste))\n",
    "print(\"Taxa de derrotas (TD): %.1f\" % (loss/n_episodios_teste))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "RNAP-04-Avaliacao_solucoes.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}