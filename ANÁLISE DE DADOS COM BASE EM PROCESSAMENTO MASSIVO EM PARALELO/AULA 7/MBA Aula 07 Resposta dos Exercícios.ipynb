{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MBA Aula 07 Resposta dos Exercícios.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"flekT6GFDN6m"},"source":["# <span style=\"color:blue\">MBA em Ciência de Dados</span>\n","# <span style=\"color:blue\">Análise de Dados com Base em Processamento Massivo em Paralelo</span>\n","\n","## <span style=\"color:blue\">Aula 07: Consultas OLAP usando Spark SQL</span>\n","## <span style=\"color:blue\">Apache Spark SQL</span>\n","\n","**Material Produzido por:**<br>\n",">**Profa. Dra. Cristina Dutra de Aguiar Ciferri**<br>\n",">**André Marcos Perez**<br> \n",">**Guilherme Muzzi da Rocha**<br>\n",">**Piero Lima Capelo**<br>\n","\n","**CEMEAI - ICMC/USP São Carlos**\n","\n","**Esta lista de exercícios contém 7 exercícios, os quais estão espalhados ao longo do texto. Por favor, procurem por EXERCÍCIO para encontrar a especificação dos exercícios e também o local no qual os comandos devem ser inseridos. Também é possível localizar os exercícios utilizando o menu de navegação. Por completude, o notebook possui todas as descrições apresentadas na parte prática da Aula 07.**\n","\n","**Recomenda-se fortemente que a lista de exercícios seja respondida antes de se consultar as respostas dos exercícios.** "]},{"cell_type":"markdown","metadata":{"id":"3o3dN_WLQcyD"},"source":["#1 Otimizador de Consultas Catalyst\n","\n","O componente mais importante do Spark SQL é o seu otimizador de consultas, chamado Catalyst. Catalyst é baseado em construtores de programação funcional e é implementado na linguagem de programação Scala. Sua implementação tem dois propósitos principais. O primeiro é permitir que novas técnicas de otimização e novas características possam ser facilmente adicionadas ao Spark SQL. O segundo propósito consiste em possibilitar que desenvolvedores externos estendam o otimizador de consultas, por exemplo, adicionando novas regras de otimização e provendo suporte para novos tipos de dados, dentre outros."]},{"cell_type":"markdown","metadata":{"id":"i7rEE-45DhOW"},"source":["## 1.1 Plano de Consulta\n","\n","Dada uma consulta em alto nível, existem diferentes estratégias de execução (ou seja, planos de consulta) alternativas para se processar essa consulta, principalmente se ela for complexa. A otimização de consultas consiste no processo de gerar e selecionar o plano de consulta mais eficiente dentre as diversas possibilidades disponíveis, ou seja, consiste no processo de selecionar o plano de consulta de menor custo. \n","\n","De fato, a quantidade de possíveis planos de consulta que podem ser gerados para processar uma consulta pode ser muito grande. Assim, no processamento de uma consulta, nem todos os planos possíveis são gerados e analisados, uma vez que o tempo gasto nesta atividade seria provavelmente excessivo, talvez superando o tempo de responder à consulta por meio de uma busca sequencial. Heurísticas são usualmente empregadas para diminuir o espaço de busca. \n","\n","Portanto, o otimizador de consultas em geral não produz uma solução que é a ótima ou de menor custo frente a todas as possibilidades existentes, mas produz uma solução que é a melhor frente a algumas dessas possibilidades. \n","\n"]},{"cell_type":"markdown","metadata":{"id":"KT2g66RkPd6d"},"source":["\n","##1.2 Técnicas de Otimização\n","\n","Duas técnicas amplamente utilizadas pelo otimizador de consultas são descritas a seguir.\n","\n","- Otimização baseada em regras. O otimizador de consultas baseado em regras tem por objetivo gerar apenas um subconjunto de planos a serem analisados, usado como base heurísticas. Cada plano gerado pelo otimizador de consultas baseado em regras consiste de uma expressão algébrica que determina a ordem na qual as operações deve ser executadas, de forma que todos os planos para uma determinada consulta sejam equivalentes.\n","\n","- Otimização baseada em custo. O otimizador de consultas baseado em custo identifica, para cada plano gerado, o custo para processar a consulta, e seleciona o plano de menor custo. Isso depende de diversos fatores, tais como quais as operações algébricas que encontram-se efetivamente implementadas por meio de algoritmos disponilizados e os índices disponíveis para processar a consulta. \n","\n","Catalyst realiza otimização de consultas baseada em regras e otimização de consultas baseada em custo para transformar uma consulta escrita em SQL em códigos que executam sobre RDDs e utilizam os princípios do modelo MapReduce e do sistema de arquivos distribuídos HDFS.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"BGeh8KdXwVCQ"},"source":["# 2 Constelação de Fatos da BI Solutions\n","\n","A aplicação de *data warehousing* da BI Solutions utiliza como base uma contelação de fatos, conforme descrita a seguir.\n","\n","**Tabelas de dimensão**\n","\n","- data (dataPK, dataCompleta, dataDia, dataMes, dataBimestre, dataTrimestre, dataSemestre, dataAno)\n","- funcionario (funcPK, funcMatricula, funcNome, funcSexo, funcDataNascimento, funcDiaNascimento, funcMesNascimento, funcAnoNascimento, funcCidade, funcEstadoNome, funcEstadoSigla, funcRegiaoNome, funcRegiaoSigla, funcPaisNome, funcPaisSigla)\n","- equipe (equipePK, equipeNome, filialNome, filialCidade, filialEstadoNome, filialEstadoSigla, filialRegiaoNome, filialRegiaoSigla, filialPaisNome, filialPaisSigla)\n","- cargo (cargoPK, cargoNome, cargoRegimeTrabalho, cargoEscolaridadeMinima, cargoNivel)\n","- cliente (clientePK, clienteNomeFantasia, clienteSetor, clienteCidade, clienteEstadoNome, clienteEstadoSigla, clienteRegiaoNome, clienteRegiaoSigla, clientePaisNome, clientePaisSigla)\n","\n","**Tabelas de fatos**\n","- pagamento (dataPK, funcPK, equipePK, cargoPK, salario, quantidadeLancamentos)\n","- negociacao (dataPK, equipePK, clientePK, receita, quantidadeNegociacoes)\n"]},{"cell_type":"markdown","metadata":{"id":"CCCNC64AzBG0"},"source":["## 2.1 Baixando o Módulo wget\n","\n","Para baixar os dados referentes ao esquema relacional da constelação de fatos da BI Solutions, é utilizado o módulo  **wget**. O comando a seguir realiza a instalação desse módulo. <br>"]},{"cell_type":"code","metadata":{"id":"3e0Eao1K0EYG"},"source":["#instalando o módulo wget\n","%%capture\n","!pip install -q wget\n","!mkdir data"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j56pVJ2hZ2i5"},"source":["## 2.2 Obtenção dos Dados das Tabelas de Dimensão\n","\n","Os comandos a seguir baixam os dados que povoam as tabelas de dimensão. "]},{"cell_type":"code","metadata":{"id":"46QzTpLJwfkW","cellView":"both"},"source":["#baixando os dados das tabelas de dimensão\n","import wget\n","\n","url = \"https://raw.githubusercontent.com/GuiMuzziUSP/Data_Mart_BI_Solutions/main/data.csv\"\n","wget.download(url, \"data/data.csv\")\n","\n","url = \"https://raw.githubusercontent.com/GuiMuzziUSP/Data_Mart_BI_Solutions/main/funcionario.csv\"\n","wget.download(url, \"data/funcionario.csv\")\n","\n","url = \"https://raw.githubusercontent.com/GuiMuzziUSP/Data_Mart_BI_Solutions/main/equipe.csv\"\n","wget.download(url, \"data/equipe.csv\")\n","\n","url = \"https://raw.githubusercontent.com/GuiMuzziUSP/Data_Mart_BI_Solutions/main/cargo.csv\"\n","wget.download(url, \"data/cargo.csv\")\n","\n","url = \"https://raw.githubusercontent.com/GuiMuzziUSP/Data_Mart_BI_Solutions/main/cliente.csv\"\n","wget.download(url, \"data/cliente.csv\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0o-dC7feszRc"},"source":["## 2.3 Obtenção dos Dados Tabelas de Fatos\n","\n","Os comandos a seguir baixam os dados que povoam as tabelas de fatos. "]},{"cell_type":"code","metadata":{"id":"XWM-CUFgBl_8"},"source":["#baixando os dados das tabelas de fatos\n","url = \"https://raw.githubusercontent.com/GuiMuzziUSP/Data_Mart_BI_Solutions/main/pagamento.csv\"\n","wget.download(url, \"data/pagamento.csv\")\n","\n","url = \"https://raw.githubusercontent.com/GuiMuzziUSP/Data_Mart_BI_Solutions/main/negociacao.csv\"\n","wget.download(url, \"data/negociacao.csv\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sO16-7-jOioq"},"source":["# 3 Apache Spark Cluster"]},{"cell_type":"markdown","metadata":{"id":"YVEgY9qKflBV"},"source":["## 3.1 Instalação\n","\n","Neste *notebook* é criado um *cluster* Spark composto apenas por um **nó mestre**. Ou seja, o *cluster* não possui um ou mais **nós de trabalho** e o **gerenciador de cluster**. Nessa configuração, as tarefas (*tasks*) são realizadas no próprio *driver* localizado no **nó mestre**."]},{"cell_type":"markdown","metadata":{"id":"KaM-OnIjgLS2"},"source":["Para que o cluster possa ser criado, primeiramente é instalado o Java Runtime Environment (JRE) versão 8. "]},{"cell_type":"code","metadata":{"id":"NXls3bfoglKW"},"source":["#instalando Java Runtime Environment (JRE) versão 8\n","%%capture\n","!apt-get remove openjdk*\n","!apt-get update --fix-missing\n","!apt-get install openjdk-8-jdk-headless -qq > /dev/null"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7BQzZfDYhb4j"},"source":["Na sequência, é feito o *download* do Apache Spark versão 3.0.0."]},{"cell_type":"code","metadata":{"id":"8a_Yv59zg3gm"},"source":["#baixando Apache Spark versão 3.0.0\n","%%capture\n","!wget -q https://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop2.7.tgz\n","!tar xf spark-3.0.0-bin-hadoop2.7.tgz && rm spark-3.0.0-bin-hadoop2.7.tgz"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RETWX6wqhkLf"},"source":["Na sequência, são configuradas as variáveis de ambiente JAVA_HOME e SPARK_HOME. Isto permite que tanto o Java quanto o Spark possam ser encontrados."]},{"cell_type":"code","metadata":{"id":"iZpR7NwOh2EB"},"source":["import os\n","#configurando a variável de ambiente JAVA_HOME\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","#configurando a variável de ambiente SPARK_HOME\n","os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-bin-hadoop2.7\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ql0z7Ro1iHQb"},"source":["Por fim, são instalados dois pacotes da linguagem de programação Python, cujas funcionalidades são descritas a seguir.\n","\n","> **Pacote findspark:** Usado para ler a variável de ambiente SPARK_HOME e armazenar seu valor na variável dinâmica de ambiente PYTHONPATH. Como resultado, Python pode encontrar a instalação do Spark. \n","\n","> **Pacote pyspark:** PySpark é a API do Python para Spark. Ela possibilita o uso de Python, considerando que o *framework* Apache Spark encontra-se desenvolvido na linguagem de programação Scala. "]},{"cell_type":"code","metadata":{"id":"5oSYOwKljPf5"},"source":["%%capture\n","#instalando o pacote findspark\n","!pip install -q findspark==1.4.2\n","#instalando o pacote pyspark\n","!pip install -q pyspark==3.0.0"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eAaLyjPzmIwZ"},"source":["## 3.2 Conexão\n","\n","PySpark não é adicionado ao *sys.path* por padrão. Isso significa que não é possível importá-lo, pois o interpretador da linguagem Python não sabe onde encontrá-lo. \n","\n","Para resolver esse aspecto, é necessário instalar o módulo `findspark`. Esse módulo mostra onde PySpark está localizado. Os comandos a seguir têm essa finalidade.\n"]},{"cell_type":"code","metadata":{"id":"-zm1pBTEmjp4"},"source":["#importando o módulo findspark\n","import findspark\n","#carregando a variávels SPARK_HOME na variável dinâmica PYTHONPATH\n","findspark.init()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZDqfefF7YUab"},"source":["Depois de configurados os pacotes e módulos e inicializadas as variáveis de ambiente, é possível iniciar o uso do Spark na aplicação de `data warehousing`. Para tanto, é necessário importar o comando `SparkSession` do módulo `pyspark.sql`. São utilizados os seguintes conceitos: <br>\n","\n","- `SparkSession`: permite a criação de `DataFrames`. Como resultado, as tabelas relacionais podem ser manipuladas por meio de `DataFrames` e é possível realizar consultas OLAP por meio de comandos SQL. <br>\n","- `builder`: cria uma instância de SparkSession. <br>\n","- `appName`: define um nome para a aplicação, o qual pode ser visto na interface de usuário web do Spark. <br> \n","- `master`: define onde está o nó mestre do *cluster*. Como a aplicação é executada localmente e não em um *cluster*, indica-se isso pela *string* `local` seguida do parâmetro `[*]`. Ou seja, define-se que apenas núcleos locais são utilizados. \n","- `getOrCreate`: cria uma SparkSession. Caso ela já exista, retorna a instância existente. \n","\n","\n","**Observação**: A lista completa de todos os parâmetros que podem ser utilizados na inicialização do *cluster* pode ser encontrada neste [link](https://spark.apache.org/docs/latest/spark-standalone.html#cluster-launch-scripts)."]},{"cell_type":"code","metadata":{"id":"9TxljJ_cwBCy"},"source":["from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.appName(\"pyspark-notebook\").master(\"local[*]\").getOrCreate()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5qL9SiR_pQE2"},"source":["# 4 Preparação dos Dados\n"]},{"cell_type":"markdown","metadata":{"id":"WtUGn-uyBJWY"},"source":["## 4.1 Geração dos DataFrames\n","\n","Para a leitura dos dados dos arquivos .csv, é utilizado o método `spark.read.csv`. Seus parâmetros são:\n","\n","\n","- `path`: endereço do arquivo que é lido.\n","- `header`: indica se o arquivo possui um cabeçalho.\n","- `sep`: especifica o caractere que separa os campos do arquivo.\n","\n"]},{"cell_type":"code","metadata":{"cellView":"both","id":"FNR-3dV6oYk4"},"source":["#criando e exibindo o DataFrame para a tabela de dimensão cargo\n","cargo = spark.read.csv(path=\"data/cargo.csv\", header=True, sep=\",\")\n","cargo.show(5)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LPCF-SyBtuPW"},"source":["#criando e exibindo o DataFrame para a tabela de dimensão cliente\n","cliente = spark.read.csv(path=\"data/cliente.csv\", header=True, sep=\",\")\n","cliente.show(5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w3p9dLUKts73"},"source":["#criando e exibindo o DataFrame para a tabela de dimensão data\n","data = spark.read.csv(path=\"data/data.csv\", header=True, sep=\",\") \n","data.show(5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Obef8NfyttuJ"},"source":["#criando e exibindo o DataFrame para a tabela de dimensão equipe\n","equipe = spark.read.csv(path=\"data/equipe.csv\", header=True, sep=\",\")\n","equipe.show(5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RsF4rcS7Zp4O"},"source":["#criando e exibindo o DataFrame para a tabela de fatos funcionario\n","funcionario = spark.read.csv(path=\"data/funcionario.csv\", header=True, sep=\",\")\n","funcionario.show(5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s4aZ0M6OZvg2"},"source":["#criando e exibindo o DataFrame para a tabela de fatos negociacao\n","negociacao = spark.read.csv(path=\"data/negociacao.csv\", header=True, sep=\",\")\n","negociacao.show(5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DzVUaFptodHJ"},"source":["#criando e exibindo o DataFrame para a tabela de fatos pagamento\n","pagamento = spark.read.csv(path=\"data/pagamento.csv\", header=True, sep=\",\")\n","pagamento.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"09knZQIwO03y"},"source":["## 4.2 Criação de Visões Temporárias\n","\n","Para que seja possível executar consultas SQL usando Spark SQL, é necessário criar visões temporárias. Uma visão temporária é uma forma na qual um DataFrame pode ser consultado como se fosse uma tabela.\n","\n","Para tanto, deve ser utilizado o método  `createOrReplaceTempView` e deve ser passado como parâmetro uma *string* que é o nome da tabela que é criada a partir do DataFrame.  Os comandos a seguir criam uma visão temporária para cada DataFrame da aplicação de *data warehousing*. \n"," "]},{"cell_type":"code","metadata":{"id":"nB4dFUqHoiaW"},"source":["#criando as visões temporárias para as tabelas de dimensão\n","cargo.createOrReplaceTempView(\"cargo\")\n","cliente.createOrReplaceTempView(\"cliente\")\n","data.createOrReplaceTempView(\"data\")\n","equipe.createOrReplaceTempView(\"equipe\")\n","funcionario.createOrReplaceTempView(\"funcionario\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7PQgcY3honGm"},"source":["#criando a visão temporária para as tabelas de fatos\n","negociacao.createOrReplaceTempView(\"negociacao\")\n","pagamento.createOrReplaceTempView(\"pagamento\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ss0pmgplPAL3"},"source":["# 5 Execução de Consultas com Foco nas Operações OLAP"]},{"cell_type":"markdown","metadata":{"id":"MABYjSYf7L4g"},"source":["## 5.1 Operação Slice and Dice \n","\n","**Definição**: Restringe os dados sendo analisados a um subconjunto desses dados.\n","\n","- Slice: corte para um valor fixo, diminuindo a dimensionalidade do cubo.\n","- Dice: seleção de faixas de valores.\n","\n","**Exemplo de consulta**: Qual a quantidade de pagamentos realizados no mês de setembro de 2020?"]},{"cell_type":"code","metadata":{"id":"DGMSoyQRoqnz"},"source":["query = \"\"\"\n","SELECT CAST(SUM(quantidadeLancamentos) AS INTEGER) AS `Quantidade de Lançamentos`\n","FROM data JOIN pagamento ON (data.dataPK = pagamento.dataPK) \n","WHERE dataAno = 2020 \n","      AND dataMes = 9\n","\"\"\"\n","\n","spark.sql(query).show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FjeYu20EoUyO"},"source":["### **EXERCÍCIO 1** \n","\n","Qual o total de negociações realizadas nos meses de Abril, Maio e Junho de 2016? Renomeie o resultado final como \"Quantidade de Negociações\". \n","\n","Dica: utilize a medida numérica `quantidadeNegociacoes` presente na tabela `negociacao` e os dados históricos presentes na tabela `data`."]},{"cell_type":"code","metadata":{"id":"ToTm8e6chF09"},"source":["# resposta do exercício 1\n","\n","query = \"\"\"\n","SELECT CAST(SUM(quantidadeNegociacoes) AS INTEGER) AS `Quantidade de Negociações`\n","FROM data JOIN negociacao ON (data.dataPK = negociacao.dataPK) \n","WHERE dataAno = 2016 \n","      AND dataTrimestre = 2\n","\"\"\"\n","\n","spark.sql(query).show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"crCnMYwi7rIm"},"source":["## 5.2 Operações Drill-Down e Roll-Up\n","\n","**Definição**: Analisam os dados considerando níveis progressivos de agregação.\n","\n","- Drill-down: níveis de agregação progressivamente mais detalhados, ou de menor granularidade.\n","- Roll-up: níveis de agregação progressivamente menos detalhados, ou de maior granularidade."]},{"cell_type":"markdown","metadata":{"id":"YC-qVTVIpyX4"},"source":["Para ilustrar as operações de drill-down e roll-up, considere a consulta base definida a seguir.\n","\n","**Consulta base:** Qual o valor gasto em salários por ano, considerando cada **semestre**?"]},{"cell_type":"code","metadata":{"id":"YnaUOx3uow4Z"},"source":["query = \"\"\"\n","SELECT dataAno, dataSemestre, ROUND(SUM(salario),2) AS `Valor gasto em salários por semestre`\n","FROM data JOIN pagamento ON data.dataPK = pagamento.dataPK \n","GROUP BY dataAno, dataSemestre\n","ORDER BY dataAno, dataSemestre\n","\"\"\"\n","\n","spark.sql(query).show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1PzzQT1Xz-4-"},"source":["**Exemplo de consulta drill-down:** Qual o valor gasto em salários por ano, considerando cada trimestre?"]},{"cell_type":"code","metadata":{"id":"f8EZJPBSquj4"},"source":["query = \"\"\"\n","SELECT dataAno, dataTrimestre, ROUND(SUM(salario),2) AS `Valor gasto em salários por trimestre`\n","FROM data JOIN pagamento ON (data.dataPK = pagamento.dataPK) \n","GROUP BY dataAno, dataTrimestre\n","ORDER BY dataAno, dataTrimestre\n","\"\"\"\n","\n","spark.sql(query).show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7bFWeBr87xpN"},"source":["**Exemplo de consulta roll-up:** Qual o valor gasto em salários por ano?"]},{"cell_type":"code","metadata":{"id":"dPKhALIJrllz","outputId":"0ca0ff15-715d-4972-8f30-d29c310728cb","colab":{"base_uri":"https://localhost:8080/"}},"source":["query = \"\"\"\n","SELECT dataAno, ROUND(SUM(salario),2) AS `Valor gasto em salários por ano`\n","FROM data JOIN pagamento ON (data.dataPK = pagamento.dataPK) \n","GROUP BY dataAno\n","ORDER BY dataAno\n","\"\"\"\n","\n","spark.sql(query).show()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["+-------+-------------------------------+\n","|dataAno|Valor gasto em salários por ano|\n","+-------+-------------------------------+\n","|   2016|                     4442617.08|\n","|   2017|                      9775279.8|\n","|   2018|                   1.49355264E7|\n","|   2019|                  1.856766636E7|\n","|   2020|                  1.856766636E7|\n","+-------+-------------------------------+\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"nG_cvrxlrdx7"},"source":["### **EXERCÍCIO 2** \n","\n","Faça 3 consultas SQL, uma para cada consulta especificada a seguir.\n","\n","- **Consulta base:** Qual é a soma da receita recebida em cada um dos semestres do ano de 2019? Renomeie o resultado final como \"Valor recebido em receitas por semestre no ano de 2019\".\n","\n","- **Consulta drill-down:** Qual é a soma da receita recebida em cada um dos trimestres do ano de 2019? Renomeie o resultado final como \"Valor recebido em receitas por trimestre no ano de 2019\".\n","\n","- **Consulta roll-up:** Qual é a soma da receita recebida no ano de 2019? Renomeie o resultado final como \"Valor recebido em receitas no ano de 2019\".\n","\n","Dica: utilize a medida numérica `receita` presente na tabela `negociacao` e os dados históricos presentes na tabela `data`."]},{"cell_type":"code","metadata":{"id":"YRzqGoTnw5cG"},"source":["# resposta do exercício 2 - consulta base\n","\n","query = \"\"\"\n","SELECT dataAno, dataSemestre, ROUND(SUM(receita),2) AS `Valor recebido em receitas por semestre no ano de 2019`\n","FROM data JOIN negociacao ON data.dataPK = negociacao.dataPK\n","WHERE dataAno = 2019\n","GROUP BY dataAno, dataSemestre\n","ORDER BY dataAno, dataSemestre\n","\"\"\"\n","\n","spark.sql(query).show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"F_wIArbCyHtO"},"source":["# resposta do exercício 2 - consulta drill-down\n","query = \"\"\"\n","SELECT dataAno, dataTrimestre, ROUND(SUM(receita),2) AS `Valor recebido em receitas por trimestre no ano de 2019`\n","FROM data JOIN negociacao ON data.dataPK = negociacao.dataPK\n","WHERE dataAno = 2019\n","GROUP BY dataAno, dataTrimestre\n","ORDER BY dataAno, dataTrimestre\n","\"\"\"\n","\n","spark.sql(query).show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ylFLmjXkyhx8"},"source":["# resposta do exercício 2 - consulta roll-up\n","query = \"\"\"\n","SELECT dataAno, ROUND(SUM(receita),2) AS `Valor recebido em receitas no ano de 2019`\n","FROM data JOIN negociacao ON data.dataPK = negociacao.dataPK\n","WHERE dataAno = 2019\n","GROUP BY dataAno\n","ORDER BY dataAno\n","\"\"\"\n","\n","spark.sql(query).show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h1kRDhciLQnj"},"source":["## 5.3 Operação Pivot\n","\n","**Definição:** Reorienta a visão multidimensional dos dados, oferecendo diferentes perspectivas dos mesmos dados."]},{"cell_type":"markdown","metadata":{"id":"eaXAXkeF1atH"},"source":["Para ilustrar a operação pivot, considere a consulta base definida a seguir. \n","\n","**Consulta base:**  Qual o valor gasto em salários por ano, considerando cada nível de cargo?"]},{"cell_type":"code","metadata":{"id":"NiA8yW23o9H5"},"source":["query = \"\"\"\n","SELECT dataAno, cargoNivel, ROUND(SUM(salario),2) AS `Gastos em Salários`\n","FROM pagamento JOIN data ON pagamento.dataPK = data.dataPK \n","               JOIN cargo ON pagamento.cargoPK = cargo.cargoPK \n","GROUP BY dataAno, cargoNivel\n","ORDER BY dataAno, cargoNivel\n","\"\"\"\n","\n","spark.sql(query).show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DPBaswdrLWv6"},"source":["**Exemplo de consulta pivot:** Qual o valor gasto em salários por nível de cargo, considerando cada ano?"]},{"cell_type":"code","metadata":{"id":"v63e5Yps2CSD"},"source":["query = \"\"\"\n","SELECT cargoNivel, dataAno, ROUND(SUM(salario),2) AS `Gastos em Salários`\n","FROM pagamento JOIN data ON pagamento.dataPK = data.dataPK \n","               JOIN cargo ON pagamento.cargoPK = cargo.cargoPK \n","GROUP BY cargoNivel, dataAno\n","ORDER BY cargoNivel, dataAno\n","\"\"\"\n","\n","spark.sql(query).show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3FNsObn_vVmp"},"source":["### **EXERCÍCIO 3**\n","\n","Faça 2 consultas SQL, uma para cada consulta especificada a seguir.\n","\n","- **Consulta base:** Qual é a receita média gerada pelos clientes considerando seus segmentos (ou seja, os setores nos quais eles atuam), em cada um dos anos? Renomeie o resultado final como \"Total de Receitas\".\n","\n","- **Consulta pivot:** Qual é a receita média gerada pelos clientes em cada um dos anos, considerando seus segmentos (ou seja, os setores nos quais eles atuam)? Renomeie o resultado final como \"Total de Receitas\".\n","\n","Dica: utilize a medida numérica `receita` presente na tabela `negociacao`, os dados de clientes presentes na tabela `cliente` e os dados históricos presentes na tabela `data`. "]},{"cell_type":"code","metadata":{"id":"rEEKHpb15BMq"},"source":["# resposta do exercício 3 - consulta base\n","\n","query = \"\"\"\n","SELECT dataAno, clienteSetor, ROUND(AVG(receita),2) AS `Total de Receitas`\n","FROM negociacao JOIN data ON data.dataPK = negociacao.dataPK\n","                JOIN cliente on cliente.clientePK = negociacao.clientePK \n","GROUP BY dataAno, clienteSetor\n","ORDER BY dataAno, clienteSetor\n","\"\"\"\n","\n","spark.sql(query).show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ocDbNvCS6irD"},"source":["# resposta do exercício 3 - consulta pivot\n","\n","query = \"\"\"\n","SELECT clienteSetor, dataAno, ROUND(AVG(receita),2) AS `Total de Receitas`\n","FROM negociacao JOIN data ON data.dataPK = negociacao.dataPK\n","                JOIN cliente on cliente.clientePK = negociacao.clientePK \n","GROUP BY clienteSetor, dataAno \n","ORDER BY clienteSetor, dataAno\n","\"\"\"\n","\n","spark.sql(query).show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UtJILugUN2K_"},"source":["## 5.4 Operação Drill-Across\n","\n","**Definição:** Compara medidas numéricas de tabelas de fatos diferentes, utilizando pelo menos uma dimensão em comum. \n","\n","**Exemplo de consulta:** Qual o total gasto com salários e qual o total de receitas recebidas, considerando cada ano?"]},{"cell_type":"code","metadata":{"id":"oyoWQDJiIgQx"},"source":["# utilizando a cláusula JOIN ... ON ...\n","query = \"\"\"\n","SELECT anoPag AS `Ano`, ROUND(salario,2) AS `Total Gasto com Salários`, ROUND(receita,2) AS `Total de Receitas Recebidas`\n","FROM ( SELECT dataAno, SUM(salario)  \n","       FROM pagamento JOIN data on data.dataPK = pagamento.dataPK\n","       GROUP BY dataAno\n","      ) AS pag(anoPag, salario)\n","     JOIN \n","     ( SELECT dataAno, SUM(receita)\n","       FROM negociacao JOIN data ON data.dataPK = negociacao.dataPK\n","       GROUP BY dataAno\n","      ) AS neg(anoNeg, receita) \n","     ON anoPag = anoNeg \n","ORDER BY anoPag\n","\"\"\"\n","\n","spark.sql(query).show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CP9gstSQ9XUu"},"source":["# utilizando a cláusula WHERE\n","query = \"\"\"\n","SELECT anoPag AS `Ano`, ROUND(salario,2) AS `Total Gasto com Salários`, ROUND(receita,2) AS `Total de Receitas Recebidas`\n","FROM ( SELECT dataAno, SUM(salario) \n","       FROM pagamento JOIN data on data.dataPK = pagamento.dataPK\n","       GROUP BY dataAno\n","      ) AS pag(anoPag, salario), \n","     ( SELECT dataAno, SUM(receita)\n","       FROM negociacao JOIN data ON data.dataPK = negociacao.dataPK\n","       GROUP BY dataAno\n","      ) AS neg(anoNeg, receita) \n","WHERE anoPag = anoNeg \n","ORDER BY anoPag\n","\"\"\"\n","\n","spark.sql(query).show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h1qyZMktxuwc"},"source":["### **EXERCÍCIO 4**\n","\n","Qual o total gasto em salários e qual o total de receitas recebidas, considerando apenas os anos de 2017, 2018 e 2019? Renomeie a saída para \"Ano\", \"Total Gasto com Salários\" e \"Total de Receitas Recebidas\".\n","\n","Dica: utilize a medida numérica `salario` da tabela `pagamento`, a medida numérica `receita` da tabela `negociacao` e os dados históricos presentes na tabela `data`."]},{"cell_type":"code","metadata":{"id":"yHMnaRdr-xe6"},"source":["# resposta do exercício 4\n","\n","query = \"\"\"\n","SELECT anoPag AS `Ano`, ROUND(salario,2) AS `Total Gasto com Salários`, ROUND(receita,2) AS `Total de Receitas Recebidas`\n","FROM ( SELECT dataAno, SUM(salario) \n","       FROM pagamento JOIN data on data.dataPK = pagamento.dataPK\n","       WHERE dataAno BETWEEN 2017 AND 2019\n","       GROUP BY dataAno\n","      ) AS pag(anoPag, salario)\n","     JOIN \n","     ( SELECT dataAno, SUM(receita)\n","       FROM negociacao JOIN data ON data.dataPK = negociacao.dataPK\n","       WHERE dataAno BETWEEN 2017 AND 2019\n","       GROUP BY dataAno\n","      ) AS neg(anoNeg, receita) \n","     ON anoPag = anoNeg \n","ORDER BY anoPag\n","\"\"\"\n","\n","spark.sql(query).show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mTjo_CX1G5jy"},"source":["## 5.5 Extensões ROLLUP, CUBE e GROUPING SETS \n","\n","**Definição:** Constrém vários níveis de agregação.\n","\n","- ROLLUP: criação de subtotais para as combinações dos atributos da lista de agrupamento de acordo com a ordem desses atributos. São criados n+1 níveis de agregação, sendo n o número de atributos especificados na lista de agrupamento.\n","\n","- CUBE: criação de subtotais para todas as combinações dos atributos da lista de agrupamento. São criados 2ˆn (2 elevado a n) níveis, sendo n o número de atributos especificados na lista de agrupamento.\n","\n","- GROUPING SETS: criação de subtotais para quaisquer combinações de atributos de agrupamentos. É criada a quantidade de subtotais especificados na lista de níveis de agregação desejados. \n"]},{"cell_type":"markdown","metadata":{"id":"5RRlVccmYhV_"},"source":["\n","**Exemplo de consulta com ROLLUP:** Liste as agregações que podem ser geradas a partir da soma da receita por setor do cliente e por cidade do cliente, para totais de receita superiores a 3.000.000,00. Crie subtotais considerando a ordem dos atributos na lista de agrupamento."]},{"cell_type":"code","metadata":{"id":"BwZHLSsCW0zD"},"source":["query = \"\"\"\n","SELECT clienteSetor, clientecidade, ROUND(SUM(receita),2) AS `Total de Receitas`\n","FROM cliente JOIN negociacao ON cliente.clientePk = negociacao.clientePK\n","GROUP BY ROLLUP (clienteSetor, clienteCidade)\n","HAVING SUM(receita) > 3000000\n","ORDER BY clienteSetor, clienteCidade\n","\"\"\"\n","\n","spark.sql(query).show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZUFL8-CcYojA"},"source":["**Exemplo de consulta com GROUPING SETS com semântica de ROLLUP:** Liste todas as agregações que podem ser geradas a partir da soma da receita por setor do cliente e por cidade do cliente, para totais de receita superiores a 3.000.000,00. Crie subtotais considerando a ordem dos atributos na lista de agrupamento."]},{"cell_type":"code","metadata":{"id":"Tq9x3HMyPwHe"},"source":["query = \"\"\"\n","SELECT clienteSetor, clientecidade, ROUND(SUM(receita),2) AS `Total de Receitas`\n","FROM cliente JOIN negociacao ON cliente.clientePk = negociacao.clientePK\n","GROUP BY GROUPING SETS ((clienteSetor, clienteCidade), (clienteSetor), ())\n","HAVING SUM(receita) > 3000000\n","ORDER BY clienteSetor, clienteCidade\n","\"\"\"\n","\n","spark.sql(query).show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IjjDB_mtPkzw"},"source":["**Exemplo de consulta com CUBE:** Liste todas as agregações que podem ser geradas a partir da soma da receita por setor do cliente e por cidade do cliente, para totais de receita superiores a 3.000.000,00."]},{"cell_type":"code","metadata":{"id":"v_5lF9EVFZr6"},"source":["query = \"\"\"\n","SELECT clienteSetor, clientecidade, ROUND(SUM(receita),2) AS `Total de Receitas`\n","FROM cliente JOIN negociacao ON cliente.clientePk = negociacao.clientePK\n","GROUP BY CUBE (clienteSetor, clienteCidade)\n","HAVING SUM(receita) > 3000000\n","ORDER BY clienteSetor, clienteCidade\n","\"\"\"\n","\n","spark.sql(query).show(40)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jZ0y48oPbwiO"},"source":["**Exemplo de consulta com GROUPING SETS com semântica de CUBE:** Liste todas as agregações que podem ser geradas a partir da soma da receita por setor do cliente e por cidade do cliente, para totais de receita superiores a 3.000.000,00. Crie subtotais considerando a ordem dos atributos na lista de agrupamento."]},{"cell_type":"code","metadata":{"id":"W8oaCRpEb29W"},"source":["query = \"\"\"\n","SELECT clienteSetor, clientecidade, ROUND(SUM(receita),2) AS `Total de Receitas`\n","FROM cliente JOIN negociacao ON cliente.clientePk = negociacao.clientePK\n","GROUP BY GROUPING SETS ((clienteSetor, clienteCidade), (clienteSetor), (clienteCidade), ())\n","HAVING SUM(receita) > 3000000\n","ORDER BY clienteSetor, clienteCidade\n","\"\"\"\n","\n","spark.sql(query).show(40)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tgV9cze8MBY3"},"source":["# 6 Execução de Consultas com Foco na Tomada de Decisão\n","As consultas OLAP requisitadas por usuários de sistemas de suporte à decisão usualmente requerem que várias operações OLAP sejam realizadas simultaneamente. A seguir são ilustrados exemplos de consultas OLAP que podem ser requisitadas para a tomada de decisão estratégica."]},{"cell_type":"markdown","metadata":{"id":"OcaRLxX_Jd1w"},"source":["## 6.1 Consulta 1\n","\n","Qual é a média dos salários recebidos por nível do cargo e por sexo no ano passado (ou seja, no ano de 2019)?\n","\n","Para se realizar esta consulta, é necessário obter dados das tabelas de dimensão `cargo`, `funcionario` e `data`, bem como da tabela de fatos `pagamento`. A junção estrela deve ocorrer considerando as seguintes integridades referenciais:\n","- `pagamento.cargoPK = cargo.cargoPK`\n","- `pagamento.funcPK = funcionario.funcPK`\n","- `pagamento.dataPK = data.dataPK` "]},{"cell_type":"code","metadata":{"id":"UeKlyc-mMt64"},"source":["# utilizando a cláusula JOIN ... ON ...\n","query = \"\"\"\n","SELECT cargoNivel, funcSexo, ROUND(AVG(salario),2) AS `Média dos Salários`\n","FROM pagamento JOIN data ON data.dataPK = pagamento.dataPK\n","               JOIN cargo ON cargo.cargoPK = pagamento.cargoPK\n","               JOIN funcionario ON funcionario.funcPK = pagamento.funcPK \n","WHERE dataAno = 2019\n","GROUP BY cargoNivel, funcSexo\n","ORDER BY cargoNivel, funcSexo\n","\"\"\"\n","\n","spark.sql(query).show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"so-Hec1xeZHL"},"source":["# utilizando a cláusula WHERE\n","query = \"\"\"\n","SELECT cargoNivel, funcSexo, ROUND(AVG(salario),2) AS `Média dos Salários`\n","FROM pagamento, cargo, funcionario, data\n","WHERE data.dataPK = pagamento.dataPK\n","      AND cargo.cargoPK = pagamento.cargoPK\n","      AND funcionario.funcPK = pagamento.funcPK\n","      AND dataAno = 2019\n","GROUP BY cargoNivel, funcSexo\n","ORDER BY cargoNivel, funcSexo\n","\"\"\"\n","\n","spark.sql(query).show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"varyr5djDP2s"},"source":["## 6.2 Consulta 2\n","\n","Qual o total de gastos em salários considerando os estados nos quais as equipes estão localizadas no último trimestre deste ano (ou seja, o trimestre 3 do ano de 2020)? \n","\n","Para se realizar esta consulta, é necessário obter dados das tabelas de dimensão `equipe` e `data`, bem como da tabela de fatos `pagamento`. A junção estrela deve ocorrer considerando as seguintes integridades referenciais:\n","- `pagamento.dataPK = data.dataPK`\n","- `pagamento.equipePK = equipe.equipePK`\n"]},{"cell_type":"code","metadata":{"id":"KSgE7xtPkGYf"},"source":["query = \"\"\"\n","SELECT filialEstadoNome, ROUND(SUM(salario),2) AS Total\n","FROM pagamento JOIN data ON data.dataPK = pagamento.dataPK \n","               JOIN equipe ON equipe.equipePK = pagamento.equipePK\n","WHERE dataAno = 2019\n","      AND dataTrimestre = 3\n","GROUP BY filialEstadoNome\n","ORDER BY Total \n","\"\"\"\n","\n","spark.sql(query).show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RAqNkANBOZSI"},"source":["## 6.3 Consulta 3\n","\n","Qual o custo/benefício das equipes quando analisado o último semestre deste ano (ou seja, semestre 1 do ano de 2020)?\n","\n","A idea da consulta é relacionar os gastos em salários e os ganhos em receitas considerando cada equipe e o período especificado. Portanto, para se realizar essa consulta, é necessário obter dados das tabelas de dimensão `equipe` e `data`, bem como das tabelas de fatos `pagamento` e `negociacao`. \n","\n"," A junção estrela deve ocorrer considerando as seguintes integridades referenciais:\n","- `pagamento.dataPK = data.dataPK`\n","- `pagamento.equipePK = equipe.equipe.PK`\n","- `negociacao.dataPK = data.dataPK`\n","- `negociacao.equipePK = equipe.equipe.PK`\n"]},{"cell_type":"markdown","metadata":{"id":"HMyeJDwisUgu"},"source":["Uma observação muito importante refere-se ao fato que, para evitar dubiedade nas respostas, elas devem ser feitas sempre considerando a chave primária, desde que a chave primária identifica univocamente cada tupla. Depois de ser resolvida a consulta em termos da chave primária, então deve ser obtido os demais atributos a serem exibidos."]},{"cell_type":"code","metadata":{"id":"MybQ9xten5-x"},"source":["query = \"\"\"\n","-- obtendo os dados a serem exibidos na resposta\n","SELECT equipeNome, filialNome, ROUND(Lucro,2)\n","FROM equipe,\n","(\n","   SELECT pag.equipePK AS retornaEquipePK, (TotalReceita - TotalSalario) AS Lucro\n","   FROM ( \n","        -- investigando os gastos em salarios de cada equipe no último semestre deste ano \n","        SELECT equipePK, SUM(salario) AS TotalSalario\n","        FROM pagamento JOIN data ON data.dataPK = pagamento.dataPK\n","        WHERE dataSemestre = 1 \n","              AND dataAno = 2020\n","        GROUP BY equipePK \n","        ORDER BY equipePK \n","        ) AS pag,  \n","        (\n","        -- investigando os ganhos em receitas de cada equipe no último semestre deste ano   \n","        SELECT equipePK, SUM(receita) AS TotalReceita\n","        FROM negociacao JOIN data ON data.dataPK = negociacao.dataPK\n","        WHERE dataSemestre = 1 \n","              AND dataAno = 2020\n","        GROUP BY equipePK \n","        ORDER BY equipePK\n","        ) AS neg\n","   WHERE pag.equipePK = neg.equipePK\n","   ) AS parte\n","WHERE equipe.equipePK = parte.retornaEquipePK\n","ORDER BY Lucro DESC\n","\n","\"\"\"\n","\n","spark.sql(query).show(5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3HpvYqBcvDS0"},"source":["## 6.4 Consultas Exercício"]},{"cell_type":"markdown","metadata":{"id":"jN4Ss3OWvWQT"},"source":["### **EXERCÍCIO 5**\n","\n","Qual o total de receita gerada no último semestre de 2020 (ou seja, semestre 2 do ano de 2020), considerando cada setor de clientes? Ordene o resultado final pelo total de receita em ordem ascendente. Renomeie o total de receitas como \"Total de Receitas\". "]},{"cell_type":"code","metadata":{"id":"Boxgnt9qwE5E"},"source":["# resposta do exercício 5\n","\n","query = \"\"\"\n","SELECT clienteSetor, ROUND(SUM(receita), 2) As `Total de Receitas`\n","FROM negociacao JOIN data ON data.dataPK = negociacao.dataPK\n","                JOIN cliente ON cliente.clientePK = negociacao.clientePK\n","WHERE dataAno = 2020 AND dataSemestre = 2\n","GROUP BY clienteSetor\n","ORDER BY SUM(receita) \n","\"\"\"\n","\n","spark.sql(query).show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-Itsh4D7xJxQ"},"source":["### **EXERCÍCIO 6** \n","\n","No exercício anterior, foi calculado o total de receita gerada no último semestre de 2020 para cada setor de atuação dos clientes. Analisando-se os resultados obtidos, é possível identificar o setor de maior receita. Utilizando esse setor, responda à seguinte consulta analítica. \n","\n","Qual o total de receita gerada por mês para o setor de clientes com maior receita no último semestre de 2020? Ordene o resultado final por mês em ordem ascendente. Renomeie o total de receitas como \"Total de Receitas\". "]},{"cell_type":"code","metadata":{"id":"57lxJu_5xWRx"},"source":["# resposta do exercício 6\n","\n","query = \"\"\"\n","SELECT dataMes, ROUND(SUM(receita), 2) AS `Total de Receitas`\n","FROM negociacao JOIN data ON data.dataPK = negociacao.dataPK \n","                JOIN cliente ON cliente.clientePK = negociacao.clientePK \n","WHERE dataAno = 2020 \n","      AND dataSemestre = 2 \n","      AND clienteSetor = 'BEBIDAS E ALIMENTOS' \n","GROUP BY dataMes \n","ORDER BY CAST(dataMes AS INTEGER)\n","\"\"\"\n","\n","spark.sql(query).show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SfI-JNP_ysxy"},"source":["### **EXERCÍCIO 7** \n","\n","No exercício anterior, foi calculado o total de receita gerada por mês para o setor de clientes com maior receita no último semestre de 2020. Analisando-se os resultados obtidos, é possível identificar o mês de maior receita. Utilizando esse mês, responda à seguinte consulta analítica.\n","\n","Qual equipe gerou mais receita para o mês de maior receita da consulta analítica anterior? Renomeie o total de receitas como \"Total de Receitas\"."]},{"cell_type":"code","metadata":{"id":"vXl88MHDy5vE"},"source":["# resposta do exercício 7\n","\n","query = \"\"\"\n","SELECT equipeNome, ROUND(SUM(receita), 2) AS `Total de Receitas`\n","FROM negociacao JOIN data ON data.dataPK = negociacao.dataPK \n","                JOIN cliente ON cliente.clientePK = negociacao.clientePK\n","                JOIN equipe ON equipe.equipePK = negociacao.equipePK \n","WHERE dataAno = 2020 \n","      AND dataMes = 11 \n","      AND clienteSetor = 'BEBIDAS E ALIMENTOS' \n","GROUP BY equipenome \n","ORDER BY SUM(receita)\n","\"\"\"\n","\n","spark.sql(query).show()"],"execution_count":null,"outputs":[]}]}