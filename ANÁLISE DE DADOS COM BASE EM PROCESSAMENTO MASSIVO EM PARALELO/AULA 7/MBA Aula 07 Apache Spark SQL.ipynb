{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MBA Aula 07 Apache Spark SQL.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"flekT6GFDN6m"},"source":["# <span style=\"color:blue\">MBA em Ciência de Dados</span>\n","# <span style=\"color:blue\">Análise de Dados com Base em Processamento Massivo em Paralelo</span>\n","\n","## <span style=\"color:blue\">Aula 07: Consultas OLAP usando Spark SQL</span>\n","## <span style=\"color:blue\">Apache Spark SQL</span>\n","\n","**Material Produzido por:**<br>\n",">**Profa. Dra. Cristina Dutra de Aguiar Ciferri**<br>\n",">**André Marcos Perez**<br> \n",">**Guilherme Muzzi da Rocha**<br>\n",">**Piero Lima Capelo**<br>\n","\n","**CEMEAI - ICMC/USP São Carlos**"]},{"cell_type":"markdown","metadata":{"id":"3o3dN_WLQcyD"},"source":["#1 Otimizador de Consultas Catalyst\n","\n","O componente mais importante do Spark SQL é o seu otimizador de consultas, chamado Catalyst. Catalyst é baseado em construtores de programação funcional e é implementado na linguagem de programação Scala. Sua implementação tem dois propósitos principais. O primeiro é permitir que novas técnicas de otimização e novas características possam ser facilmente adicionadas ao Spark SQL. O segundo propósito consiste em possibilitar que desenvolvedores externos estendam o otimizador de consultas, por exemplo, adicionando novas regras de otimização e provendo suporte para novos tipos de dados, dentre outros."]},{"cell_type":"markdown","metadata":{"id":"i7rEE-45DhOW"},"source":["## 1.1 Plano de Consulta\n","\n","Dada uma consulta em alto nível, existem diferentes estratégias de execução (ou seja, planos de consulta) alternativas para se processar essa consulta, principalmente se ela for complexa. A otimização de consultas consiste no processo de gerar e selecionar o plano de consulta mais eficiente dentre as diversas possibilidades disponíveis, ou seja, consiste no processo de selecionar o plano de consulta de menor custo. \n","\n","De fato, a quantidade de possíveis planos de consulta que podem ser gerados para processar uma consulta pode ser muito grande. Assim, no processamento de uma consulta, nem todos os planos possíveis são gerados e analisados, uma vez que o tempo gasto nesta atividade seria provavelmente excessivo, talvez superando o tempo de responder à consulta por meio de uma busca sequencial. Heurísticas são usualmente empregadas para diminuir o espaço de busca. \n","\n","Portanto, o otimizador de consultas em geral não produz uma solução que é a ótima ou de menor custo frente a todas as possibilidades existentes, mas produz uma solução que é a melhor frente a algumas dessas possibilidades. \n","\n"]},{"cell_type":"markdown","metadata":{"id":"KT2g66RkPd6d"},"source":["\n","##1.2 Técnicas de Otimização\n","\n","Duas técnicas amplamente utilizadas pelo otimizador de consultas são descritas a seguir.\n","\n","- Otimização baseada em regras. O otimizador de consultas baseado em regras tem por objetivo gerar apenas um subconjunto de planos a serem analisados, usado como base heurísticas. Cada plano gerado pelo otimizador de consultas baseado em regras consiste de uma expressão algébrica que determina a ordem na qual as operações deve ser executadas, de forma que todos os planos para uma determinada consulta sejam equivalentes.\n","\n","- Otimização baseada em custo. O otimizador de consultas baseado em custo identifica, para cada plano gerado, o custo para processar a consulta, e seleciona o plano de menor custo. Isso depende de diversos fatores, tais como quais as operações algébricas que encontram-se efetivamente implementadas por meio de algoritmos disponilizados e os índices disponíveis para processar a consulta. \n","\n","Catalyst realiza otimização de consultas baseada em regras e otimização de consultas baseada em custo para transformar uma consulta escrita em SQL em códigos que executam sobre RDDs e utilizam os princípios do modelo MapReduce e do sistema de arquivos distribuídos HDFS.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"BGeh8KdXwVCQ"},"source":["# 2 Constelação de Fatos da BI Solutions\n","\n","A aplicação de *data warehousing* da BI Solutions utiliza como base uma contelação de fatos, conforme descrita a seguir.\n","\n","**Tabelas de dimensão**\n","\n","- data (dataPK, dataCompleta, dataDia, dataMes, dataBimestre, dataTrimestre, dataSemestre, dataAno)\n","- funcionario (funcPK, funcMatricula, funcNome, funcSexo, funcDataNascimento, funcDiaNascimento, funcMesNascimento, funcAnoNascimento, funcCidade, funcEstadoNome, funcEstadoSigla, funcRegiaoNome, funcRegiaoSigla, funcPaisNome, funcPaisSigla)\n","- equipe (equipePK, equipeNome, filialNome, filialCidade, filialEstadoNome, filialEstadoSigla, filialRegiaoNome, filialRegiaoSigla, filialPaisNome, filialPaisSigla)\n","- cargo (cargoPK, cargoNome, cargoRegimeTrabalho, cargoEscolaridadeMinima, cargoNivel)\n","- cliente (clientePK, clienteNomeFantasia, clienteSetor, clienteCidade, clienteEstadoNome, clienteEstadoSigla, clienteRegiaoNome, clienteRegiaoSigla, clientePaisNome, clientePaisSigla)\n","\n","**Tabelas de fatos**\n","- pagamento (dataPK, funcPK, equipePK, cargoPK, salario, quantidadeLancamentos)\n","- negociacao (dataPK, equipePK, clientePK, receita, quantidadeNegociacoes)\n"]},{"cell_type":"markdown","metadata":{"id":"CCCNC64AzBG0"},"source":["## 2.1 Baixando o Módulo wget\n","\n","Para baixar os dados referentes ao esquema relacional da constelação de fatos da BI Solutions, é utilizado o módulo  **wget**. O comando a seguir realiza a instalação desse módulo. <br>"]},{"cell_type":"code","metadata":{"id":"3e0Eao1K0EYG"},"source":["#instalando o módulo wget\n","%%capture\n","!pip install -q wget\n","!mkdir data"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j56pVJ2hZ2i5"},"source":["## 2.2 Obtenção dos Dados das Tabelas de Dimensão\n","\n","Os comandos a seguir baixam os dados que povoam as tabelas de dimensão. "]},{"cell_type":"code","metadata":{"id":"46QzTpLJwfkW","cellView":"both","outputId":"a2be9fe9-bf55-4180-bd95-97b92775d4bf","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["#baixando os dados das tabelas de dimensão\n","import wget\n","\n","url = \"https://raw.githubusercontent.com/GuiMuzziUSP/Data_Mart_BI_Solutions/main/data.csv\"\n","wget.download(url, \"data/data.csv\")\n","\n","url = \"https://raw.githubusercontent.com/GuiMuzziUSP/Data_Mart_BI_Solutions/main/funcionario.csv\"\n","wget.download(url, \"data/funcionario.csv\")\n","\n","url = \"https://raw.githubusercontent.com/GuiMuzziUSP/Data_Mart_BI_Solutions/main/equipe.csv\"\n","wget.download(url, \"data/equipe.csv\")\n","\n","url = \"https://raw.githubusercontent.com/GuiMuzziUSP/Data_Mart_BI_Solutions/main/cargo.csv\"\n","wget.download(url, \"data/cargo.csv\")\n","\n","url = \"https://raw.githubusercontent.com/GuiMuzziUSP/Data_Mart_BI_Solutions/main/cliente.csv\"\n","wget.download(url, \"data/cliente.csv\")"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'data/cliente.csv'"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"markdown","metadata":{"id":"0o-dC7feszRc"},"source":["## 2.3 Obtenção dos Dados Tabelas de Fatos\n","\n","Os comandos a seguir baixam os dados que povoam as tabelas de fatos. "]},{"cell_type":"code","metadata":{"id":"XWM-CUFgBl_8","outputId":"d78e5b9c-304d-41e2-e52f-2afb6cb60e0b","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["#baixando os dados das tabelas de fatos\n","url = \"https://raw.githubusercontent.com/GuiMuzziUSP/Data_Mart_BI_Solutions/main/pagamento.csv\"\n","wget.download(url, \"data/pagamento.csv\")\n","\n","url = \"https://raw.githubusercontent.com/GuiMuzziUSP/Data_Mart_BI_Solutions/main/negociacao.csv\"\n","wget.download(url, \"data/negociacao.csv\")"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'data/negociacao.csv'"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"sO16-7-jOioq"},"source":["# 3 Apache Spark Cluster"]},{"cell_type":"markdown","metadata":{"id":"YVEgY9qKflBV"},"source":["## 3.1 Instalação\n","\n","Neste *notebook* é criado um *cluster* Spark composto apenas por um **nó mestre**. Ou seja, o *cluster* não possui um ou mais **nós de trabalho** e o **gerenciador de cluster**. Nessa configuração, as tarefas (*tasks*) são realizadas no próprio *driver* localizado no **nó mestre**."]},{"cell_type":"markdown","metadata":{"id":"KaM-OnIjgLS2"},"source":["Para que o cluster possa ser criado, primeiramente é instalado o Java Runtime Environment (JRE) versão 8. "]},{"cell_type":"code","metadata":{"id":"NXls3bfoglKW"},"source":["#instalando Java Runtime Environment (JRE) versão 8\n","%%capture\n","!apt-get remove openjdk*\n","!apt-get update --fix-missing\n","!apt-get install openjdk-8-jdk-headless -qq > /dev/null"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7BQzZfDYhb4j"},"source":["Na sequência, é feito o *download* do Apache Spark versão 3.0.0."]},{"cell_type":"code","metadata":{"id":"8a_Yv59zg3gm"},"source":["#baixando Apache Spark versão 3.0.0\n","%%capture\n","!wget -q https://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop2.7.tgz\n","!tar xf spark-3.0.0-bin-hadoop2.7.tgz && rm spark-3.0.0-bin-hadoop2.7.tgz"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RETWX6wqhkLf"},"source":["Na sequência, são configuradas as variáveis de ambiente JAVA_HOME e SPARK_HOME. Isto permite que tanto o Java quanto o Spark possam ser encontrados."]},{"cell_type":"code","metadata":{"id":"iZpR7NwOh2EB"},"source":["import os\n","#configurando a variável de ambiente JAVA_HOME\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","#configurando a variável de ambiente SPARK_HOME\n","os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-bin-hadoop2.7\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ql0z7Ro1iHQb"},"source":["Por fim, são instalados dois pacotes da linguagem de programação Python, cujas funcionalidades são descritas a seguir.\n","\n","> **Pacote findspark:** Usado para ler a variável de ambiente SPARK_HOME e armazenar seu valor na variável dinâmica de ambiente PYTHONPATH. Como resultado, Python pode encontrar a instalação do Spark. \n","\n","> **Pacote pyspark:** PySpark é a API do Python para Spark. Ela possibilita o uso de Python, considerando que o *framework* Apache Spark encontra-se desenvolvido na linguagem de programação Scala. "]},{"cell_type":"code","metadata":{"id":"5oSYOwKljPf5"},"source":["%%capture\n","#instalando o pacote findspark\n","!pip install -q findspark==1.4.2\n","#instalando o pacote pyspark\n","!pip install -q pyspark==3.0.0"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eAaLyjPzmIwZ"},"source":["## 3.2 Conexão\n","\n","PySpark não é adicionado ao *sys.path* por padrão. Isso significa que não é possível importá-lo, pois o interpretador da linguagem Python não sabe onde encontrá-lo. \n","\n","Para resolver esse aspecto, é necessário instalar o módulo `findspark`. Esse módulo mostra onde PySpark está localizado. Os comandos a seguir têm essa finalidade.\n"]},{"cell_type":"code","metadata":{"id":"-zm1pBTEmjp4"},"source":["#importando o módulo findspark\n","import findspark\n","#carregando a variávels SPARK_HOME na variável dinâmica PYTHONPATH\n","findspark.init()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZDqfefF7YUab"},"source":["Depois de configurados os pacotes e módulos e inicializadas as variáveis de ambiente, é possível iniciar o uso do Spark na aplicação de `data warehousing`. Para tanto, é necessário importar o comando `SparkSession` do módulo `pyspark.sql`. São utilizados os seguintes conceitos: <br>\n","\n","- `SparkSession`: permite a criação de `DataFrames`. Como resultado, as tabelas relacionais podem ser manipuladas por meio de `DataFrames` e é possível realizar consultas OLAP por meio de comandos SQL. <br>\n","- `builder`: cria uma instância de SparkSession. <br>\n","- `appName`: define um nome para a aplicação, o qual pode ser visto na interface de usuário web do Spark. <br> \n","- `master`: define onde está o nó mestre do *cluster*. Como a aplicação é executada localmente e não em um *cluster*, indica-se isso pela *string* `local` seguida do parâmetro `[*]`. Ou seja, define-se que apenas núcleos locais são utilizados. \n","- `getOrCreate`: cria uma SparkSession. Caso ela já exista, retorna a instância existente. \n","\n","\n","**Observação**: A lista completa de todos os parâmetros que podem ser utilizados na inicialização do *cluster* pode ser encontrada neste [link](https://spark.apache.org/docs/latest/spark-standalone.html#cluster-launch-scripts)."]},{"cell_type":"code","metadata":{"id":"9TxljJ_cwBCy"},"source":["from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.appName(\"pyspark-notebook\").master(\"local[*]\").getOrCreate()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5qL9SiR_pQE2"},"source":["# 4 Preparação dos Dados\n"]},{"cell_type":"markdown","metadata":{"id":"WtUGn-uyBJWY"},"source":["## 4.1 Geração dos DataFrames\n","\n","Para a leitura dos dados dos arquivos .csv, é utilizado o método `spark.read.csv`. Seus parâmetros são:\n","\n","\n","- `path`: endereço do arquivo que é lido.\n","- `header`: indica se o arquivo possui um cabeçalho.\n","- `sep`: especifica o caractere que separa os campos do arquivo.\n","\n"]},{"cell_type":"code","metadata":{"cellView":"both","id":"FNR-3dV6oYk4","outputId":"5700105c-f90d-4764-933e-cfe74acb591b","colab":{"base_uri":"https://localhost:8080/"}},"source":["#criando e exibindo o DataFrame para a tabela de dimensão cargo\n","cargo = spark.read.csv(path=\"data/cargo.csv\", header=True, sep=\",\")\n","cargo.show(5)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["+-------+--------------------+-------------------+--------------------+-----------------------+----------+\n","|cargoPK|           cargoNome|cargoRegimeTrabalho|cargoJornadaTrabalho|cargoEscolaridadeMinima|cargoNivel|\n","+-------+--------------------+-------------------+--------------------+-----------------------+----------+\n","|      1|PROGRAMADOR DE SI...|         TEMPORARIO|                 20H|                  MEDIO|    JUNIOR|\n","|      2|PROGRAMADOR DE SI...|         TEMPORARIO|                 20H|               SUPERIOR|     PLENO|\n","|      3|PROGRAMADOR DE SI...|         TEMPORARIO|                 20H|                    POS|    SENIOR|\n","|      4|PROGRAMADOR DE SI...|         TEMPORARIO|                 40H|                  MEDIO|    JUNIOR|\n","|      5|PROGRAMADOR DE SI...|         TEMPORARIO|                 40H|               SUPERIOR|     PLENO|\n","+-------+--------------------+-------------------+--------------------+-----------------------+----------+\n","only showing top 5 rows\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LPCF-SyBtuPW","outputId":"0d778dda-d6bc-47dd-82e0-88d92c09bd60","colab":{"base_uri":"https://localhost:8080/"}},"source":["#criando e exibindo o DataFrame para a tabela de dimensão cliente\n","cliente = spark.read.csv(path=\"data/cliente.csv\", header=True, sep=\",\")\n","cliente.show(5)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["+---------+-------------------+-------------------+-------------+-----------------+------------------+-----------------+------------------+---------------+----------------+\n","|clientePK|clienteNomeFantasia|       clienteSetor|clienteCidade|clienteEstadoNome|clienteEstadoSigla|clienteRegiaoNome|clienteRegiaoSigla|clientePaisNome|clientePaisSigla|\n","+---------+-------------------+-------------------+-------------+-----------------+------------------+-----------------+------------------+---------------+----------------+\n","|        1|           VIA FOOD|BEBIDAS E ALIMENTOS|    SAO PAULO|        SAO PAULO|                SP|          SUDESTE|                SE|         BRASIL|              BR|\n","|        2|          VIA PIZZA|BEBIDAS E ALIMENTOS|    SAO PAULO|        SAO PAULO|                SP|          SUDESTE|                SE|         BRASIL|              BR|\n","|        3|           VIA JAPA|BEBIDAS E ALIMENTOS|    SAO PAULO|        SAO PAULO|                SP|          SUDESTE|                SE|         BRASIL|              BR|\n","|        4|            VIA VEG|BEBIDAS E ALIMENTOS|    SAO PAULO|        SAO PAULO|                SP|          SUDESTE|                SE|         BRASIL|              BR|\n","|        5|          VIA DRINK|BEBIDAS E ALIMENTOS|   SAO CARLOS|        SAO PAULO|                SP|          SUDESTE|                SE|         BRASIL|              BR|\n","+---------+-------------------+-------------------+-------------+-----------------+------------------+-----------------+------------------+---------------+----------------+\n","only showing top 5 rows\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"w3p9dLUKts73","outputId":"af1a5061-e128-460f-980e-eff54816ba8a","colab":{"base_uri":"https://localhost:8080/"}},"source":["#criando e exibindo o DataFrame para a tabela de dimensão data\n","data = spark.read.csv(path=\"data/data.csv\", header=True, sep=\",\") \n","data.show(5)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["+------+------------+-------+-------+------------+-------------+------------+-------+\n","|dataPK|dataCompleta|dataDia|dataMes|dataBimestre|dataTrimestre|dataSemestre|dataAno|\n","+------+------------+-------+-------+------------+-------------+------------+-------+\n","|     1|    1/1/2016|      1|      1|           1|            1|           1|   2016|\n","|     2|    2/1/2016|      2|      1|           1|            1|           1|   2016|\n","|     3|    3/1/2016|      3|      1|           1|            1|           1|   2016|\n","|     4|    4/1/2016|      4|      1|           1|            1|           1|   2016|\n","|     5|    5/1/2016|      5|      1|           1|            1|           1|   2016|\n","+------+------------+-------+-------+------------+-------------+------------+-------+\n","only showing top 5 rows\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Obef8NfyttuJ","outputId":"3e906278-787d-4481-80f4-acba4258ed7e","colab":{"base_uri":"https://localhost:8080/"}},"source":["#criando e exibindo o DataFrame para a tabela de dimensão equipe\n","equipe = spark.read.csv(path=\"data/equipe.csv\", header=True, sep=\",\")\n","equipe.show(5)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["+--------+-------------+--------------------+--------------+------------------+-----------------+----------------+-----------------+--------------+---------------+\n","|equipePK|   equipeNome|          filialNome|  filialCidade|  filialEstadoNome|filialEstadoSigla|filialRegiaoNome|filialRegiaoSigla|filialPaisNome|filialPaisSigla|\n","+--------+-------------+--------------------+--------------+------------------+-----------------+----------------+-----------------+--------------+---------------+\n","|       1|APP - DESKTOP|SAO PAULO - AV. P...|     SAO PAULO|         SAO PAULO|               SP|         SUDESTE|               SE|        BRASIL|             BR|\n","|       2|APP - DESKTOP|RIO DE JANEIRO - ...|RIO DE JANEIRO|    RIO DE JANEIRO|               RJ|         SUDESTE|               SE|        BRASIL|             BR|\n","|       3|          WEB|SAO PAULO - AV. P...|     SAO PAULO|         SAO PAULO|               SP|         SUDESTE|               SE|        BRASIL|             BR|\n","|       4|          WEB|RIO DE JANEIRO - ...|RIO DE JANEIRO|    RIO DE JANEIRO|               RJ|         SUDESTE|               SE|        BRASIL|             BR|\n","|       5|          WEB|CAMPO GRANDE - CE...|  CAMPO GRANDE|MATO GROSSO DO SUL|               MS|    CENTRO-OESTE|               CO|        BRASIL|             BR|\n","+--------+-------------+--------------------+--------------+------------------+-----------------+----------------+-----------------+--------------+---------------+\n","only showing top 5 rows\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RsF4rcS7Zp4O","outputId":"83699c1b-6261-41ae-a76f-d20d46e667e2","colab":{"base_uri":"https://localhost:8080/"}},"source":["#criando e exibindo o DataFrame para a tabela de fatos funcionario\n","funcionario = spark.read.csv(path=\"data/funcionario.csv\", header=True, sep=\",\")\n","funcionario.show(5)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["+------+-------------+-------------+--------+------------------+-----------------+-----------------+-----------------+-----------+--------------+---------------+--------------+---------------+------------+-------------+\n","|funcPK|funcMatricula|     funcNome|funcSexo|funcDataNascimento|funcDiaNascimento|funcMesNascimento|funcAnoNascimento| funcCidade|funcEstadoNome|funcEstadoSigla|funcRegiaoNome|funcRegiaoSigla|funcPaisNome|funcPaisSigla|\n","+------+-------------+-------------+--------+------------------+-----------------+-----------------+-----------------+-----------+--------------+---------------+--------------+---------------+------------+-------------+\n","|     1|          M-1|ALINE ALMEIDA|       F|          1/1/1990|                1|                1|             1990|  SAO PAULO|     SAO PAULO|             SP|       SUDESTE|             SE|      BRASIL|           BR|\n","|     2|          M-2|   ARAO ALVES|       M|          2/2/1990|                2|                2|             1990|   CAMPINAS|     SAO PAULO|             SP|       SUDESTE|             SE|      BRASIL|           BR|\n","|     3|          M-3| ARON ANDRADE|       M|          3/3/1990|                3|                3|             1990|     SANTOS|     SAO PAULO|             SP|       SUDESTE|             SE|      BRASIL|           BR|\n","|     4|          M-4|  ADA BARBOSA|       F|          4/4/1990|                4|                4|             1990|SANTO ANDRE|     SAO PAULO|             SP|       SUDESTE|             SE|      BRASIL|           BR|\n","|     5|          M-5|ABADE BATISTA|       M|          5/5/1990|                5|                5|             1990| PIRACICABA|     SAO PAULO|             SP|       SUDESTE|             SE|      BRASIL|           BR|\n","+------+-------------+-------------+--------+------------------+-----------------+-----------------+-----------------+-----------+--------------+---------------+--------------+---------------+------------+-------------+\n","only showing top 5 rows\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"s4aZ0M6OZvg2","outputId":"11465955-b434-4694-b967-1375883270ee","colab":{"base_uri":"https://localhost:8080/"}},"source":["#criando e exibindo o DataFrame para a tabela de fatos negociacao\n","negociacao = spark.read.csv(path=\"data/negociacao.csv\", header=True, sep=\",\")\n","negociacao.show(5)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["+--------+---------+------+--------+---------------------+\n","|equipePK|clientePK|dataPK| receita|quantidadeNegociacoes|\n","+--------+---------+------+--------+---------------------+\n","|       2|        9|    22|11564.75|                    1|\n","|       2|       24|    11| 17990.5|                    1|\n","|       2|       28|    21| 16335.9|                    1|\n","|       1|       30|    23| 8495.55|                    1|\n","|       2|       43|    30|24748.75|                    1|\n","+--------+---------+------+--------+---------------------+\n","only showing top 5 rows\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DzVUaFptodHJ","outputId":"6b74e010-bb21-47f9-e111-b23686108f40","colab":{"base_uri":"https://localhost:8080/"}},"source":["#criando e exibindo o DataFrame para a tabela de fatos pagamento\n","pagamento = spark.read.csv(path=\"data/pagamento.csv\", header=True, sep=\",\")\n","pagamento.show()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["+------+--------+------+-------+--------+---------------------+\n","|funcPK|equipePK|dataPK|cargoPK| salario|quantidadeLancamentos|\n","+------+--------+------+-------+--------+---------------------+\n","|   147|       2|     5|     64| 1559.94|                    1|\n","|   124|       2|     5|    329| 8102.77|                    1|\n","|   175|       1|     5|    328| 2532.51|                    1|\n","|   171|       1|     5|    245|  7882.7|                    1|\n","|   148|       2|     5|     65| 4404.59|                    1|\n","|     5|       2|     5|    112| 2226.66|                    1|\n","|   128|       1|     5|    341| 6157.04|                    1|\n","|    82|       2|     5|     43| 1585.51|                    1|\n","|    28|       1|     5|    253| 1594.02|                    1|\n","|    46|       1|     5|    390| 9880.16|                    1|\n","|    91|       2|     5|    233|10931.47|                    1|\n","|   176|       2|     5|    241| 2005.49|                    1|\n","|   172|       1|     5|    351|14218.28|                    1|\n","|   155|       1|     5|    121| 2002.57|                    1|\n","|    19|       2|     5|    223| 1778.26|                    1|\n","|    94|       1|     5|    394|  2505.1|                    1|\n","|    26|       2|     5|    350| 9162.46|                    1|\n","|    55|       2|     5|    313| 3215.53|                    1|\n","|    29|       1|     5|     62| 4128.44|                    1|\n","|   181|       1|     5|    249|11082.75|                    1|\n","+------+--------+------+-------+--------+---------------------+\n","only showing top 20 rows\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"09knZQIwO03y"},"source":["## 4.2 Criação de Visões Temporárias\n","\n","Para que seja possível executar consultas SQL usando Spark SQL, é necessário criar visões temporárias. Uma visão temporária é uma forma na qual um DataFrame pode ser consultado como se fosse uma tabela.\n","\n","Para tanto, deve ser utilizado o método  `createOrReplaceTempView` e deve ser passado como parâmetro uma *string* que é o nome da tabela que é criada a partir do DataFrame.  Os comandos a seguir criam uma visão temporária para cada DataFrame da aplicação de *data warehousing*. \n"," "]},{"cell_type":"code","metadata":{"id":"nB4dFUqHoiaW"},"source":["#criando as visões temporárias para as tabelas de dimensão\n","cargo.createOrReplaceTempView(\"cargo\")\n","cliente.createOrReplaceTempView(\"cliente\")\n","data.createOrReplaceTempView(\"data\")\n","equipe.createOrReplaceTempView(\"equipe\")\n","funcionario.createOrReplaceTempView(\"funcionario\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7PQgcY3honGm"},"source":["#criando a visão temporária para as tabelas de fatos\n","negociacao.createOrReplaceTempView(\"negociacao\")\n","pagamento.createOrReplaceTempView(\"pagamento\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ss0pmgplPAL3"},"source":["# 5 Execução de Consultas com Foco nas Operações OLAP"]},{"cell_type":"markdown","metadata":{"id":"MABYjSYf7L4g"},"source":["## 5.1 Operação Slice and Dice \n","\n","**Definição**: Restringe os dados sendo analisados a um subconjunto desses dados.\n","\n","- Slice: corte para um valor fixo, diminuindo a dimensionalidade do cubo.\n","- Dice: seleção de faixas de valores.\n","\n","**Exemplo de consulta**: Qual a quantidade de pagamentos realizados no mês de setembro de 2020?"]},{"cell_type":"code","metadata":{"id":"DGMSoyQRoqnz","outputId":"dab6b41c-20b6-4df5-e608-0337bc3680ce","colab":{"base_uri":"https://localhost:8080/"}},"source":["query = \"\"\"\n","SELECT CAST(SUM(quantidadeLancamentos) AS INTEGER) AS `Quantidade de Lançamentos`\n","FROM data JOIN pagamento ON (data.dataPK = pagamento.dataPK) \n","WHERE dataAno = 2020 \n","      AND dataMes = 9\n","\"\"\"\n","\n","spark.sql(query).show()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["+-------------------------+\n","|Quantidade de Lançamentos|\n","+-------------------------+\n","|                      200|\n","+-------------------------+\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"crCnMYwi7rIm"},"source":["## 5.2 Operações Drill-Down e Roll-Up\n","\n","**Definição**: Analisam os dados considerando níveis progressivos de agregação.\n","\n","- Drill-down: níveis de agregação progressivamente mais detalhados, ou de menor granularidade.\n","- Roll-up: níveis de agregação progressivamente menos detalhados, ou de maior granularidade."]},{"cell_type":"markdown","metadata":{"id":"YC-qVTVIpyX4"},"source":["Para ilustrar as operações de drill-down e roll-up, considere a consulta base definida a seguir.\n","\n","**Consulta base:** Qual o valor gasto em salários por ano, considerando cada **semestre**?"]},{"cell_type":"code","metadata":{"id":"YnaUOx3uow4Z","outputId":"e31901aa-6f52-4f18-8a72-2ec2d11b2f3e","colab":{"base_uri":"https://localhost:8080/"}},"source":["query = \"\"\"\n","SELECT dataAno, dataSemestre, ROUND(SUM(salario),2) AS `Valor gasto em salários por semestre`\n","FROM data JOIN pagamento ON data.dataPK = pagamento.dataPK \n","GROUP BY dataAno, dataSemestre\n","ORDER BY dataAno, dataSemestre\n","\"\"\"\n","\n","spark.sql(query).show()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["+-------+------------+------------------------------------+\n","|dataAno|dataSemestre|Valor gasto em salários por semestre|\n","+-------+------------+------------------------------------+\n","|   2016|           1|                          2221308.54|\n","|   2016|           2|                          2221308.54|\n","|   2017|           1|                           4887639.9|\n","|   2017|           2|                           4887639.9|\n","|   2018|           1|                           7467763.2|\n","|   2018|           2|                           7467763.2|\n","|   2019|           1|                          9283833.18|\n","|   2019|           2|                          9283833.18|\n","|   2020|           1|                          9283833.18|\n","|   2020|           2|                          9283833.18|\n","+-------+------------+------------------------------------+\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"1PzzQT1Xz-4-"},"source":["**Exemplo de consulta drill-down:** Qual o valor gasto em salários por ano, considerando cada trimestre?"]},{"cell_type":"code","metadata":{"id":"f8EZJPBSquj4","outputId":"4c2e63a5-5215-4655-fad9-6c5299c25c40","colab":{"base_uri":"https://localhost:8080/"}},"source":["query = \"\"\"\n","SELECT dataAno, dataTrimestre, ROUND(SUM(salario),2) AS `Valor gasto em salários por trimestre`\n","FROM data JOIN pagamento ON (data.dataPK = pagamento.dataPK) \n","GROUP BY dataAno, dataTrimestre\n","ORDER BY dataAno, dataTrimestre\n","\"\"\"\n","\n","spark.sql(query).show()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["+-------+-------------+-------------------------------------+\n","|dataAno|dataTrimestre|Valor gasto em salários por trimestre|\n","+-------+-------------+-------------------------------------+\n","|   2016|            1|                           1110654.27|\n","|   2016|            2|                           1110654.27|\n","|   2016|            3|                           1110654.27|\n","|   2016|            4|                           1110654.27|\n","|   2017|            1|                           2443819.95|\n","|   2017|            2|                           2443819.95|\n","|   2017|            3|                           2443819.95|\n","|   2017|            4|                           2443819.95|\n","|   2018|            1|                            3733881.6|\n","|   2018|            2|                            3733881.6|\n","|   2018|            3|                            3733881.6|\n","|   2018|            4|                            3733881.6|\n","|   2019|            1|                           4641916.59|\n","|   2019|            2|                           4641916.59|\n","|   2019|            3|                           4641916.59|\n","|   2019|            4|                           4641916.59|\n","|   2020|            1|                           4641916.59|\n","|   2020|            2|                           4641916.59|\n","|   2020|            3|                           4641916.59|\n","|   2020|            4|                           4641916.59|\n","+-------+-------------+-------------------------------------+\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"7bFWeBr87xpN"},"source":["**Exemplo de consulta roll-up:** Qual o valor gasto em salários por ano?"]},{"cell_type":"code","metadata":{"id":"dPKhALIJrllz","outputId":"481e1580-fec9-4565-c83c-b8dca8afea53","colab":{"base_uri":"https://localhost:8080/"}},"source":["query = \"\"\"\n","SELECT dataAno, ROUND(SUM(salario),2) AS `Valor gasto em salários por ano`\n","FROM data JOIN pagamento ON (data.dataPK = pagamento.dataPK) \n","GROUP BY dataAno\n","ORDER BY dataAno\n","\"\"\"\n","\n","spark.sql(query).show()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["+-------+-------------------------------+\n","|dataAno|Valor gasto em salários por ano|\n","+-------+-------------------------------+\n","|   2016|                     4442617.08|\n","|   2017|                      9775279.8|\n","|   2018|                   1.49355264E7|\n","|   2019|                  1.856766636E7|\n","|   2020|                  1.856766636E7|\n","+-------+-------------------------------+\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"h1kRDhciLQnj"},"source":["## 5.3 Operação Pivot\n","\n","**Definição:** Reorienta a visão multidimensional dos dados, oferecendo diferentes perspectivas dos mesmos dados."]},{"cell_type":"markdown","metadata":{"id":"eaXAXkeF1atH"},"source":["Para ilustrar a operação pivot, considere a consulta base definida a seguir. \n","\n","**Consulta base:**  Qual o valor gasto em salários por ano, considerando cada nível de cargo?"]},{"cell_type":"code","metadata":{"id":"NiA8yW23o9H5","outputId":"d9043dac-23fd-48a3-ce1f-329b4aae6a59","colab":{"base_uri":"https://localhost:8080/"}},"source":["query = \"\"\"\n","SELECT dataAno, cargoNivel, ROUND(SUM(salario),2) AS `Gastos em Salários`\n","FROM pagamento JOIN data ON pagamento.dataPK = data.dataPK \n","               JOIN cargo ON pagamento.cargoPK = cargo.cargoPK \n","GROUP BY dataAno, cargoNivel\n","ORDER BY dataAno, cargoNivel\n","\"\"\"\n","\n","spark.sql(query).show()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["+-------+----------+------------------+\n","|dataAno|cargoNivel|Gastos em Salários|\n","+-------+----------+------------------+\n","|   2016|    JUNIOR|         489456.84|\n","|   2016|     PLENO|        1454152.44|\n","|   2016|    SENIOR|         2499007.8|\n","|   2017|    JUNIOR|         1030642.8|\n","|   2017|     PLENO|        3791593.92|\n","|   2017|    SENIOR|        4953043.08|\n","|   2018|    JUNIOR|         1393282.2|\n","|   2018|     PLENO|        5357227.44|\n","|   2018|    SENIOR|        8185016.76|\n","|   2019|    JUNIOR|        1755714.36|\n","|   2019|     PLENO|        6132228.24|\n","|   2019|    SENIOR|     1.067972376E7|\n","|   2020|    JUNIOR|        1755714.36|\n","|   2020|     PLENO|        6132228.24|\n","|   2020|    SENIOR|     1.067972376E7|\n","+-------+----------+------------------+\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"DPBaswdrLWv6"},"source":["**Exemplo de consulta pivot:** Qual o valor gasto em salários por nível de cargo, considerando cada ano?"]},{"cell_type":"code","metadata":{"id":"v63e5Yps2CSD","outputId":"d21025d7-52c2-4de6-b048-5968a43d82db","colab":{"base_uri":"https://localhost:8080/"}},"source":["query = \"\"\"\n","SELECT cargoNivel, dataAno, ROUND(SUM(salario),2) AS `Gastos em Salários`\n","FROM pagamento JOIN data ON pagamento.dataPK = data.dataPK \n","               JOIN cargo ON pagamento.cargoPK = cargo.cargoPK \n","GROUP BY cargoNivel, dataAno\n","ORDER BY cargoNivel, dataAno\n","\"\"\"\n","\n","spark.sql(query).show()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["+----------+-------+------------------+\n","|cargoNivel|dataAno|Gastos em Salários|\n","+----------+-------+------------------+\n","|    JUNIOR|   2016|         489456.84|\n","|    JUNIOR|   2017|         1030642.8|\n","|    JUNIOR|   2018|         1393282.2|\n","|    JUNIOR|   2019|        1755714.36|\n","|    JUNIOR|   2020|        1755714.36|\n","|     PLENO|   2016|        1454152.44|\n","|     PLENO|   2017|        3791593.92|\n","|     PLENO|   2018|        5357227.44|\n","|     PLENO|   2019|        6132228.24|\n","|     PLENO|   2020|        6132228.24|\n","|    SENIOR|   2016|         2499007.8|\n","|    SENIOR|   2017|        4953043.08|\n","|    SENIOR|   2018|        8185016.76|\n","|    SENIOR|   2019|     1.067972376E7|\n","|    SENIOR|   2020|     1.067972376E7|\n","+----------+-------+------------------+\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"UtJILugUN2K_"},"source":["## 5.4 Operação Drill-Across\n","\n","**Definição:** Compara medidas numéricas de tabelas de fatos diferentes, utilizando pelo menos uma dimensão em comum. \n","\n","**Exemplo de consulta:** Qual o total gasto com salários e qual o total de receitas recebidas, considerando cada ano?"]},{"cell_type":"code","metadata":{"id":"oyoWQDJiIgQx","outputId":"349ec199-bd49-4146-f9eb-e984fb832f25","colab":{"base_uri":"https://localhost:8080/"}},"source":["# utilizando a cláusula JOIN ... ON ...\n","query = \"\"\"\n","SELECT anoPag AS `Ano`, ROUND(salario,2) AS `Total Gasto com Salários`, ROUND(receita,2) AS `Total de Receitas Recebidas`\n","FROM ( SELECT dataAno, SUM(salario)  \n","       FROM pagamento JOIN data on data.dataPK = pagamento.dataPK\n","       GROUP BY dataAno\n","      ) AS pag(anoPag, salario)\n","     JOIN \n","     ( SELECT dataAno, SUM(receita)\n","       FROM negociacao JOIN data ON data.dataPK = negociacao.dataPK\n","       GROUP BY dataAno\n","      ) AS neg(anoNeg, receita) \n","     ON anoPag = anoNeg \n","ORDER BY anoPag\n","\"\"\"\n","\n","spark.sql(query).show()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["+----+------------------------+---------------------------+\n","| Ano|Total Gasto com Salários|Total de Receitas Recebidas|\n","+----+------------------------+---------------------------+\n","|2016|              4442617.08|                 4614246.95|\n","|2017|               9775279.8|                 7200423.35|\n","|2018|            1.49355264E7|              1.159353965E7|\n","|2019|           1.856766636E7|               3.53533183E7|\n","|2020|           1.856766636E7|              3.022217595E7|\n","+----+------------------------+---------------------------+\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"CP9gstSQ9XUu","outputId":"988cce69-9d74-4ea6-ad8a-4fa9bb73c27f","colab":{"base_uri":"https://localhost:8080/"}},"source":["# utilizando a cláusula WHERE\n","query = \"\"\"\n","SELECT anoPag AS `Ano`, ROUND(salario,2) AS `Total Gasto com Salários`, ROUND(receita,2) AS `Total de Receitas Recebidas`\n","FROM ( SELECT dataAno, SUM(salario) \n","       FROM pagamento JOIN data on data.dataPK = pagamento.dataPK\n","       GROUP BY dataAno\n","      ) AS pag(anoPag, salario), \n","     ( SELECT dataAno, SUM(receita)\n","       FROM negociacao JOIN data ON data.dataPK = negociacao.dataPK\n","       GROUP BY dataAno\n","      ) AS neg(anoNeg, receita) \n","WHERE anoPag = anoNeg \n","ORDER BY anoPag\n","\"\"\"\n","\n","spark.sql(query).show()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["+----+------------------------+---------------------------+\n","| Ano|Total Gasto com Salários|Total de Receitas Recebidas|\n","+----+------------------------+---------------------------+\n","|2016|              4442617.08|                 4614246.95|\n","|2017|               9775279.8|                 7200423.35|\n","|2018|            1.49355264E7|              1.159353965E7|\n","|2019|           1.856766636E7|               3.53533183E7|\n","|2020|           1.856766636E7|              3.022217595E7|\n","+----+------------------------+---------------------------+\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"mTjo_CX1G5jy"},"source":["## 5.5 Extensões ROLLUP, CUBE e GROUPING SETS \n","\n","**Definição:** Constrém vários níveis de agregação.\n","\n","- ROLLUP: criação de subtotais para as combinações dos atributos da lista de agrupamento de acordo com a ordem desses atributos. São criados n+1 níveis de agregação, sendo n o número de atributos especificados na lista de agrupamento.\n","\n","- CUBE: criação de subtotais para todas as combinações dos atributos da lista de agrupamento. São criados 2ˆn (2 elevado a n) níveis, sendo n o número de atributos especificados na lista de agrupamento.\n","\n","- GROUPING SETS: criação de subtotais para quaisquer combinações de atributos de agrupamentos. É criada a quantidade de subtotais especificados na lista de níveis de agregação desejados. \n"]},{"cell_type":"markdown","metadata":{"id":"5RRlVccmYhV_"},"source":["\n","**Exemplo de consulta com ROLLUP:** Liste as agregações que podem ser geradas a partir da soma da receita por setor do cliente e por cidade do cliente, para totais de receita superiores a 3.000.000,00. Crie subtotais considerando a ordem dos atributos na lista de agrupamento."]},{"cell_type":"code","metadata":{"id":"BwZHLSsCW0zD","outputId":"9185de39-9bee-4fc1-a1bc-10074a387a8a","colab":{"base_uri":"https://localhost:8080/"}},"source":["query = \"\"\"\n","SELECT clienteSetor, clientecidade, ROUND(SUM(receita),2) AS `Total de Receitas`\n","FROM cliente JOIN negociacao ON cliente.clientePk = negociacao.clientePK\n","GROUP BY ROLLUP (clienteSetor, clienteCidade)\n","HAVING SUM(receita) > 3000000\n","ORDER BY clienteSetor, clienteCidade\n","\"\"\"\n","\n","spark.sql(query).show()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["+-------------------+--------------+-----------------+\n","|       clienteSetor| clienteCidade|Total de Receitas|\n","+-------------------+--------------+-----------------+\n","|               null|          null|     8.89837042E7|\n","|BEBIDAS E ALIMENTOS|          null|     3.54087859E7|\n","|BEBIDAS E ALIMENTOS|BELO HORIZONTE|        4206811.8|\n","|BEBIDAS E ALIMENTOS|       MARILIA|        4033680.5|\n","|BEBIDAS E ALIMENTOS|        RECIFE|        3899358.3|\n","|BEBIDAS E ALIMENTOS|RIO DE JANEIRO|       7351629.95|\n","|BEBIDAS E ALIMENTOS|     SAO PAULO|        6177945.5|\n","|BEBIDAS E ALIMENTOS|    UBERLANDIA|       3283351.55|\n","|            CREDITO|          null|        6621387.7|\n","|              SAUDE|          null|    1.831261245E7|\n","|              SAUDE|        MANAUS|       3329638.25|\n","|              SAUDE|     SAO PAULO|       3963055.35|\n","|         TECNOLOGIA|          null|      1.6568033E7|\n","|         TECNOLOGIA|     SAO PAULO|        9200352.8|\n","|          VESTUARIO|          null|    1.207288515E7|\n","|          VESTUARIO|RIO DE JANEIRO|        5948448.1|\n","|          VESTUARIO|     SAO PAULO|        4686200.3|\n","+-------------------+--------------+-----------------+\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ZUFL8-CcYojA"},"source":["**Exemplo de consulta com GROUPING SETS com semântica de ROLLUP:** Liste todas as agregações que podem ser geradas a partir da soma da receita por setor do cliente e por cidade do cliente, para totais de receita superiores a 3.000.000,00. Crie subtotais considerando a ordem dos atributos na lista de agrupamento."]},{"cell_type":"code","metadata":{"id":"Tq9x3HMyPwHe","outputId":"1a999eae-7eb6-4a7a-ae00-432a81988376","colab":{"base_uri":"https://localhost:8080/"}},"source":["query = \"\"\"\n","SELECT clienteSetor, clientecidade, ROUND(SUM(receita),2) AS `Total de Receitas`\n","FROM cliente JOIN negociacao ON cliente.clientePk = negociacao.clientePK\n","GROUP BY GROUPING SETS ((clienteSetor, clienteCidade), (clienteSetor), ())\n","HAVING SUM(receita) > 3000000\n","ORDER BY clienteSetor, clienteCidade\n","\"\"\"\n","\n","spark.sql(query).show()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["+-------------------+--------------+-----------------+\n","|       clienteSetor| clienteCidade|Total de Receitas|\n","+-------------------+--------------+-----------------+\n","|               null|          null|     8.89837042E7|\n","|BEBIDAS E ALIMENTOS|          null|     3.54087859E7|\n","|BEBIDAS E ALIMENTOS|BELO HORIZONTE|        4206811.8|\n","|BEBIDAS E ALIMENTOS|       MARILIA|        4033680.5|\n","|BEBIDAS E ALIMENTOS|        RECIFE|        3899358.3|\n","|BEBIDAS E ALIMENTOS|RIO DE JANEIRO|       7351629.95|\n","|BEBIDAS E ALIMENTOS|     SAO PAULO|        6177945.5|\n","|BEBIDAS E ALIMENTOS|    UBERLANDIA|       3283351.55|\n","|            CREDITO|          null|        6621387.7|\n","|              SAUDE|          null|    1.831261245E7|\n","|              SAUDE|        MANAUS|       3329638.25|\n","|              SAUDE|     SAO PAULO|       3963055.35|\n","|         TECNOLOGIA|          null|      1.6568033E7|\n","|         TECNOLOGIA|     SAO PAULO|        9200352.8|\n","|          VESTUARIO|          null|    1.207288515E7|\n","|          VESTUARIO|RIO DE JANEIRO|        5948448.1|\n","|          VESTUARIO|     SAO PAULO|        4686200.3|\n","+-------------------+--------------+-----------------+\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"IjjDB_mtPkzw"},"source":["**Exemplo de consulta com CUBE:** Liste todas as agregações que podem ser geradas a partir da soma da receita por setor do cliente e por cidade do cliente, para totais de receita superiores a 3.000.000,00."]},{"cell_type":"code","metadata":{"id":"v_5lF9EVFZr6","outputId":"59e68516-54a1-484d-9535-77787125f0a8","colab":{"base_uri":"https://localhost:8080/"}},"source":["query = \"\"\"\n","SELECT clienteSetor, clientecidade, ROUND(SUM(receita),2) AS `Total de Receitas`\n","FROM cliente JOIN negociacao ON cliente.clientePk = negociacao.clientePK\n","GROUP BY CUBE (clienteSetor, clienteCidade)\n","HAVING SUM(receita) > 3000000\n","ORDER BY clienteSetor, clienteCidade\n","\"\"\"\n","\n","spark.sql(query).show(40)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["+-------------------+--------------+-----------------+\n","|       clienteSetor| clienteCidade|Total de Receitas|\n","+-------------------+--------------+-----------------+\n","|               null|          null|     8.89837042E7|\n","|               null|BELO HORIZONTE|       4379523.85|\n","|               null|  CAMPO GRANDE|       3422117.05|\n","|               null|      CURITIBA|       3697625.35|\n","|               null|     FORTALEZA|       3391233.25|\n","|               null|        MANAUS|       5234539.25|\n","|               null|       MARILIA|        7289146.1|\n","|               null|  PORTO ALEGRE|        4319625.7|\n","|               null|        RECIFE|       4717719.45|\n","|               null|RIO DE JANEIRO|    1.525596265E7|\n","|               null|    SAO CARLOS|       4192741.95|\n","|               null|     SAO PAULO|    2.464599965E7|\n","|               null|    UBERLANDIA|        4357595.2|\n","|BEBIDAS E ALIMENTOS|          null|     3.54087859E7|\n","|BEBIDAS E ALIMENTOS|BELO HORIZONTE|        4206811.8|\n","|BEBIDAS E ALIMENTOS|       MARILIA|        4033680.5|\n","|BEBIDAS E ALIMENTOS|        RECIFE|        3899358.3|\n","|BEBIDAS E ALIMENTOS|RIO DE JANEIRO|       7351629.95|\n","|BEBIDAS E ALIMENTOS|     SAO PAULO|        6177945.5|\n","|BEBIDAS E ALIMENTOS|    UBERLANDIA|       3283351.55|\n","|            CREDITO|          null|        6621387.7|\n","|              SAUDE|          null|    1.831261245E7|\n","|              SAUDE|        MANAUS|       3329638.25|\n","|              SAUDE|     SAO PAULO|       3963055.35|\n","|         TECNOLOGIA|          null|      1.6568033E7|\n","|         TECNOLOGIA|     SAO PAULO|        9200352.8|\n","|          VESTUARIO|          null|    1.207288515E7|\n","|          VESTUARIO|RIO DE JANEIRO|        5948448.1|\n","|          VESTUARIO|     SAO PAULO|        4686200.3|\n","+-------------------+--------------+-----------------+\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"jZ0y48oPbwiO"},"source":["**Exemplo de consulta com GROUPING SETS com semântica de CUBE:** Liste todas as agregações que podem ser geradas a partir da soma da receita por setor do cliente e por cidade do cliente, para totais de receita superiores a 3.000.000,00. Crie subtotais considerando a ordem dos atributos na lista de agrupamento."]},{"cell_type":"code","metadata":{"id":"W8oaCRpEb29W","outputId":"1a581d6c-0f40-4586-c34f-21fdf314eb76","colab":{"base_uri":"https://localhost:8080/"}},"source":["query = \"\"\"\n","SELECT clienteSetor, clientecidade, ROUND(SUM(receita),2) AS `Total de Receitas`\n","FROM cliente JOIN negociacao ON cliente.clientePk = negociacao.clientePK\n","GROUP BY GROUPING SETS ((clienteSetor, clienteCidade), (clienteSetor), (clienteCidade), ())\n","HAVING SUM(receita) > 3000000\n","ORDER BY clienteSetor, clienteCidade\n","\"\"\"\n","\n","spark.sql(query).show(40)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["+-------------------+--------------+-----------------+\n","|       clienteSetor| clienteCidade|Total de Receitas|\n","+-------------------+--------------+-----------------+\n","|               null|          null|     8.89837042E7|\n","|               null|BELO HORIZONTE|       4379523.85|\n","|               null|  CAMPO GRANDE|       3422117.05|\n","|               null|      CURITIBA|       3697625.35|\n","|               null|     FORTALEZA|       3391233.25|\n","|               null|        MANAUS|       5234539.25|\n","|               null|       MARILIA|        7289146.1|\n","|               null|  PORTO ALEGRE|        4319625.7|\n","|               null|        RECIFE|       4717719.45|\n","|               null|RIO DE JANEIRO|    1.525596265E7|\n","|               null|    SAO CARLOS|       4192741.95|\n","|               null|     SAO PAULO|    2.464599965E7|\n","|               null|    UBERLANDIA|        4357595.2|\n","|BEBIDAS E ALIMENTOS|          null|     3.54087859E7|\n","|BEBIDAS E ALIMENTOS|BELO HORIZONTE|        4206811.8|\n","|BEBIDAS E ALIMENTOS|       MARILIA|        4033680.5|\n","|BEBIDAS E ALIMENTOS|        RECIFE|        3899358.3|\n","|BEBIDAS E ALIMENTOS|RIO DE JANEIRO|       7351629.95|\n","|BEBIDAS E ALIMENTOS|     SAO PAULO|        6177945.5|\n","|BEBIDAS E ALIMENTOS|    UBERLANDIA|       3283351.55|\n","|            CREDITO|          null|        6621387.7|\n","|              SAUDE|          null|    1.831261245E7|\n","|              SAUDE|        MANAUS|       3329638.25|\n","|              SAUDE|     SAO PAULO|       3963055.35|\n","|         TECNOLOGIA|          null|      1.6568033E7|\n","|         TECNOLOGIA|     SAO PAULO|        9200352.8|\n","|          VESTUARIO|          null|    1.207288515E7|\n","|          VESTUARIO|RIO DE JANEIRO|        5948448.1|\n","|          VESTUARIO|     SAO PAULO|        4686200.3|\n","+-------------------+--------------+-----------------+\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"tgV9cze8MBY3"},"source":["# 6 Execução de Consultas com Foco na Tomada de Decisão\n","As consultas OLAP requisitadas por usuários de sistemas de suporte à decisão usualmente requerem que várias operações OLAP sejam realizadas simultaneamente. A seguir são ilustrados exemplos de consultas OLAP que podem ser requisitadas para a tomada de decisão estratégica."]},{"cell_type":"markdown","metadata":{"id":"OcaRLxX_Jd1w"},"source":["## 6.1 Consulta 1\n","\n","Qual é a média dos salários recebidos por nível do cargo e por sexo no ano passado (ou seja, no ano de 2019)?\n","\n","Para se realizar esta consulta, é necessário obter dados das tabelas de dimensão `cargo`, `funcionario` e `data`, bem como da tabela de fatos `pagamento`. A junção estrela deve ocorrer considerando as seguintes integridades referenciais:\n","- `pagamento.cargoPK = cargo.cargoPK`\n","- `pagamento.funcPK = funcionario.funcPK`\n","- `pagamento.dataPK = data.dataPK` "]},{"cell_type":"code","metadata":{"id":"UeKlyc-mMt64","outputId":"c638f38e-73f2-4106-b195-62f0461e3e94","colab":{"base_uri":"https://localhost:8080/"}},"source":["# utilizando a cláusula JOIN ... ON ...\n","query = \"\"\"\n","SELECT cargoNivel, funcSexo, ROUND(AVG(salario),2) AS `Média dos Salários`\n","FROM pagamento JOIN data ON data.dataPK = pagamento.dataPK\n","               JOIN cargo ON cargo.cargoPK = pagamento.cargoPK\n","               JOIN funcionario ON funcionario.funcPK = pagamento.funcPK \n","WHERE dataAno = 2019\n","GROUP BY cargoNivel, funcSexo\n","ORDER BY cargoNivel, funcSexo\n","\"\"\"\n","\n","spark.sql(query).show()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["+----------+--------+------------------+\n","|cargoNivel|funcSexo|Média dos Salários|\n","+----------+--------+------------------+\n","|    JUNIOR|       F|           2440.23|\n","|    JUNIOR|       M|           2437.86|\n","|     PLENO|       F|           7641.94|\n","|     PLENO|       M|           6259.61|\n","|    SENIOR|       F|          12994.19|\n","|    SENIOR|       M|           14480.5|\n","+----------+--------+------------------+\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"so-Hec1xeZHL","outputId":"3bc09c76-f308-4867-ac68-486269c2b34c","colab":{"base_uri":"https://localhost:8080/"}},"source":["# utilizando a cláusula WHERE\n","query = \"\"\"\n","SELECT cargoNivel, funcSexo, ROUND(AVG(salario),2) AS `Média dos Salários`\n","FROM pagamento, cargo, funcionario, data\n","WHERE data.dataPK = pagamento.dataPK\n","      AND cargo.cargoPK = pagamento.cargoPK\n","      AND funcionario.funcPK = pagamento.funcPK\n","      AND dataAno = 2019\n","GROUP BY cargoNivel, funcSexo\n","ORDER BY cargoNivel, funcSexo\n","\"\"\"\n","\n","spark.sql(query).show()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["+----------+--------+------------------+\n","|cargoNivel|funcSexo|Média dos Salários|\n","+----------+--------+------------------+\n","|    JUNIOR|       F|           2440.23|\n","|    JUNIOR|       M|           2437.86|\n","|     PLENO|       F|           7641.94|\n","|     PLENO|       M|           6259.61|\n","|    SENIOR|       F|          12994.19|\n","|    SENIOR|       M|           14480.5|\n","+----------+--------+------------------+\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"varyr5djDP2s"},"source":["## 6.2 Consulta 2\n","\n","Qual o total de gastos em salários considerando os estados nos quais as equipes estão localizadas no último trimestre deste ano (ou seja, o trimestre 3 do ano de 2020)? \n","\n","Para se realizar esta consulta, é necessário obter dados das tabelas de dimensão `equipe` e `data`, bem como da tabela de fatos `pagamento`. A junção estrela deve ocorrer considerando as seguintes integridades referenciais:\n","- `pagamento.dataPK = data.dataPK`\n","- `pagamento.equipePK = equipe.equipePK`\n"]},{"cell_type":"code","metadata":{"id":"KSgE7xtPkGYf","outputId":"2fceaa70-fb2a-4776-a21b-6e2db23628a5","colab":{"base_uri":"https://localhost:8080/"}},"source":["query = \"\"\"\n","SELECT filialEstadoNome, ROUND(SUM(salario),2) AS Total\n","FROM pagamento JOIN data ON data.dataPK = pagamento.dataPK \n","               JOIN equipe ON equipe.equipePK = pagamento.equipePK\n","WHERE dataAno = 2019\n","      AND dataTrimestre = 3\n","GROUP BY filialEstadoNome\n","ORDER BY Total \n","\"\"\"\n","\n","spark.sql(query).show()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["+------------------+----------+\n","|  filialEstadoNome|     Total|\n","+------------------+----------+\n","|        PERNAMBUCO| 438121.26|\n","|MATO GROSSO DO SUL|1013857.74|\n","|    RIO DE JANEIRO|1258479.57|\n","|         SAO PAULO|1931458.02|\n","+------------------+----------+\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"RAqNkANBOZSI"},"source":["## 6.3 Consulta 3\n","\n","Qual o custo/benefício das equipes quando analisado o último semestre deste ano (ou seja, semestre 1 do ano de 2020)?\n","\n","A idea da consulta é relacionar os gastos em salários e os ganhos em receitas considerando cada equipe e o período especificado. Portanto, para se realizar essa consulta, é necessário obter dados das tabelas de dimensão `equipe` e `data`, bem como das tabelas de fatos `pagamento` e `negociacao`. \n","\n"," A junção estrela deve ocorrer considerando as seguintes integridades referenciais:\n","- `pagamento.dataPK = data.dataPK`\n","- `pagamento.equipePK = equipe.equipe.PK`\n","- `negociacao.dataPK = data.dataPK`\n","- `negociacao.equipePK = equipe.equipe.PK`\n"]},{"cell_type":"markdown","metadata":{"id":"HMyeJDwisUgu"},"source":["Uma observação muito importante refere-se ao fato que, para evitar dubiedade nas respostas, elas devem ser feitas sempre considerando a chave primária, desde que a chave primária identifica univocamente cada tupla. Depois de ser resolvida a consulta em termos da chave primária, então deve ser obtido os demais atributos a serem exibidos."]},{"cell_type":"code","metadata":{"id":"MybQ9xten5-x","outputId":"39cba2dd-16af-48f4-dede-ca0439b56ef5","colab":{"base_uri":"https://localhost:8080/"}},"source":["query = \"\"\"\n","-- obtendo os dados a serem exibidos na resposta\n","SELECT equipeNome, filialNome, ROUND(Lucro,2)\n","FROM equipe,\n","(\n","   SELECT pag.equipePK AS retornaEquipePK, (TotalReceita - TotalSalario) AS Lucro\n","   FROM ( \n","        -- investigando os gastos em salarios de cada equipe no último semestre deste ano \n","        SELECT equipePK, SUM(salario) AS TotalSalario\n","        FROM pagamento JOIN data ON data.dataPK = pagamento.dataPK\n","        WHERE dataSemestre = 1 \n","              AND dataAno = 2020\n","        GROUP BY equipePK \n","        ORDER BY equipePK \n","        ) AS pag,  \n","        (\n","        -- investigando os ganhos em receitas de cada equipe no último semestre deste ano   \n","        SELECT equipePK, SUM(receita) AS TotalReceita\n","        FROM negociacao JOIN data ON data.dataPK = negociacao.dataPK\n","        WHERE dataSemestre = 1 \n","              AND dataAno = 2020\n","        GROUP BY equipePK \n","        ORDER BY equipePK\n","        ) AS neg\n","   WHERE pag.equipePK = neg.equipePK\n","   ) AS parte\n","WHERE equipe.equipePK = parte.retornaEquipePK\n","ORDER BY Lucro DESC\n","\n","\"\"\"\n","\n","spark.sql(query).show(5)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["+--------------+--------------------+---------------+\n","|    equipeNome|          filialNome|round(Lucro, 2)|\n","+--------------+--------------------+---------------+\n","|BI & ANALYTICS|SAO PAULO - AV. P...|     4054890.24|\n","|BI & ANALYTICS|     RECIFE - CENTRO|     3788503.18|\n","| APP - DESKTOP|RIO DE JANEIRO - ...|      278013.26|\n","|  APP - MOBILE|SAO PAULO - AV. P...|      151160.49|\n","| APP - DESKTOP|SAO PAULO - AV. P...|       99579.85|\n","+--------------+--------------------+---------------+\n","only showing top 5 rows\n","\n"],"name":"stdout"}]}]}