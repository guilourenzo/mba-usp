{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MBA Aula 06 Apache Spark RDD.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flekT6GFDN6m"
      },
      "source": [
        "# <span style=\"color:blue\">MBA em Ciência de Dados</span>\n",
        "# <span style=\"color:blue\">Análise de Dados com Base em Processamento Massivo em Paralelo</span>\n",
        "\n",
        "## <span style=\"color:blue\">Aula 06: Processamento Paralelo e Distribuído</span>\n",
        "## <span style=\"color:blue\">Apache Spark RDD</span>\n",
        "\n",
        "**Material Produzido por:**<br>\n",
        ">**Profa. Dra. Cristina Dutra de Aguiar Ciferri**<br>\n",
        ">**André Marcos Perez**<br> \n",
        "\n",
        "**CEMEAI - ICMC/USP São Carlos**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_JqOLSZDggi"
      },
      "source": [
        "## **1 Introdução**\n",
        "\n",
        "O objetivo deste notebook é apresentar conceitos relacionados ao Apache Spark RDD. Antes de apresentar esses conceitos, a seguir são definidas as principais características do *framework* Apache Spark.\n",
        "\n",
        "O *framework* Apache Spark:\n",
        "\n",
        "- Executa sobre o sistema de arquivos distribuídos **HDFS** (*Hadoop Distributed File System*). \n",
        "\n",
        "- Incorpora e estende os conceitos relacionados ao modelo de programação funcional **MapReduce**. Em especial, o *framework* Apache Spark introduz diversas operações que podem ser executadas sobre os RDDs, além da possibilidade de criação de dois tipos de variáveis compartilhadas. \n",
        "\n",
        "- É baseado no uso de **conjuntos de dados distribuídos e resilientes, RDDs** (*Resilient and Distributed Datasets*). RDDs são abstrações que representam blocos de dados que podem ser reconstruídos em caso de falhas. Eles possibilitam o armazenamento dos resultados intermediários em memória primária sempre que possível. Como resultado, o uso de RDDs diminui o número de acessos a disco.\n",
        "\n",
        "- Possibilita o **agendamento de tarefas na forma de grafos acíclicos e direcionados, DAGs**. O processamento das tarefas consiste de vários estágios, os quais podem ser executados em paralelo caso não haja dependências entre os estágios. Como resultado, o uso de DAGs melhora o desempenho computacional das aplicações. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sO16-7-jOioq"
      },
      "source": [
        "## **2 Apache Spark Cluster**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHLoxgrqOm-M"
      },
      "source": [
        "### 2.1 Arquitetura\n",
        "\n",
        "Um *cluster* Spark é uma arquitetura de sistema distribuído para processamento paralelo composto por três elementos: \n",
        "\n",
        "> **Nó mestre**: O nó mestre (*master node*) é o componente responsável por coordenar a execução das tarefas. Um programa Spark possui apenas um nó mestre. Neste nó, roda-se o *driver*, um *software* que distribui as tarefas (*tasks*) entre os nós do *cluster* e obtém os resultados dessas tarefas. Para criar o *driver*, é necessário criar um objeto chamado *SparkContext*, no qual são definidos os recursos computacionais desejados.\n",
        "\n",
        "> **Nó de trabalho**: O nó de trabalho (*worker node*) é o componente responsável por realizar as tarefas. Um programa Spark possui um ou mais nós de trabalho. Nesses nós, roda-se um ou mais *executors*. Um *executor* é um *software* que recebe as tarefas (*tasks*) do *software* *driver*, armazena os resultados intermediários e retorna os dados gerados quando requisitado.\n",
        "\n",
        "> **Gerenciador do cluster**: O gerenciador do *cluster* (*cluster manager*) é o componente responsável por disponibilizar os recursos computacionais (por exemplo: memória e núcleos de processamento) por meio dos *executors* para o *driver*. O Spark é agnóstico quanto ao gerenciador e oferece suporte para os seguintes gerenciadores em sua versão atual (versão 3.0.1): Spark Standalone (gerenciador próprio do Spark), Apache YARN (mais utilizado), Apache Mesos e Kubernetes (muito utilizado em ambientes de computação em nuvem). \n",
        "\n",
        "Detalhes adicionais sobre o Apache Spark Cluster podem ser obtidos na documentação oficial do Spark neste [link](https://spark.apache.org/docs/latest/cluster-overview.html#cluster-manager-types).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HkKtJdkaQuMQ"
      },
      "source": [
        "### 2.2 Criação de uma Aplicação Spark\n",
        "\n",
        "A criação de uma aplicação Spark é feita da forma descrita a seguir, utilizando como base a Figura 1. \n",
        "\n",
        "1. Deve ser criado um objeto *SparkContext* definindo os recursos computacionais desejados. A criação desse objeto instancia o *driver* no **nó mestre**. \n",
        "\n",
        "2. O *driver* solicita ao **gerenciador do cluster** um conjunto de *executors* de acordo com os recursos computacionais selecionados.\n",
        "\n",
        "3.  O **gerenciador do cluster** cria os _executors_ nos **nós de trabalho** e retorna a informação relacionada a essa criação ao *driver*;\n",
        "\n",
        "4.  O *driver* submete as tarefas (*tasks*) para os *executors* e obtém os resultados retornados por esses *executors*.\n",
        "\n",
        "<br>\n",
        "\n",
        "<p align=\"center\"><img src=\"https://raw.githubusercontent.com/cdaciferri/Figures/main/CreateSparkApp.png\" width=\"600\" height=\"300\"></p>\n",
        "<p align=\"center\">Figura 1. Exemplo ilustrativo da criação de uma aplicação Spark.</p>\n",
        "\n",
        "<br>\n",
        "\n",
        "**Exemplo:** O exemplo a seguir cria uma aplicação Spark utilizando as seguintes configurações: (i) *cluster* gerenciado pelo Apache YARN; (ii) 1 *driver* de 1 núcleo de computação e 4 gigabytes de memória; e (iii) 4 *executors* de 2 núcleos de computação e 16 gigabytes de memória cada. O objeto criado é um objeto *SparkContext*. \n",
        "\n",
        "```\n",
        "from pyspark import SparkConf, SparkContext\n",
        "\n",
        "conf = SparkConf(). \\\n",
        "       setMaster(\"yarn\"). \\\n",
        "       set(\"spark.driver.cores\", \"1\"). \\\n",
        "       set(\"spark.driver.memory\", \"4g\"). \\\n",
        "       set(\"spark.executor.cores\", \"2\"). \\\n",
        "       set(\"spark.executor.memory\", \"16g\"). \\\n",
        "       set(\"spark.executor.instances\", \"4\"). \\\n",
        "\n",
        "spark = SparkContext(conf=conf)\n",
        "```\n",
        "Uma lista completa de configurações pode ser obtida na documentação oficial do Spark neste [link](https://spark.apache.org/docs/latest/configuration.html).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVEgY9qKflBV"
      },
      "source": [
        "### 2.3 Instalação\n",
        "\n",
        "Neste *notebook* é criado um *cluster* Spark composto apenas por um **nó mestre**. Ou seja, o *cluster* não possui um ou mais **nós de trabalho** e o **gerenciador de cluster**. Nessa configuração, as tarefas (*tasks*) são realizadas no próprio *driver* localizado no **nó mestre**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KaM-OnIjgLS2"
      },
      "source": [
        "Para que o cluster possa ser criado, primeiramente é instalado o Java Runtime Environment (JRE) versão 8. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXls3bfoglKW"
      },
      "source": [
        "#instalando Java Runtime Environment (JRE) versão 8\n",
        "%%capture\n",
        "!apt-get remove openjdk*\n",
        "!apt-get update --fix-missing\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BQzZfDYhb4j"
      },
      "source": [
        "Na sequência, é feito o *download* do Apache Spark versão 3.0.0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8a_Yv59zg3gm"
      },
      "source": [
        "#baixando Apache Spark versão 3.0.0\n",
        "%%capture\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop2.7.tgz\n",
        "!tar xf spark-3.0.0-bin-hadoop2.7.tgz && rm spark-3.0.0-bin-hadoop2.7.tgz"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RETWX6wqhkLf"
      },
      "source": [
        "Na sequência, são configuradas as variáveis de ambiente JAVA_HOME e SPARK_HOME. Isto permite que tanto o Java quanto o Spark possam ser encontrados."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZpR7NwOh2EB"
      },
      "source": [
        "import os\n",
        "#configurando a variável de ambiente JAVA_HOME\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "#configurando a variável de ambiente SPARK_HOME\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-bin-hadoop2.7\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ql0z7Ro1iHQb"
      },
      "source": [
        "Por fim, são instalados dois pacotes da linguagem de programação Python, cujas funcionalidades são descritas a seguir.\n",
        "\n",
        "> **Pacote findspark:** Usado para ler a variável de ambiente SPARK_HOME e armazenar seu valor na variável dinâmica de ambiente PYTHONPATH. Como resultado, Python pode encontrar a instalação do Spark. \n",
        "\n",
        "> **Pacote pyspark:** PySpark é a API do Python para Spark. Ela possibilita o uso de Python, considerando que o *framework* Apache Spark encontra-se desenvolvido na linguagem de programação Scala. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5oSYOwKljPf5"
      },
      "source": [
        "%%capture\n",
        "#instalando o pacote findspark\n",
        "!pip install -q findspark==1.4.2\n",
        "#instalando o pacote pyspark\n",
        "!pip install -q pyspark==3.0.0"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAaLyjPzmIwZ"
      },
      "source": [
        "### 2.4 Conexão\n",
        "\n",
        "PySpark não é adicionado ao *sys.path* por padrão. Isso significa que não é possível importá-lo, pois o interpretador da linguagem Python não sabe onde encontrá-lo. \n",
        "\n",
        "Para resolver esse aspecto, é necessário instalar o módulo `findspark`. Esse módulo mostra onde PySpark está localizado. Os comandos a seguir têm essa finalidade.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zm1pBTEmjp4"
      },
      "source": [
        "#importando o módulo findspark\n",
        "import findspark\n",
        "#carregando a variávels SPARK_HOME na variável dinâmica PYTHONPATH\n",
        "findspark.init()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmABUaOHv8XH"
      },
      "source": [
        "Depois de configurados os pacotes e módulos e inicializadas as variáveis de ambiente, é possível criar o objeto *SparkContext*. No comando de criação a seguir, é definido que é utilizado o próprio sistema operacional deste *notebook* como **nó mestre** por meio do parâmetro **local** do método **setMaster**. O complemento do parametro **[*]** indica que são alocados todos os núcleos de processamento disponíveis para o objeto *driver* criado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TxljJ_cwBCy"
      },
      "source": [
        "from pyspark import SparkConf, SparkContext\n",
        "\n",
        "conf = SparkConf().setMaster(\"local[*]\")\n",
        "spark = SparkContext(conf=conf)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PP2VzIYFKa3"
      },
      "source": [
        "## **3 API Apache Spark RDD**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hs_oPEQz_0vG"
      },
      "source": [
        "Nesta seção, é detalhada a classe `pyspark.RDD`, que está relacionada ao uso de RDDs. Primeiramente, são descritos dois conjuntos de dados usados como base para os exemplos. Depois, os principais conceitos envolvidos na classe pyspark.RDD são introduzidos. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85BaySlL7UvJ"
      },
      "source": [
        "O detalhamento da classe `pyspark.RDD` pode ser encontrada na documentação oficial do Spark neste [link](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD). Neste notebook são explicados os métodos mais utilizados e também os métodos que são necessários para o desenvolvimento da lista de exercícios."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTe2TOXE1WGH"
      },
      "source": [
        "### 3.1 Conjuntos de Dados\n",
        "\n",
        "Para descrever a API Apache Spark RDD, são utilizadas dois conjuntos de dados, conforme descrito a seguir:\n",
        "\n",
        "- `fib`: Uma lista Python contendo os 30 primeiros números da Sequência de Fibonacci. Na matemática, a Sucessão de Fibonacci (ou Sequência de Fibonacci), é uma sequência de números inteiros, começando normalmente por 0 e 1, na qual cada termo subsequente corresponde à soma dos dois anteriores.\n",
        "\n",
        "- `logs.txt`: Um arquivo texto com 56.481 linhas de *log* de um servidor web. O arquivo fonte pode ser obtido neste ([link](https://github.com/logpai/loghub/tree/master/Apache)).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7f-SxWOm5zNX"
      },
      "source": [
        "O comando a seguir cria a lista `fib`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3M_OnL8Z5yeS"
      },
      "source": [
        "#criando a lista fib com os 30 primeiros números da Sequência de Fibonacci\n",
        "def fibonacci(i: int) -> int:\n",
        "  if i <= 0: raise Exception()\n",
        "  elif i == 1: return 0\n",
        "  elif i == 2: return 1\n",
        "  else: return fibonacci(i-1) + fibonacci(i-2)\n",
        "\n",
        "fib = [fibonacci(i) for i in range(1, 31)]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHLioRoz6PcS",
        "outputId": "662e1eea-ad41-4dea-8d94-3e6d2715bef1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#exibindo a lista criada\n",
        "fib"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 3,\n",
              " 5,\n",
              " 8,\n",
              " 13,\n",
              " 21,\n",
              " 34,\n",
              " 55,\n",
              " 89,\n",
              " 144,\n",
              " 233,\n",
              " 377,\n",
              " 610,\n",
              " 987,\n",
              " 1597,\n",
              " 2584,\n",
              " 4181,\n",
              " 6765,\n",
              " 10946,\n",
              " 17711,\n",
              " 28657,\n",
              " 46368,\n",
              " 75025,\n",
              " 121393,\n",
              " 196418,\n",
              " 317811,\n",
              " 514229]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytrmdSBwA_fk"
      },
      "source": [
        "O comando a seguir obtém os dados do arquivo texto `logs.txt`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxVUqvO1A9cR"
      },
      "source": [
        "#obtendo os dados do arquivo texto logs.txt\n",
        "%%capture\n",
        "!wget -q https://zenodo.org/record/3227177/files/Apache.tar.gz?download=1 -O logs.tar.gz\n",
        "!tar xf logs.tar.gz && rm -rf logs.tar.gz\n",
        "!mv Apache.log logs.txt"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wr7aAeRCBH58",
        "outputId": "4ba4b85a-3186-460f-cfb6-072bb5d66722",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#abrindo o arquivo logs.txt no modo leitura (mode=\"r\") \n",
        "#exibindo os primeiros 10 registros do arquivo\n",
        "with open(file=\"logs.txt\", mode=\"r\") as fp: \n",
        "  for _ in range(0, 10): print(fp.readline())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Thu Jun 09 06:07:04 2005] [notice] LDAP: Built with OpenLDAP LDAP SDK\n",
            "\n",
            "[Thu Jun 09 06:07:04 2005] [notice] LDAP: SSL support unavailable\n",
            "\n",
            "[Thu Jun 09 06:07:04 2005] [notice] suEXEC mechanism enabled (wrapper: /usr/sbin/suexec)\n",
            "\n",
            "[Thu Jun 09 06:07:05 2005] [notice] Digest: generating secret for digest authentication ...\n",
            "\n",
            "[Thu Jun 09 06:07:05 2005] [notice] Digest: done\n",
            "\n",
            "[Thu Jun 09 06:07:05 2005] [notice] LDAP: Built with OpenLDAP LDAP SDK\n",
            "\n",
            "[Thu Jun 09 06:07:05 2005] [notice] LDAP: SSL support unavailable\n",
            "\n",
            "[Thu Jun 09 06:07:05 2005] [error] env.createBean2(): Factory error creating channel.jni:jni ( channel.jni, jni)\n",
            "\n",
            "[Thu Jun 09 06:07:05 2005] [error] config.update(): Can't create channel.jni:jni\n",
            "\n",
            "[Thu Jun 09 06:07:05 2005] [error] env.createBean2(): Factory error creating vm: ( vm, )\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7irXf7rie0S"
      },
      "source": [
        "### 3.2 Criação de RDDs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzZKld7pEJz5"
      },
      "source": [
        "Existem duas formas de se criar RDDs, conforme descrito a seguir:\n",
        "\n",
        "- Paralelizando uma coleção de dados já existente no *driver*.\n",
        "\n",
        "- Referenciando um conjunto de dados armazenado em um sistema de armazenamento externo, como um sistema de arquivo compartilhado, HDFS, HBase, Cassandra, ou qualquer outra fonte de dados que ofereça suporte para o formato de entrada do Hadoop. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTDZvGNTGsLp"
      },
      "source": [
        "#### Método parallelize()\n",
        "\n",
        "``parallelize(nameCollection, nPartitions=None)``\n",
        "\n",
        "Distribui uma coleção de dados em Python para formar um RDD. O parâmetro `nameCollection` indica o nome da coleção, enquanto que o parâmetro `nPartitions` indica o número de partições nas quais os dados da coleção são particionados. Usualmente, Spark seta o número de partições automaticamente, com base no *cluster* sendo utilizado. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grD5fAO8E2kH"
      },
      "source": [
        "O comando a seguir utiliza o método `parellelize()` para armazenar no RDD chamado fib_rdd o conjunto de dados referente à Sequência de Fibonacci. Uma vez criado, fib_rdd pode ser manipulado em paralelo utilizando comandos que são apresentados ao longo do *notebook*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCgxsU4NHG-1"
      },
      "source": [
        "fib_rdd = spark.parallelize(fib)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcWXbKKkIGze"
      },
      "source": [
        "#### Método textFile()\n",
        "\n",
        "``textFile(nameFile, nPartitions=None, use_unicode=True)``\n",
        "\n",
        "Lê um arquivo do tipo texto chamado `nameFile`, o qual encontra-se armazenado em um sistema de armazenamento externo, e o retorna como um RDD de *strings*. O segundo parâmetro, `nPartitions`, indica o número de partições nas quais os registros do arquivo são particionados. O último parâmetro se refere ao formato do arquivo texto, cujo padrão é o formato UTF-8. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKCruYrMKKuw"
      },
      "source": [
        "O comando a seguir utiliza o método `textFile()` para armazenar no RDD chamado lines_rdd os registros do arquivo de texto `\"logs.txt\"`. Uma vez criado, lines_rdd pode ser manipulado em paralelo utilizando comandos que são apresentados ao longo do notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tftKj18AJXmU"
      },
      "source": [
        "lines_rdd = spark.textFile(\"logs.txt\")"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wk9pGtQdDUFE"
      },
      "source": [
        "### 3.2 Transformações"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Seu4HwNzL1g6"
      },
      "source": [
        "Transformações são operações que transformam um RDD em outro RDD.  \n",
        "\n",
        "No Spark, as **transformações** não são executadas imediatamente sobre os dados do RDD. Elas são anexadas ao grafo acíclico direcionado de transformações, o qual é executado apenas quando uma **ação** sobre o RDD em questão for solicitada. Essa característica é conhecida como *lazy-evaluation*. Mais informações sobre essa característica podem ser encontradas neste [link](https://databricks.com/blog/2015/06/22/understanding-your-spark-application-through-visualization.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c94R0hihYzMB"
      },
      "source": [
        "#### Método map()\n",
        "\n",
        "``map(func)``\n",
        "\n",
        "Aplica a função `func` sobre todos os elementos do RDD."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XR3jZUSsO_t9"
      },
      "source": [
        "Os comandos a seguir definem uma função que multiplica todos os elementos da Sequência de Fibonacci contidos em `fib_rdd` por 10."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfHkGbOkQpYU",
        "outputId": "bd52eb66-626f-4792-a370-352fa808dcc8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#definindo a função separadamente e chamando a função\n",
        "def multiply_by_ten(element): return element * 10 \n",
        "\n",
        "fib_rdd. \\\n",
        "  map(multiply_by_ten). \\\n",
        "  collect()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0,\n",
              " 10,\n",
              " 10,\n",
              " 20,\n",
              " 30,\n",
              " 50,\n",
              " 80,\n",
              " 130,\n",
              " 210,\n",
              " 340,\n",
              " 550,\n",
              " 890,\n",
              " 1440,\n",
              " 2330,\n",
              " 3770,\n",
              " 6100,\n",
              " 9870,\n",
              " 15970,\n",
              " 25840,\n",
              " 41810,\n",
              " 67650,\n",
              " 109460,\n",
              " 177110,\n",
              " 286570,\n",
              " 463680,\n",
              " 750250,\n",
              " 1213930,\n",
              " 1964180,\n",
              " 3178110,\n",
              " 5142290]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1P2CGdB2vf4w",
        "outputId": "9448ac7a-bcea-4a9d-b9a9-0416a24c64f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#definindo a função lambda\n",
        "fib_rdd. \\\n",
        "  map(lambda element: element * 10). \\\n",
        "  take(5)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 10, 10, 20, 30]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WcHgFcNA3Od"
      },
      "source": [
        "#### Método flatMap()\n",
        "\n",
        "`flatMap(func, preservesPartitioning=False)`\n",
        "\n",
        "Aplica a função `func` sobre todos os elementos do RDD e nivela os resultados gerados."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wucWpvuYPAhV"
      },
      "source": [
        "No comando a seguir, aplica-se o método nativo do Python `split`, por meio da transformação do método `map`, sobre os elementos em `line_rdd` para separar as linhas do arquivo `logs.txt` em palavras. Contudo, o método `split` retorna uma lista Python. Portanto, os elementos do novo RDD não são palavras individuais, mas sim listas de palavras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DsgHlLaPAxx",
        "outputId": "a2185f1a-5809-4b88-ac94-1ae0aab26621",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# aplicando a transformaçao map para transformar linhas em palavras\n",
        "# o resultado não são palavras individuais, mas sim listas Python de palavras\n",
        "lines_rdd. \\\n",
        "  map(lambda line: line.split(\" \")). \\\n",
        "  take(2)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['[Thu',\n",
              "  'Jun',\n",
              "  '09',\n",
              "  '06:07:04',\n",
              "  '2005]',\n",
              "  '[notice]',\n",
              "  'LDAP:',\n",
              "  'Built',\n",
              "  'with',\n",
              "  'OpenLDAP',\n",
              "  'LDAP',\n",
              "  'SDK'],\n",
              " ['[Thu',\n",
              "  'Jun',\n",
              "  '09',\n",
              "  '06:07:04',\n",
              "  '2005]',\n",
              "  '[notice]',\n",
              "  'LDAP:',\n",
              "  'SSL',\n",
              "  'support',\n",
              "  'unavailable']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEAr-wMOPY-f"
      },
      "source": [
        "Para unir as coleções Python resultantes da transformação provida pelo método `map`, ou seja, para nivelar os elementos do novo RDD, utiliza-se a transformação `flatMap`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-XG0tkpPZft",
        "outputId": "0bcf6ad2-5f60-4dc5-b21f-0c309d269da0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# aplicando a transformaçao flatMap para transformar linhas em palavras\n",
        "# as listas Python de palavras resultantes são niveladas, gerando um RDD em que cada elemente é uma palavra individual\n",
        "lines_rdd. \\\n",
        "  flatMap(lambda line: line.split(\" \")). \\\n",
        "  take(12)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[Thu',\n",
              " 'Jun',\n",
              " '09',\n",
              " '06:07:04',\n",
              " '2005]',\n",
              " '[notice]',\n",
              " 'LDAP:',\n",
              " 'Built',\n",
              " 'with',\n",
              " 'OpenLDAP',\n",
              " 'LDAP',\n",
              " 'SDK']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OP-p7ue8Ilm6"
      },
      "source": [
        "#### Método mapValues()\n",
        "\n",
        "``mapValues(func)``\n",
        "\n",
        "Considerando um RDD composto por pares chave-valor (C,V), aplica a função `func` apenas sobre os valores V do RDD."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mf2FIjULwwE"
      },
      "source": [
        "Nos comandos a seguir, primeiro aplica-se a transformação `map` para criar um RDD composto por pares chave-valor, de forma que chave = valor para cada elemento do RDD. Na sequência, aplica-se a transformação `mapValues` usando uma função `lambda` que mapeia os valores da sequência de Fibonacci em par (*even*) ou ímpar (*odd*)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pk9byBz0ImQf",
        "outputId": "32cd5984-946f-4d77-cae6-a7b14e4f438f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#produzindo pares na forma chave-valor \n",
        "fib_rdd. \\\n",
        "  map(lambda element: (element, element)). \\\n",
        "  mapValues(lambda element: \"even\" if element % 2 == 0 else \"odd\"). \\\n",
        "  collect()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0, 'even'),\n",
              " (1, 'odd'),\n",
              " (1, 'odd'),\n",
              " (2, 'even'),\n",
              " (3, 'odd'),\n",
              " (5, 'odd'),\n",
              " (8, 'even'),\n",
              " (13, 'odd'),\n",
              " (21, 'odd'),\n",
              " (34, 'even'),\n",
              " (55, 'odd'),\n",
              " (89, 'odd'),\n",
              " (144, 'even'),\n",
              " (233, 'odd'),\n",
              " (377, 'odd'),\n",
              " (610, 'even'),\n",
              " (987, 'odd'),\n",
              " (1597, 'odd'),\n",
              " (2584, 'even'),\n",
              " (4181, 'odd'),\n",
              " (6765, 'odd'),\n",
              " (10946, 'even'),\n",
              " (17711, 'odd'),\n",
              " (28657, 'odd'),\n",
              " (46368, 'even'),\n",
              " (75025, 'odd'),\n",
              " (121393, 'odd'),\n",
              " (196418, 'even'),\n",
              " (317811, 'odd'),\n",
              " (514229, 'odd')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTvXGjh5Uuv3"
      },
      "source": [
        "#### Método filter()\n",
        "\n",
        "``filter(func)``\n",
        "\n",
        "Aplica a função booleana `func` sobre todos os elementos do RDD e retorna apenas os elementos que são verdadeiros."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUtaMQ5uRk2q"
      },
      "source": [
        "Os comandos a seguir definem uma função booleana que retorna verdadeiro caso a string `[error]` esteja presente na linha de `lines_rdd`.  Essa função é utilizada pela transformação `filter` para exibir as linhas de `lines_rdd`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1Y87b5AU8CP",
        "outputId": "26681157-e471-4f9c-86e2-6e65faaea46b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#aplicando o método filter para recuperar as linhas de interesse\n",
        "def is_error_line(line) -> bool: return True if \"[error]\" in line else False\n",
        "\n",
        "lines_rdd. \\\n",
        "  filter(is_error_line). \\\n",
        "  take(15)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[Thu Jun 09 06:07:05 2005] [error] env.createBean2(): Factory error creating channel.jni:jni ( channel.jni, jni)',\n",
              " \"[Thu Jun 09 06:07:05 2005] [error] config.update(): Can't create channel.jni:jni\",\n",
              " '[Thu Jun 09 06:07:05 2005] [error] env.createBean2(): Factory error creating vm: ( vm, )',\n",
              " \"[Thu Jun 09 06:07:05 2005] [error] config.update(): Can't create vm:\",\n",
              " '[Thu Jun 09 06:07:05 2005] [error] env.createBean2(): Factory error creating worker.jni:onStartup ( worker.jni, onStartup)',\n",
              " \"[Thu Jun 09 06:07:05 2005] [error] config.update(): Can't create worker.jni:onStartup\",\n",
              " '[Thu Jun 09 06:07:05 2005] [error] env.createBean2(): Factory error creating worker.jni:onShutdown ( worker.jni, onShutdown)',\n",
              " \"[Thu Jun 09 06:07:05 2005] [error] config.update(): Can't create worker.jni:onShutdown\",\n",
              " '[Thu Jun 09 06:07:20 2005] [error] mod_jk child init 1 0',\n",
              " '[Thu Jun 09 07:11:21 2005] [error] [client 204.100.200.22] Directory index forbidden by rule: /var/www/html/',\n",
              " '[Thu Jun 09 12:08:57 2005] [error] [client 207.203.80.15] Directory index forbidden by rule: /var/www/html/',\n",
              " '[Thu Jun 09 12:17:49 2005] [error] [client 216.68.171.39] Directory index forbidden by rule: /var/www/html/',\n",
              " '[Thu Jun 09 12:48:10 2005] [error] [client 24.158.204.7] Directory index forbidden by rule: /var/www/html/',\n",
              " '[Thu Jun 09 13:59:36 2005] [error] [client 216.85.154.124] Directory index forbidden by rule: /var/www/html/',\n",
              " '[Thu Jun 09 14:22:15 2005] [error] [client 219.136.165.149] Directory index forbidden by rule: /var/www/html/']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uM1RVgXvRlTs"
      },
      "source": [
        "#### Método join()\n",
        "\n",
        "``join(RDD²)``\n",
        "\n",
        "Quando aplicado a um RDD¹ composto por elementos na forma pares chave-valor (C¹,V¹) e, considerando o parâmetro RDD² na forma de pares chave-valor (C²,V²), junta os valores V¹,V² que possuem o mesmo valor de chave (C¹ = C²), gerando  (C¹, (V¹,V²)). O método `join()` combina os valores dos elementos dos RDDs que compartilham a mesma chave dois a dois. Esse método requer uma operação de `suffle`. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjDoZwBgRcYq"
      },
      "source": [
        "Os comandos a seguir dividem o RDD `fib_rdd` em dois novos RDDs usando o método `filter` e o método `map`. O método `filter` é usado para filtrar os dados do RDD de acordo com os seus valores. No primeiro RDD são considerados valores menores ou iguais a 100, enquanto que no segundo RDD são considerados valores maiores do que 100. O método `map` é usado para mapear o RDD em um RDD do tipo chave-valor, com chave igual a par (even) ou ímpar (odd) de acordo com o valor. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Ah6_rnfReOD",
        "outputId": "e3e26102-8c1a-4315-e631-86eb406c0506",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "up_to_100_rdd = fib_rdd. \\\n",
        "                filter(lambda element: element <= 100). \\\n",
        "                map(lambda element: (\"even\", element) if element % 2 == 0 else (\"odd\", element))\n",
        "up_to_100_rdd.take(3)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('even', 0), ('odd', 1), ('odd', 1)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZhanmHMTHHQ",
        "outputId": "69c06d7f-a736-44f8-b309-9b6c04ce77bb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "greater_than_100_rdd = fib_rdd. \\\n",
        "                       filter(lambda element: element > 100). \\\n",
        "                       map(lambda element: (\"even\", element) if element % 2 == 0 else (\"odd\", element))\n",
        "greater_than_100_rdd.take(4)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('even', 144), ('odd', 233), ('odd', 377), ('even', 610)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ul3pxLKchu1W"
      },
      "source": [
        "O comando a seguir aplica o método `join` para juntar os valores que possuem a mesma chave. O resultado produzido é referente a um `inner join`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqViY6yeTJ4T",
        "outputId": "e5cf0915-a9b2-45e7-ae9b-3b9e8ec9baa4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "up_to_100_rdd. \\\n",
        "  join(greater_than_100_rdd). \\\n",
        "  take(3)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('even', (0, 144)), ('even', (0, 610)), ('even', (0, 2584))]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DA4sVamjWxqW"
      },
      "source": [
        "#### Método reduceByKey()\n",
        "\n",
        "``reduceByKey(func,nPartitions=None)``\n",
        "\n",
        "Quando aplicado a um RDD composto por pares chave-valor (C,V), agrega os valores V, de forma que esses valores são computados dois a dois usando a função `func` e agrupados de acordo com a chave C. O segundo parâmetro, `nPartitions`, possibilita que o número de tarefas *reduce* seja configurado. Esse método requer uma operação de `suffle`. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyVUS2PtVzCJ"
      },
      "source": [
        "Os comandos a seguir analisam os elementos de `fib_rdd`, retornando a quantidade de elementos que possuem valores menores do que 10 e a quantidade de elementos que possuem valores maiores ou iguais a 10. \n",
        "\n",
        "Note que a entrada para o método `reduceByKey()` é um RDD composto por pares chave-valor. Portanto, primeiramente é aplicado o método `map()`, para depois ser aplicado o método `reduceByKey()`. A saída é formada por pares chave-valor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76f80nVzW4qm",
        "cellView": "both",
        "outputId": "c8ae20eb-2c93-4621-bed8-c5a0e2acaf0d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "fib_rdd. \\\n",
        "  map(lambda element: (True if element > 10 else False, 1)). \\\n",
        "  reduceByKey(lambda x, y: x + y). \\\n",
        "  collect()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(False, 7), (True, 23)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0ZaZ7AVaKmS"
      },
      "source": [
        "#### Método sortByKey()\n",
        "\n",
        "``sortByKey(order=ascending, nPartitions=None)``\n",
        "\n",
        "Quando aplicado a um RDD composto por pares chave-valor (C,V), ordena os elementos de acordo com os valores de C em ordem ascendente ou descentende. O primeiro parâmetro, `order`, indica ascendente ou descendente. O segundo parâmetro, `nPartitions`, permite configurar o número de partições. Este método requer uma operação de `shuffle`. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_xJh6uTY43u"
      },
      "source": [
        "Os comandos a seguir verificam se cada elemento de `fib_rdd` é par ou ímpar, retornando True para elementos pares e False para elementos ímpares. O resultado é ordenado em ordem ascentende, exibindo primeiro os elementos referentes às chaves False e depois os elementos referentes às chaves True.\n",
        "\n",
        "Note que a entrada para o método `sortByKey()` é um RDD composto por pares chave-valor. Portanto, primeiramente é aplicado o método `map()`, para depois ser aplicado o método `sortByKey()`. A saída é formada por pares chave-valor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GmWYjiJ8aKWU",
        "outputId": "0e3ba4ca-1377-41f9-fbbe-201b9ac8993e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "fib_rdd. \\\n",
        "  map(lambda element: (True if element % 2 == 0 else False, element)). \\\n",
        "  sortByKey(). \\\n",
        "  collect()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(False, 1),\n",
              " (False, 1),\n",
              " (False, 3),\n",
              " (False, 5),\n",
              " (False, 13),\n",
              " (False, 21),\n",
              " (False, 55),\n",
              " (False, 89),\n",
              " (False, 233),\n",
              " (False, 377),\n",
              " (False, 987),\n",
              " (False, 1597),\n",
              " (False, 4181),\n",
              " (False, 6765),\n",
              " (False, 17711),\n",
              " (False, 28657),\n",
              " (False, 75025),\n",
              " (False, 121393),\n",
              " (False, 317811),\n",
              " (False, 514229),\n",
              " (True, 0),\n",
              " (True, 2),\n",
              " (True, 8),\n",
              " (True, 34),\n",
              " (True, 144),\n",
              " (True, 610),\n",
              " (True, 2584),\n",
              " (True, 10946),\n",
              " (True, 46368),\n",
              " (True, 196418)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fiMssoCGKe9k"
      },
      "source": [
        "#### Método sortBy()\n",
        "\n",
        "sortBy(keyfunc, ascending=True, nPartitions=None)\n",
        "\n",
        "Ordena os elementos do RDD usando como base o parâmetro `keyfunc`. O segundo parâmetro, `ascending=True`, indica a ordem ascendente é a ordem padrão. O terceiro parâmetro, `nPartitions`, permite configurar o número de partições.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAUo-CoATDtA"
      },
      "source": [
        "Os comandos a seguir são análogos aos comandos da transformação do método `sortByKey`. No método `sortBy`, a ordenação dos elementos do RDD é definida pela função `keyfunc` que aponta para o primeiro item do elemento, ou seja, a chave."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hY0bRe79TEG_",
        "outputId": "520e2937-51f4-42f7-dd11-0e35e42da4ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "fib_rdd. \\\n",
        "  map(lambda element: (True if element % 2 == 0 else False, element)). \\\n",
        "  sortBy(lambda element: element[0]). \\\n",
        "  collect()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(False, 1),\n",
              " (False, 1),\n",
              " (False, 3),\n",
              " (False, 5),\n",
              " (False, 13),\n",
              " (False, 21),\n",
              " (False, 55),\n",
              " (False, 89),\n",
              " (False, 233),\n",
              " (False, 377),\n",
              " (False, 987),\n",
              " (False, 1597),\n",
              " (False, 4181),\n",
              " (False, 6765),\n",
              " (False, 17711),\n",
              " (False, 28657),\n",
              " (False, 75025),\n",
              " (False, 121393),\n",
              " (False, 317811),\n",
              " (False, 514229),\n",
              " (True, 0),\n",
              " (True, 2),\n",
              " (True, 8),\n",
              " (True, 34),\n",
              " (True, 144),\n",
              " (True, 610),\n",
              " (True, 2584),\n",
              " (True, 10946),\n",
              " (True, 46368),\n",
              " (True, 196418)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1sHv_35DaUV"
      },
      "source": [
        "### 3.3 Ações\n",
        "\n",
        "Ações são operações que retornam valores calculados sobre um RDD.  \n",
        "\n",
        "No Spark, as ações disparam a execução das **transformações** que foram anexadas ao grafo acíclico direcionado dessas transformações."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foLSPVWJdvUq"
      },
      "source": [
        "#### Métodos collect(), take() e first()\n",
        "\n",
        "Os métodos collect(), take() e first() têm como objetivo retornar os elementos de um RDD, conforme descrito a seguir.\n",
        "\n",
        "- `collect()`: Retorna uma lista com todos os elementos do RDD.\n",
        "\n",
        "- `take(num)`: Retorna uma lista com os primeiros `num` elementos do RDD.\n",
        "\n",
        "- `first()`:  Retorna o primeiro elemento do RDD."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLpopsIqd0Hi",
        "outputId": "36b891df-7d9d-4c9b-f906-bfe4cdd01f15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#listando todos os elementos da Sequência de Fibonacci armazenados em fib_rdd\n",
        "fib_rdd.collect()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 3,\n",
              " 5,\n",
              " 8,\n",
              " 13,\n",
              " 21,\n",
              " 34,\n",
              " 55,\n",
              " 89,\n",
              " 144,\n",
              " 233,\n",
              " 377,\n",
              " 610,\n",
              " 987,\n",
              " 1597,\n",
              " 2584,\n",
              " 4181,\n",
              " 6765,\n",
              " 10946,\n",
              " 17711,\n",
              " 28657,\n",
              " 46368,\n",
              " 75025,\n",
              " 121393,\n",
              " 196418,\n",
              " 317811,\n",
              " 514229]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MV12pyXSeDR8",
        "outputId": "6bbba3e3-be70-4b01-a803-1907db34dd77",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#listando os 10 primeiros elementos da Sequência de Fibonacci armazenados em fib_rdd\n",
        "fib_rdd.take(10)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 1, 1, 2, 3, 5, 8, 13, 21, 34]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iudxpdtIePVw",
        "outputId": "96266b2c-dc4d-4f28-8041-2002b5dccf8c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#listando o primeiro elemento da Sequência de Fibonacci armazenados em fib_rdd\n",
        "fib_rdd.first()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GH2UFVmscv9I"
      },
      "source": [
        "#### Métodos  max(), min(), mean(), count()\n",
        "\n",
        "Os métodos max(), min(), mean() e count() desempenham funcionalidades semelhantes às funções de agregação, conforme descrito a seguir.\n",
        "\n",
        "- `max(key=None)`: Retorna o maior elemento do RDD. O parâmetro opcional `key` pode ser usado para gerar uma `key` para comparação.\n",
        "-  `min(key=None)`: Retorna o menor elemento do RDD. O parâmetro opcional `key` pode ser usado para gerar uma `key` para comparação.\n",
        "-  `mean()`: Calcula a média dos valores dos elementos do RDD. \n",
        "- `count()`: Retorna o número de elementos do RDD."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p63faogcl8Ux",
        "outputId": "238472f8-403b-406c-ca51-fb9a9257c40c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#listando o maior elemento armazenado em fib_rdd\n",
        "fib_rdd.max()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "514229"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8UHHm1uRl-Bs",
        "outputId": "85614071-3def-46da-ccd5-d75dcb034f49",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#listando o menor elemento armazenado em fib_rdd\n",
        "fib_rdd.min()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TeEgIDjBmDGl",
        "outputId": "195a6d60-ef9b-4db6-ec24-f9da269c0adc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#calculando a média dos valores dos elementos armazeandos em fib_rdd\n",
        "fib_rdd.mean()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "44875.6"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RA41f753ftNR",
        "outputId": "663ba740-6c96-4b85-c2b4-42bf7eb3d418",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#listando a quantidade de elementos armazenados em fib_rdd\n",
        "fib_rdd.count()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OmxhJltdJvB"
      },
      "source": [
        "#### Método reduce()\n",
        "\n",
        "`reduce(func)`\n",
        "\n",
        "Aplica a função `func` a todos os elementos do RDD, processando os elementos dois a dois até a geração de um resultado final agregado.\n",
        "\n",
        "Os comandos a seguir calculam a soma dos elementos armazenados em fib_rdd."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZC8Vs5_2Kjkf",
        "outputId": "fb6eafe7-f4d5-4003-f6bd-5821834d0edd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#calculando a soma dos elementos armazenados em fib_rdd\n",
        "#definindo a função separadamente e chamando a função\n",
        "def sum(x: int, y: int) -> int: return x + y\n",
        "\n",
        "fib_rdd.reduce(sum)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1346268"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "duze2ndYKQnS",
        "outputId": "47f5963f-5525-4b11-818f-f5902af1e822",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#calculando a soma dos elementos armazenados em fib_rdd\n",
        "#definindo a função lambda\n",
        "fib_rdd.reduce(lambda x, y: x + y)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1346268"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Mjt0997Dehn"
      },
      "source": [
        "## **4 Contador de Palavras**\n",
        "\n",
        "O contador de palavras é um exemplo clássico de explicação da funcionalidade do modelo de programação funcional MapReduce. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zh_EJwggGbq2"
      },
      "source": [
        "Neste notebook é apresentado um exemplo que tem como objetivo contar as palavras presentes no texto do arquivo `logs.txt`. Primeiramente, o exemplo é implementado passo a passo, visando fins didáticos. Depois, são mostrados todos os passos realizados de uma única vez. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_B7Z5nXftlf"
      },
      "source": [
        "(1) Criação do RDD `lines_rdd` com as linhas do texto do arquivo. A criação deste RDD pode ser encontrada junto à descrição do método `textFile()`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wxv4HJTf0Zj",
        "outputId": "d4d660ef-7e27-4727-9074-abed2a290a60",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "lines_rdd.take(15)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[Thu Jun 09 06:07:04 2005] [notice] LDAP: Built with OpenLDAP LDAP SDK',\n",
              " '[Thu Jun 09 06:07:04 2005] [notice] LDAP: SSL support unavailable',\n",
              " '[Thu Jun 09 06:07:04 2005] [notice] suEXEC mechanism enabled (wrapper: /usr/sbin/suexec)',\n",
              " '[Thu Jun 09 06:07:05 2005] [notice] Digest: generating secret for digest authentication ...',\n",
              " '[Thu Jun 09 06:07:05 2005] [notice] Digest: done',\n",
              " '[Thu Jun 09 06:07:05 2005] [notice] LDAP: Built with OpenLDAP LDAP SDK',\n",
              " '[Thu Jun 09 06:07:05 2005] [notice] LDAP: SSL support unavailable',\n",
              " '[Thu Jun 09 06:07:05 2005] [error] env.createBean2(): Factory error creating channel.jni:jni ( channel.jni, jni)',\n",
              " \"[Thu Jun 09 06:07:05 2005] [error] config.update(): Can't create channel.jni:jni\",\n",
              " '[Thu Jun 09 06:07:05 2005] [error] env.createBean2(): Factory error creating vm: ( vm, )',\n",
              " \"[Thu Jun 09 06:07:05 2005] [error] config.update(): Can't create vm:\",\n",
              " '[Thu Jun 09 06:07:05 2005] [error] env.createBean2(): Factory error creating worker.jni:onStartup ( worker.jni, onStartup)',\n",
              " \"[Thu Jun 09 06:07:05 2005] [error] config.update(): Can't create worker.jni:onStartup\",\n",
              " '[Thu Jun 09 06:07:05 2005] [error] env.createBean2(): Factory error creating worker.jni:onShutdown ( worker.jni, onShutdown)',\n",
              " \"[Thu Jun 09 06:07:05 2005] [error] config.update(): Can't create worker.jni:onShutdown\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBUc17V3gXwl"
      },
      "source": [
        "(2) Separação das linhas do arquivo de dados em palavras usando o método `flatMap`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibAHKBMrgX_C",
        "outputId": "b7a9593b-58cd-4221-d1ef-bb01ffab2bda",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "words_rdd = lines_rdd.flatMap(lambda line: line.split(\" \"))\n",
        "words_rdd.take(7)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[Thu', 'Jun', '09', '06:07:04', '2005]', '[notice]', 'LDAP:']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfNEeGH3hNFx"
      },
      "source": [
        "(3) Mapeamento de cada palavra presente no RDD words_rdd em um par chave-valor usando o método `map`. Cada chave corresponde a uma palavra e cada valor corresponde ao valor 1. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "056XBbB5hNOU",
        "outputId": "ca5b50dd-28a5-4a62-d468-25d42eab26a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "words_tuple_rdd = words_rdd.map(lambda word: (word, 1))\n",
        "words_tuple_rdd.take(7)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('[Thu', 1),\n",
              " ('Jun', 1),\n",
              " ('09', 1),\n",
              " ('06:07:04', 1),\n",
              " ('2005]', 1),\n",
              " ('[notice]', 1),\n",
              " ('LDAP:', 1)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvEJfAnLhkVD"
      },
      "source": [
        "(4) Agrupamento das palavras iguais usando o método `reduceByKey`. A função usada como parâmetro é soma. Ou seja, soma-se o número de vezes que cada palavra aparece.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RN7dL95whkco",
        "outputId": "39337d5b-3092-49c3-a32c-27e7623c2bf0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "words_counts_rdd = words_tuple_rdd.reduceByKey(lambda x, y: x + y)\n",
        "words_counts_rdd.take(7)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('09', 1793),\n",
              " ('06:07:04', 7),\n",
              " ('LDAP:', 106),\n",
              " ('SDK', 53),\n",
              " ('SSL', 53),\n",
              " ('support', 53),\n",
              " ('unavailable', 53)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BOmiDyuq1H-"
      },
      "source": [
        "Pode ser interessante ordenar o resultado final usando o método `sortBy`, de forma que as palavras com maior ocorrência apareçam primeiro. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzjTEi0eq1bs",
        "outputId": "e0d3d428-6a2d-4c13-94ed-16c1172ee268",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "words_counts_rdd_sorted = words_counts_rdd.sortBy(lambda word_count: word_count[1], ascending=False)\n",
        "words_counts_rdd_sorted.take(7)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('[error]', 38081),\n",
              " ('2005]', 32309),\n",
              " ('[client', 31115),\n",
              " ('not', 28808),\n",
              " ('File', 20861),\n",
              " ('does', 20861),\n",
              " ('exist:', 20861)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4ic4waXobT_"
      },
      "source": [
        "Todos os passos anteriores podem ser agrupados, conforme descrito a seguir.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFdzmRA_E7Sv"
      },
      "source": [
        "words_counts = spark.textFile(\"logs.txt\"). \\\n",
        "               flatMap(lambda line: line.split(\" \")). \\\n",
        "               map(lambda word: (word, 1)). \\\n",
        "               reduceByKey(lambda x, y: x + y). \\\n",
        "               sortBy(lambda word_count: word_count[1], ascending=False). \\\n",
        "               collect()"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRw6NfNsFRsg",
        "outputId": "b3d7a53d-fdf8-4194-f488-fc3daecf6fe5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "words_counts[0:7]"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('[error]', 38081),\n",
              " ('2005]', 32309),\n",
              " ('[client', 31115),\n",
              " ('not', 28808),\n",
              " ('File', 20861),\n",
              " ('does', 20861),\n",
              " ('exist:', 20861)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixVPoBI1Upaw"
      },
      "source": [
        "# 5 Persistência dos RDDs\n",
        "\n",
        "RDDs são de primordial importância no Spark, desde que possibilitam o armazenamento dos resultados intermediários em memória primária sempre que possível. Conforme discutido anteriormente, o uso de RDDs diminui o número de acessos a disco e, consequentemente, melhora o desempenho da aplicação.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUI_AL4RVxSV"
      },
      "source": [
        "Existem dois métodos para a persistência dos RDDs, conforme descrito a seguir.\n",
        "\n",
        "- `cache()`: Persiste um RDD utilizando o nível de armazenamento padrão, que é MEMORY_ONLY. \n",
        "\n",
        "- `persist(storageLevel=StorageLevel(False, True, False, False, 1))`: Seta o nível de armazenamento que persiste um RDD na primeira vez que o RDD é calculado. Este comando somente pode ser utilizado para associar um novo nível de armazenamento caso o RDD não possua nenhum nível já associado. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cZNCS28ZyuB"
      },
      "source": [
        "Os níveis de armazenamento são:\n",
        "\n",
        "- MEMORY_ONLY (StorageLevel(False, True, False, False, 1)): Armazena o RDD em memória primária, ou seja, todas as partições do RDD são armazenadas em memória primária quando possível. Se o RDD não couber totalmente em memória primária, as partições que não couberem são calculadas em tempo de execução todas as vezes que for necessário usá-las. Este é o nível de armazenamento default.   \n",
        "\n",
        "- MEMORY_AND_DISK (StorageLevel(True, True, False, False, 1)): Armazena o RDD em memória primária e em disco. As partições do RDD não que couberem em memória primária são armazenadas nessa memória, e as partições que não couberem são armazenadas em disco. As partições armazenadas em disco são lidas do disco todas as vezes que for necessário usá-las.\n",
        "\n",
        "- DISK_ONLY (StorageLevel(True, False, False, False, 1)): Armazena o RDD em disco, ou seja, todas as partições do RDD são armazenadas em disco. Essas partições devem ser lidas do disco todas as vezes que for necessário usá-las.\n",
        "\n",
        "- MEMORY_ONLY_2 (StorageLevel(False, True, False, False, 2)): Possui funcionalidade similar ao nível de armazenamento MEMORY_ONLY, porém replica cada partição em dois nós do *cluster*.\n",
        "\n",
        "- MEMORY_AND_DISK_2 (StorageLevel(True, True, False, False, 2)): Possui funcionalidade similar ao nível de armazenamento MEMORY_AND_DISK, porém replica cada partição em dois nós do *cluster*.\n",
        "\n",
        "- DISK_ONLY_2 (StorageLevel(True, False, False, False, 2)): Possui funcionalidade similar ao nível de armazenamento DISK_ONLY, porém replica cada partição em dois nós do *cluster*.  \n",
        "\n",
        "- OFF_HEAP (StorageLevel(True, True, True, False, 1)): Armazena o RDD em memória off-heap.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Opz4ri59f6lN"
      },
      "source": [
        "Em Python, os objetos armazenados são serializáveis por meio da biblioteca Pickle, ou seja, não existe necessidade de se escolher o nível serializável. Em linguagens de programação com suporte diferente, pode-se usar os níveis de armazenamento MEMORY_ONLY_SER e MEMORY_AND_DISK_SER.\n",
        "\n",
        "Spark automaticamente persiste alguns dados intermediários em operações *suffle* (por exemplo, `reduceByKey()`), mesmo quando isso não é definido de forma explícita. Isto é feito para se evitar a necessidade de recomputação de entrada inteira caso ocorra uma falha durante o *suffle*. "
      ]
    }
  ]
}